 /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/layer22_mlp_intermediate.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
üì§ Sent file layer22_mlp_intermediate.bin to 192.168.2.104
‚úÖ Received layer22_mlp_intermediate.bin 1/1
‚úÖ All ACKs received!
Sending to 192.168.2.101
üì§ Sent file layer22_mlp_intermediate.bin to 192.168.2.101
‚úÖ Received layer22_mlp_intermediate.bin 1/1
‚úÖ All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: layer22_mlp_intermediate
Matrix B: layers_22_mlp_down_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer22_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_22_mlp_down_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/layer22_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_22_mlp_down_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer22_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_22_mlp_down_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer22_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_22_mlp_down_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.54 seconds

üìä Result base: layers_22_mlp_down_proj_weightxlayer22_mlp_intermediate (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_22_mlp_down_proj_weightxlayer22_mlp_intermediate_combined.bin
  Original dims: [1, 1, 1, 4096]
  Result tensor shape: torch.Size([1, 4096]), size: 16,384 bytes
  Data range: [-0.401093, 0.244109]
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Preparing full matrix: input_token_embeddings.bin
Local paths - DISK: matrix_shards/input_token_embeddings.bin, RAM: /dev/shm/matrix_shards/input_token_embeddings.bin
Saving to local storage...
Saving matrix to binary file (OpenCL): matrix_shards/input_token_embeddings.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Saving matrix to binary file (OpenCL): /dev/shm/matrix_shards/input_token_embeddings.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Remote paths - RAM: /dev/shm/matrix_shards/input_token_embeddings.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/input_token_embeddings.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
üì§ Sent file input_token_embeddings.bin to 192.168.2.104
‚úÖ Received input_token_embeddings.bin 1/1
‚úÖ All ACKs received!
Sending to 192.168.2.101
üì§ Sent file input_token_embeddings.bin to 192.168.2.101
‚úÖ Received input_token_embeddings.bin 1/1
‚úÖ All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_23_self_attn_q_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_23_self_attn_q_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_23_self_attn_q_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_23_self_attn_k_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_23_self_attn_k_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_23_self_attn_k_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_23_self_attn_v_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_23_self_attn_v_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_23_self_attn_v_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embeddings
Matrix B: layers_23_self_attn_q_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_23_self_attn_q_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_23_self_attn_q_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_23_self_attn_q_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_23_self_attn_q_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.16 seconds

üìä Result base: layers_23_self_attn_q_proj_weightxinput_token_embeddings (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_23_self_attn_q_proj_weightxinput_token_embeddings_combined.bin
  Original dims: [1, 1, 1, 4096]
  Result tensor shape: torch.Size([1, 4096]), size: 16,384 bytes
  Data range: [-12.031794, 12.011848]

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embeddings
Matrix B: layers_23_self_attn_k_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_23_self_attn_k_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_23_self_attn_k_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_23_self_attn_k_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_23_self_attn_k_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.06 seconds

üìä Result base: layers_23_self_attn_k_proj_weightxinput_token_embeddings (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_23_self_attn_k_proj_weightxinput_token_embeddings_combined.bin
  Original dims: [1, 1, 1, 1024]
  Result tensor shape: torch.Size([1, 1024]), size: 4,096 bytes
  Data range: [-14.958628, 12.415341]

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embeddings
Matrix B: layers_23_self_attn_v_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_23_self_attn_v_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_23_self_attn_v_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_23_self_attn_v_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_23_self_attn_v_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.05 seconds

üìä Result base: layers_23_self_attn_v_proj_weightxinput_token_embeddings (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_23_self_attn_v_proj_weightxinput_token_embeddings_combined.bin
  Original dims: [1, 1, 1, 1024]
  Result tensor shape: torch.Size([1, 1024]), size: 4,096 bytes
  Data range: [-1.049868, 1.162908]
RoPE q shape: (32, 128)
RoPE k shape: (8, 128)
KV cache layer 23: K=(32, 25, 128) V=(32, 25, 128)
attn_output (per-head): (32, 128)
attn_hidden: (1, 4096)
residual: (1, 4096)
hidden_out(after attn+res): (1, 4096)
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Preparing full matrix: layer23_mlp_in.bin
Local paths - DISK: matrix_shards/layer23_mlp_in.bin, RAM: /dev/shm/matrix_shards/layer23_mlp_in.bin
Saving to local storage...
Saving matrix to binary file (OpenCL): matrix_shards/layer23_mlp_in.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Saving matrix to binary file (OpenCL): /dev/shm/matrix_shards/layer23_mlp_in.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Remote paths - RAM: /dev/shm/matrix_shards/layer23_mlp_in.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/layer23_mlp_in.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
üì§ Sent file layer23_mlp_in.bin to 192.168.2.104
‚úÖ Received layer23_mlp_in.bin 1/1
‚úÖ All ACKs received!
Sending to 192.168.2.101
üì§ Sent file layer23_mlp_in.bin to 192.168.2.101
‚úÖ Received layer23_mlp_in.bin 1/1
‚úÖ All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_23_mlp_gate_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_23_mlp_gate_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_23_mlp_gate_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_23_mlp_up_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_23_mlp_up_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_23_mlp_up_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_23_mlp_down_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_23_mlp_down_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_23_mlp_down_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: layer23_mlp_in
Matrix B: layers_23_mlp_gate_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer23_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_23_mlp_gate_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/layer23_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_23_mlp_gate_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer23_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_23_mlp_gate_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer23_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_23_mlp_gate_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.54 seconds

üìä Result base: layers_23_mlp_gate_proj_weightxlayer23_mlp_in (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_23_mlp_gate_proj_weightxlayer23_mlp_in_combined.bin
  Original dims: [1, 1, 1, 14336]
  Result tensor shape: torch.Size([1, 14336]), size: 57,344 bytes
  Data range: [-3.291650, 2.496866]

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: layer23_mlp_in
Matrix B: layers_23_mlp_up_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer23_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_23_mlp_up_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/layer23_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_23_mlp_up_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer23_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_23_mlp_up_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer23_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_23_mlp_up_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.54 seconds

üìä Result base: layers_23_mlp_up_proj_weightxlayer23_mlp_in (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_23_mlp_up_proj_weightxlayer23_mlp_in_combined.bin
  Original dims: [1, 1, 1, 14336]
  Result tensor shape: torch.Size([1, 14336]), size: 57,344 bytes
  Data range: [-2.762067, 1.372917]
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Preparing full matrix: layer23_mlp_intermediate.bin
Local paths - DISK: matrix_shards/layer23_mlp_intermediate.bin, RAM: /dev/shm/matrix_shards/layer23_mlp_intermediate.bin
Saving to local storage...
Saving matrix to binary file (OpenCL): matrix_shards/layer23_mlp_intermediate.bin
Original shape: (14336, 1)
Converted to 4D: (1, 1, 14336, 1)
Writing binary file...
Done.
Saving matrix to binary file (OpenCL): /dev/shm/matrix_shards/layer23_mlp_intermediate.bin
Original shape: (14336, 1)
Converted to 4D: (1, 1, 14336, 1)
Writing binary file...
Done.
Remote paths - RAM: /dev/shm/matrix_shards/layer23_mlp_intermediate.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/layer23_mlp_intermediate.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
üì§ Sent file layer23_mlp_intermediate.bin to 192.168.2.104
‚úÖ Received layer23_mlp_intermediate.bin 1/1
‚úÖ All ACKs received!
Sending to 192.168.2.101
üì§ Sent file layer23_mlp_intermediate.bin to 192.168.2.101
‚úÖ Received layer23_mlp_intermediate.bin 1/1
‚úÖ All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: layer23_mlp_intermediate
Matrix B: layers_23_mlp_down_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer23_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_23_mlp_down_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/layer23_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_23_mlp_down_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer23_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_23_mlp_down_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer23_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_23_mlp_down_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.54 seconds

üìä Result base: layers_23_mlp_down_proj_weightxlayer23_mlp_intermediate (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_23_mlp_down_proj_weightxlayer23_mlp_intermediate_combined.bin
  Original dims: [1, 1, 1, 4096]
  Result tensor shape: torch.Size([1, 4096]), size: 16,384 bytes
  Data range: [-0.342630, 0.272359]
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Preparing full matrix: input_token_embeddings.bin
Local paths - DISK: matrix_shards/input_token_embeddings.bin, RAM: /dev/shm/matrix_shards/input_token_embeddings.bin
Saving to local storage...
Saving matrix to binary file (OpenCL): matrix_shards/input_token_embeddings.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Saving matrix to binary file (OpenCL): /dev/shm/matrix_shards/input_token_embeddings.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Remote paths - RAM: /dev/shm/matrix_shards/input_token_embeddings.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/input_token_embeddings.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
üì§ Sent file input_token_embeddings.bin to 192.168.2.104
‚úÖ Received input_token_embeddings.bin 1/1
‚úÖ All ACKs received!
Sending to 192.168.2.101
üì§ Sent file input_token_embeddings.bin to 192.168.2.101
‚úÖ Received input_token_embeddings.bin 1/1
‚úÖ All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_24_self_attn_q_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_24_self_attn_q_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_24_self_attn_q_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_24_self_attn_k_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_24_self_attn_k_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_24_self_attn_k_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_24_self_attn_v_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_24_self_attn_v_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_24_self_attn_v_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embeddings
Matrix B: layers_24_self_attn_q_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_24_self_attn_q_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_24_self_attn_q_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_24_self_attn_q_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_24_self_attn_q_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.29 seconds

üìä Result base: layers_24_self_attn_q_proj_weightxinput_token_embeddings (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_24_self_attn_q_proj_weightxinput_token_embeddings_combined.bin
  Original dims: [1, 1, 1, 4096]
  Result tensor shape: torch.Size([1, 4096]), size: 16,384 bytes
  Data range: [-10.276804, 12.261297]

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embeddings
Matrix B: layers_24_self_attn_k_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_24_self_attn_k_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_24_self_attn_k_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_24_self_attn_k_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_24_self_attn_k_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.06 seconds

üìä Result base: layers_24_self_attn_k_proj_weightxinput_token_embeddings (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_24_self_attn_k_proj_weightxinput_token_embeddings_combined.bin
  Original dims: [1, 1, 1, 1024]
  Result tensor shape: torch.Size([1, 1024]), size: 4,096 bytes
  Data range: [-11.898598, 12.103438]

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embeddings
Matrix B: layers_24_self_attn_v_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_24_self_attn_v_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_24_self_attn_v_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_24_self_attn_v_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_24_self_attn_v_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.05 seconds

üìä Result base: layers_24_self_attn_v_proj_weightxinput_token_embeddings (send_back=True)
‚úÖ Received ACK_combined_matrix_saved 1/1
‚úÖ All ACKs received!
‚úÖ Loaded /dev/shm/matrix_shards/layers_24_self_attn_v_proj_weightxinput_token_embeddings_combined.bin
  Original dims: [1, 1, 1, 1024]
  Result tensor shape: torch.Size([1, 1024]), size: 4,096 bytes
  Data range: [-1.536478, 1.286769]
RoPE q shape: (32, 128)
RoPE k shape: (8, 128)
KV cache layer 24: K=(32, 25, 128) V=(32, 25, 128)
attn_output (per-head): (32, 128)
attn_hidden: (1, 4096)
residual: (1, 4096)
hidden_out(after attn+res): (1, 4096)
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Preparing full matrix: layer24_mlp_in.bin
Local paths - DISK: matrix_shards/layer24_mlp_in.bin, RAM: /dev/shm/matrix_shards/layer24_mlp_in.bin
Saving to local storage...
Saving matrix to binary file (OpenCL): matrix_shards/layer24_mlp_in.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Saving matrix to binary file (OpenCL): /dev/shm/matrix_shards/layer24_mlp_in.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Remote paths - RAM: /dev/shm/matrix_shards/layer24_mlp_in.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/layer24_mlp_in.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
üì§ Sent file layer24_mlp_in.bin to 192.168.2.104
‚úÖ Received layer24_mlp_in.bin 1/1
‚úÖ All ACKs received!
Sending to 192.168.2.101
üì§ Sent file layer24_mlp_in.bin to 192.168.2.101
‚úÖ Received layer24_mlp_in.bin 1/1
‚úÖ All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_24_mlp_gate_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_24_mlp_gate_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_24_mlp_gate_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_24_mlp_up_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_24_mlp_up_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_24_mlp_up_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_24_mlp_down_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_24_mlp_down_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_24_mlp_down_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: layer24_mlp_in
Matrix B: layers_24_mlp_gate_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer24_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_24_mlp_gate_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/layer24_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_24_mlp_gate_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer24_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_24_mlp_gate_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer24_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_24_mlp_gate_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.54 seconds

üìä Result base: layers_24_mlp_gate_proj_weightxlayer24_mlp_in (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_24_mlp_gate_proj_weightxlayer24_mlp_in_combined.bin
  Original dims: [1, 1, 1, 14336]
  Result tensor shape: torch.Size([1, 14336]), size: 57,344 bytes
  Data range: [-3.635271, 2.234759]

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: layer24_mlp_in
Matrix B: layers_24_mlp_up_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer24_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_24_mlp_up_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/layer24_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_24_mlp_up_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer24_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_24_mlp_up_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer24_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_24_mlp_up_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.54 seconds

üìä Result base: layers_24_mlp_up_proj_weightxlayer24_mlp_in (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_24_mlp_up_proj_weightxlayer24_mlp_in_combined.bin
  Original dims: [1, 1, 1, 14336]
  Result tensor shape: torch.Size([1, 14336]), size: 57,344 bytes
  Data range: [-4.139570, 1.526526]
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Preparing full matrix: layer24_mlp_intermediate.bin
Local paths - DISK: matrix_shards/layer24_mlp_intermediate.bin, RAM: /dev/shm/matrix_shards/layer24_mlp_intermediate.bin
Saving to local storage...
Saving matrix to binary file (OpenCL): matrix_shards/layer24_mlp_intermediate.bin
Original shape: (14336, 1)
Converted to 4D: (1, 1, 14336, 1)
Writing binary file...
Done.
Saving matrix to binary file (OpenCL): /dev/shm/matrix_shards/layer24_mlp_intermediate.bin
Original shape: (14336, 1)
Converted to 4D: (1, 1, 14336, 1)
Writing binary file...
Done.
Remote paths - RAM: /dev/shm/matrix_shards/layer24_mlp_intermediate.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/layer24_mlp_intermediate.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
üì§ Sent file layer24_mlp_intermediate.bin to 192.168.2.104
‚úÖ Received layer24_mlp_intermediate.bin 1/1
‚úÖ All ACKs received!
Sending to 192.168.2.101
üì§ Sent file layer24_mlp_intermediate.bin to 192.168.2.101
‚úÖ Received layer24_mlp_intermediate.bin 1/1
‚úÖ All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: layer24_mlp_intermediate
Matrix B: layers_24_mlp_down_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer24_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_24_mlp_down_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/layer24_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_24_mlp_down_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer24_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_24_mlp_down_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer24_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_24_mlp_down_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.54 seconds

üìä Result base: layers_24_mlp_down_proj_weightxlayer24_mlp_intermediate (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_24_mlp_down_proj_weightxlayer24_mlp_intermediate_combined.bin
  Original dims: [1, 1, 1, 4096]
  Result tensor shape: torch.Size([1, 4096]), size: 16,384 bytes
  Data range: [-0.324782, 0.291564]
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Preparing full matrix: input_token_embeddings.bin
Local paths - DISK: matrix_shards/input_token_embeddings.bin, RAM: /dev/shm/matrix_shards/input_token_embeddings.bin
Saving to local storage...
Saving matrix to binary file (OpenCL): matrix_shards/input_token_embeddings.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Saving matrix to binary file (OpenCL): /dev/shm/matrix_shards/input_token_embeddings.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Remote paths - RAM: /dev/shm/matrix_shards/input_token_embeddings.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/input_token_embeddings.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
üì§ Sent file input_token_embeddings.bin to 192.168.2.104
‚úÖ Received input_token_embeddings.bin 1/1
‚úÖ All ACKs received!
Sending to 192.168.2.101
üì§ Sent file input_token_embeddings.bin to 192.168.2.101
‚úÖ Received input_token_embeddings.bin 1/1
‚úÖ All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_25_self_attn_q_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_25_self_attn_q_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_25_self_attn_q_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_25_self_attn_k_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_25_self_attn_k_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_25_self_attn_k_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_25_self_attn_v_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_25_self_attn_v_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_25_self_attn_v_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embeddings
Matrix B: layers_25_self_attn_q_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_25_self_attn_q_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_25_self_attn_q_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_25_self_attn_q_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_25_self_attn_q_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.15 seconds

üìä Result base: layers_25_self_attn_q_proj_weightxinput_token_embeddings (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_25_self_attn_q_proj_weightxinput_token_embeddings_combined.bin
  Original dims: [1, 1, 1, 4096]
  Result tensor shape: torch.Size([1, 4096]), size: 16,384 bytes
  Data range: [-14.581866, 13.854811]

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embeddings
Matrix B: layers_25_self_attn_k_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_25_self_attn_k_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_25_self_attn_k_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_25_self_attn_k_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_25_self_attn_k_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.05 seconds

üìä Result base: layers_25_self_attn_k_proj_weightxinput_token_embeddings (send_back=True)
‚úÖ Received ACK_combined_matrix_saved 1/1
‚úÖ All ACKs received!
‚úÖ Loaded /dev/shm/matrix_shards/layers_25_self_attn_k_proj_weightxinput_token_embeddings_combined.bin
  Original dims: [1, 1, 1, 1024]
  Result tensor shape: torch.Size([1, 1024]), size: 4,096 bytes
  Data range: [-12.279051, 15.584866]

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embeddings
Matrix B: layers_25_self_attn_v_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_25_self_attn_v_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_25_self_attn_v_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_25_self_attn_v_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_25_self_attn_v_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.05 seconds

üìä Result base: layers_25_self_attn_v_proj_weightxinput_token_embeddings (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_25_self_attn_v_proj_weightxinput_token_embeddings_combined.bin
  Original dims: [1, 1, 1, 1024]
  Result tensor shape: torch.Size([1, 1024]), size: 4,096 bytes
  Data range: [-1.577998, 2.440543]
RoPE q shape: (32, 128)
RoPE k shape: (8, 128)
KV cache layer 25: K=(32, 25, 128) V=(32, 25, 128)
attn_output (per-head): (32, 128)
attn_hidden: (1, 4096)
residual: (1, 4096)
hidden_out(after attn+res): (1, 4096)
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Preparing full matrix: layer25_mlp_in.bin
Local paths - DISK: matrix_shards/layer25_mlp_in.bin, RAM: /dev/shm/matrix_shards/layer25_mlp_in.bin
Saving to local storage...
Saving matrix to binary file (OpenCL): matrix_shards/layer25_mlp_in.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Saving matrix to binary file (OpenCL): /dev/shm/matrix_shards/layer25_mlp_in.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Remote paths - RAM: /dev/shm/matrix_shards/layer25_mlp_in.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/layer25_mlp_in.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
üì§ Sent file layer25_mlp_in.bin to 192.168.2.104
‚úÖ Received layer25_mlp_in.bin 1/1
‚úÖ All ACKs received!
Sending to 192.168.2.101
üì§ Sent file layer25_mlp_in.bin to 192.168.2.101
‚úÖ Received layer25_mlp_in.bin 1/1
‚úÖ All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_25_mlp_gate_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_25_mlp_gate_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_25_mlp_gate_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_25_mlp_up_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_25_mlp_up_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_25_mlp_up_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_25_mlp_down_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_25_mlp_down_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_25_mlp_down_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: layer25_mlp_in
Matrix B: layers_25_mlp_gate_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer25_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_25_mlp_gate_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/layer25_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_25_mlp_gate_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer25_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_25_mlp_gate_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer25_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_25_mlp_gate_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.54 seconds

üìä Result base: layers_25_mlp_gate_proj_weightxlayer25_mlp_in (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_25_mlp_gate_proj_weightxlayer25_mlp_in_combined.bin
  Original dims: [1, 1, 1, 14336]
  Result tensor shape: torch.Size([1, 14336]), size: 57,344 bytes
  Data range: [-3.857003, 4.750349]

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: layer25_mlp_in
Matrix B: layers_25_mlp_up_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer25_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_25_mlp_up_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/layer25_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_25_mlp_up_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer25_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_25_mlp_up_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer25_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_25_mlp_up_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.54 seconds

üìä Result base: layers_25_mlp_up_proj_weightxlayer25_mlp_in (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_25_mlp_up_proj_weightxlayer25_mlp_in_combined.bin
  Original dims: [1, 1, 1, 14336]
  Result tensor shape: torch.Size([1, 14336]), size: 57,344 bytes
  Data range: [-1.755943, 1.599460]
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Preparing full matrix: layer25_mlp_intermediate.bin
Local paths - DISK: matrix_shards/layer25_mlp_intermediate.bin, RAM: /dev/shm/matrix_shards/layer25_mlp_intermediate.bin
Saving to local storage...
Saving matrix to binary file (OpenCL): matrix_shards/layer25_mlp_intermediate.bin
Original shape: (14336, 1)
Converted to 4D: (1, 1, 14336, 1)
Writing binary file...
Done.
Saving matrix to binary file (OpenCL): /dev/shm/matrix_shards/layer25_mlp_intermediate.bin
Original shape: (14336, 1)
Converted to 4D: (1, 1, 14336, 1)
Writing binary file...
Done.
Remote paths - RAM: /dev/shm/matrix_shards/layer25_mlp_intermediate.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/layer25_mlp_intermediate.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
üì§ Sent file layer25_mlp_intermediate.bin to 192.168.2.104
‚úÖ Received layer25_mlp_intermediate.bin 1/1
‚úÖ All ACKs received!
Sending to 192.168.2.101
üì§ Sent file layer25_mlp_intermediate.bin to 192.168.2.101
‚úÖ Received layer25_mlp_intermediate.bin 1/1
‚úÖ All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: layer25_mlp_intermediate
Matrix B: layers_25_mlp_down_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer25_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_25_mlp_down_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/layer25_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_25_mlp_down_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer25_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_25_mlp_down_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer25_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_25_mlp_down_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.54 seconds

üìä Result base: layers_25_mlp_down_proj_weightxlayer25_mlp_intermediate (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_25_mlp_down_proj_weightxlayer25_mlp_intermediate_combined.bin
  Original dims: [1, 1, 1, 4096]
  Result tensor shape: torch.Size([1, 4096]), size: 16,384 bytes
  Data range: [-0.474179, 0.325014]
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Preparing full matrix: input_token_embeddings.bin
Local paths - DISK: matrix_shards/input_token_embeddings.bin, RAM: /dev/shm/matrix_shards/input_token_embeddings.bin
Saving to local storage...
Saving matrix to binary file (OpenCL): matrix_shards/input_token_embeddings.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Saving matrix to binary file (OpenCL): /dev/shm/matrix_shards/input_token_embeddings.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Remote paths - RAM: /dev/shm/matrix_shards/input_token_embeddings.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/input_token_embeddings.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
üì§ Sent file input_token_embeddings.bin to 192.168.2.104
‚úÖ Received input_token_embeddings.bin 1/1
‚úÖ All ACKs received!
Sending to 192.168.2.101
üì§ Sent file input_token_embeddings.bin to 192.168.2.101
‚úÖ Received input_token_embeddings.bin 1/1
‚úÖ All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_26_self_attn_q_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_26_self_attn_q_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_26_self_attn_q_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_26_self_attn_k_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_26_self_attn_k_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_26_self_attn_k_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_26_self_attn_v_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_26_self_attn_v_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_26_self_attn_v_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embeddings
Matrix B: layers_26_self_attn_q_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_26_self_attn_q_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_26_self_attn_q_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_26_self_attn_q_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_26_self_attn_q_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.16 seconds

üìä Result base: layers_26_self_attn_q_proj_weightxinput_token_embeddings (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_26_self_attn_q_proj_weightxinput_token_embeddings_combined.bin
  Original dims: [1, 1, 1, 4096]
  Result tensor shape: torch.Size([1, 4096]), size: 16,384 bytes
  Data range: [-13.425562, 11.544291]

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embeddings
Matrix B: layers_26_self_attn_k_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_26_self_attn_k_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_26_self_attn_k_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_26_self_attn_k_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_26_self_attn_k_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.05 seconds

üìä Result base: layers_26_self_attn_k_proj_weightxinput_token_embeddings (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_26_self_attn_k_proj_weightxinput_token_embeddings_combined.bin
  Original dims: [1, 1, 1, 1024]
  Result tensor shape: torch.Size([1, 1024]), size: 4,096 bytes
  Data range: [-12.775545, 14.188286]

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embeddings
Matrix B: layers_26_self_attn_v_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_26_self_attn_v_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_26_self_attn_v_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_26_self_attn_v_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_26_self_attn_v_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.05 seconds

üìä Result base: layers_26_self_attn_v_proj_weightxinput_token_embeddings (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_26_self_attn_v_proj_weightxinput_token_embeddings_combined.bin
  Original dims: [1, 1, 1, 1024]
  Result tensor shape: torch.Size([1, 1024]), size: 4,096 bytes
  Data range: [-1.177308, 0.975411]
RoPE q shape: (32, 128)
RoPE k shape: (8, 128)
KV cache layer 26: K=(32, 25, 128) V=(32, 25, 128)
attn_output (per-head): (32, 128)
attn_hidden: (1, 4096)
residual: (1, 4096)
hidden_out(after attn+res): (1, 4096)
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Preparing full matrix: layer26_mlp_in.bin
Local paths - DISK: matrix_shards/layer26_mlp_in.bin, RAM: /dev/shm/matrix_shards/layer26_mlp_in.bin
Saving to local storage...
Saving matrix to binary file (OpenCL): matrix_shards/layer26_mlp_in.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Saving matrix to binary file (OpenCL): /dev/shm/matrix_shards/layer26_mlp_in.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Remote paths - RAM: /dev/shm/matrix_shards/layer26_mlp_in.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/layer26_mlp_in.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
üì§ Sent file layer26_mlp_in.bin to 192.168.2.104
‚úÖ Received layer26_mlp_in.bin 1/1
‚úÖ All ACKs received!
Sending to 192.168.2.101
üì§ Sent file layer26_mlp_in.bin to 192.168.2.101
‚úÖ Received layer26_mlp_in.bin 1/1
‚úÖ All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_26_mlp_gate_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_26_mlp_gate_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_26_mlp_gate_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_26_mlp_up_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_26_mlp_up_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_26_mlp_up_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_26_mlp_down_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_26_mlp_down_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_26_mlp_down_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: layer26_mlp_in
Matrix B: layers_26_mlp_gate_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer26_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_26_mlp_gate_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/layer26_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_26_mlp_gate_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer26_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_26_mlp_gate_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer26_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_26_mlp_gate_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.54 seconds

üìä Result base: layers_26_mlp_gate_proj_weightxlayer26_mlp_in (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_26_mlp_gate_proj_weightxlayer26_mlp_in_combined.bin
  Original dims: [1, 1, 1, 14336]
  Result tensor shape: torch.Size([1, 14336]), size: 57,344 bytes
  Data range: [-4.107375, 2.957542]

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: layer26_mlp_in
Matrix B: layers_26_mlp_up_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer26_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_26_mlp_up_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/layer26_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_26_mlp_up_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer26_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_26_mlp_up_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer26_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_26_mlp_up_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.55 seconds

üìä Result base: layers_26_mlp_up_proj_weightxlayer26_mlp_in (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_26_mlp_up_proj_weightxlayer26_mlp_in_combined.bin
  Original dims: [1, 1, 1, 14336]
  Result tensor shape: torch.Size([1, 14336]), size: 57,344 bytes
  Data range: [-2.997656, 2.087140]
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Preparing full matrix: layer26_mlp_intermediate.bin
Local paths - DISK: matrix_shards/layer26_mlp_intermediate.bin, RAM: /dev/shm/matrix_shards/layer26_mlp_intermediate.bin
Saving to local storage...
Saving matrix to binary file (OpenCL): matrix_shards/layer26_mlp_intermediate.bin
Original shape: (14336, 1)
Converted to 4D: (1, 1, 14336, 1)
Writing binary file...
Done.
Saving matrix to binary file (OpenCL): /dev/shm/matrix_shards/layer26_mlp_intermediate.bin
Original shape: (14336, 1)
Converted to 4D: (1, 1, 14336, 1)
Writing binary file...
Done.
Remote paths - RAM: /dev/shm/matrix_shards/layer26_mlp_intermediate.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/layer26_mlp_intermediate.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
üì§ Sent file layer26_mlp_intermediate.bin to 192.168.2.104
‚úÖ Received layer26_mlp_intermediate.bin 1/1
‚úÖ All ACKs received!
Sending to 192.168.2.101
üì§ Sent file layer26_mlp_intermediate.bin to 192.168.2.101
‚úÖ Received layer26_mlp_intermediate.bin 1/1
‚úÖ All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: layer26_mlp_intermediate
Matrix B: layers_26_mlp_down_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer26_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_26_mlp_down_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/layer26_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_26_mlp_down_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer26_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_26_mlp_down_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer26_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_26_mlp_down_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.53 seconds

üìä Result base: layers_26_mlp_down_proj_weightxlayer26_mlp_intermediate (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_26_mlp_down_proj_weightxlayer26_mlp_intermediate_combined.bin
  Original dims: [1, 1, 1, 4096]
  Result tensor shape: torch.Size([1, 4096]), size: 16,384 bytes
  Data range: [-1.268093, 0.375814]
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Preparing full matrix: input_token_embeddings.bin
Local paths - DISK: matrix_shards/input_token_embeddings.bin, RAM: /dev/shm/matrix_shards/input_token_embeddings.bin
Saving to local storage...
Saving matrix to binary file (OpenCL): matrix_shards/input_token_embeddings.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Saving matrix to binary file (OpenCL): /dev/shm/matrix_shards/input_token_embeddings.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Remote paths - RAM: /dev/shm/matrix_shards/input_token_embeddings.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/input_token_embeddings.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
üì§ Sent file input_token_embeddings.bin to 192.168.2.104
‚úÖ Received input_token_embeddings.bin 1/1
‚úÖ All ACKs received!
Sending to 192.168.2.101
üì§ Sent file input_token_embeddings.bin to 192.168.2.101
‚úÖ Received input_token_embeddings.bin 1/1
‚úÖ All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_27_self_attn_q_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_27_self_attn_q_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_27_self_attn_q_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_27_self_attn_k_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_27_self_attn_k_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_27_self_attn_k_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_27_self_attn_v_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_27_self_attn_v_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_27_self_attn_v_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embeddings
Matrix B: layers_27_self_attn_q_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_27_self_attn_q_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_27_self_attn_q_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_27_self_attn_q_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_27_self_attn_q_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.16 seconds

üìä Result base: layers_27_self_attn_q_proj_weightxinput_token_embeddings (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_27_self_attn_q_proj_weightxinput_token_embeddings_combined.bin
  Original dims: [1, 1, 1, 4096]
  Result tensor shape: torch.Size([1, 4096]), size: 16,384 bytes
  Data range: [-12.321442, 12.839450]

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embeddings
Matrix B: layers_27_self_attn_k_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_27_self_attn_k_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_27_self_attn_k_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_27_self_attn_k_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_27_self_attn_k_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.05 seconds

üìä Result base: layers_27_self_attn_k_proj_weightxinput_token_embeddings (send_back=True)
‚úÖ Received ACK_combined_matrix_saved 1/1
‚úÖ All ACKs received!
‚úÖ Loaded /dev/shm/matrix_shards/layers_27_self_attn_k_proj_weightxinput_token_embeddings_combined.bin
  Original dims: [1, 1, 1, 1024]
  Result tensor shape: torch.Size([1, 1024]), size: 4,096 bytes
  Data range: [-14.532187, 10.781570]

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embeddings
Matrix B: layers_27_self_attn_v_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_27_self_attn_v_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_27_self_attn_v_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_27_self_attn_v_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_27_self_attn_v_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.05 seconds

üìä Result base: layers_27_self_attn_v_proj_weightxinput_token_embeddings (send_back=True)
‚úÖ Received ACK_combined_matrix_saved 1/1
‚úÖ All ACKs received!
‚úÖ Loaded /dev/shm/matrix_shards/layers_27_self_attn_v_proj_weightxinput_token_embeddings_combined.bin
  Original dims: [1, 1, 1, 1024]
  Result tensor shape: torch.Size([1, 1024]), size: 4,096 bytes
  Data range: [-2.098495, 2.012249]
RoPE q shape: (32, 128)
RoPE k shape: (8, 128)
KV cache layer 27: K=(32, 25, 128) V=(32, 25, 128)
attn_output (per-head): (32, 128)
attn_hidden: (1, 4096)
residual: (1, 4096)
hidden_out(after attn+res): (1, 4096)
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Preparing full matrix: layer27_mlp_in.bin
Local paths - DISK: matrix_shards/layer27_mlp_in.bin, RAM: /dev/shm/matrix_shards/layer27_mlp_in.bin
Saving to local storage...
Saving matrix to binary file (OpenCL): matrix_shards/layer27_mlp_in.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Saving matrix to binary file (OpenCL): /dev/shm/matrix_shards/layer27_mlp_in.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Remote paths - RAM: /dev/shm/matrix_shards/layer27_mlp_in.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/layer27_mlp_in.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
üì§ Sent file layer27_mlp_in.bin to 192.168.2.104
‚úÖ Received layer27_mlp_in.bin 1/1
‚úÖ All ACKs received!
Sending to 192.168.2.101
üì§ Sent file layer27_mlp_in.bin to 192.168.2.101
‚úÖ Received layer27_mlp_in.bin 1/1
‚úÖ All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_27_mlp_gate_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_27_mlp_gate_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_27_mlp_gate_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_27_mlp_up_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_27_mlp_up_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_27_mlp_up_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_27_mlp_down_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_27_mlp_down_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_27_mlp_down_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: layer27_mlp_in
Matrix B: layers_27_mlp_gate_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer27_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_27_mlp_gate_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/layer27_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_27_mlp_gate_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer27_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_27_mlp_gate_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer27_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_27_mlp_gate_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.54 seconds

üìä Result base: layers_27_mlp_gate_proj_weightxlayer27_mlp_in (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_27_mlp_gate_proj_weightxlayer27_mlp_in_combined.bin
  Original dims: [1, 1, 1, 14336]
  Result tensor shape: torch.Size([1, 14336]), size: 57,344 bytes
  Data range: [-4.588778, 3.371840]

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: layer27_mlp_in
Matrix B: layers_27_mlp_up_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer27_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_27_mlp_up_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/layer27_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_27_mlp_up_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer27_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_27_mlp_up_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer27_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_27_mlp_up_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.54 seconds

üìä Result base: layers_27_mlp_up_proj_weightxlayer27_mlp_in (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_27_mlp_up_proj_weightxlayer27_mlp_in_combined.bin
  Original dims: [1, 1, 1, 14336]
  Result tensor shape: torch.Size([1, 14336]), size: 57,344 bytes
  Data range: [-4.171722, 1.764712]
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Preparing full matrix: layer27_mlp_intermediate.bin
Local paths - DISK: matrix_shards/layer27_mlp_intermediate.bin, RAM: /dev/shm/matrix_shards/layer27_mlp_intermediate.bin
Saving to local storage...
Saving matrix to binary file (OpenCL): matrix_shards/layer27_mlp_intermediate.bin
Original shape: (14336, 1)
Converted to 4D: (1, 1, 14336, 1)
Writing binary file...
Done.
Saving matrix to binary file (OpenCL): /dev/shm/matrix_shards/layer27_mlp_intermediate.bin
Original shape: (14336, 1)
Converted to 4D: (1, 1, 14336, 1)
Writing binary file...
Done.
Remote paths - RAM: /dev/shm/matrix_shards/layer27_mlp_intermediate.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/layer27_mlp_intermediate.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
üì§ Sent file layer27_mlp_intermediate.bin to 192.168.2.104
‚úÖ Received layer27_mlp_intermediate.bin 1/1
‚úÖ All ACKs received!
Sending to 192.168.2.101
üì§ Sent file layer27_mlp_intermediate.bin to 192.168.2.101
‚úÖ Received layer27_mlp_intermediate.bin 1/1
‚úÖ All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: layer27_mlp_intermediate
Matrix B: layers_27_mlp_down_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer27_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_27_mlp_down_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/layer27_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_27_mlp_down_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer27_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_27_mlp_down_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer27_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_27_mlp_down_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.55 seconds

üìä Result base: layers_27_mlp_down_proj_weightxlayer27_mlp_intermediate (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_27_mlp_down_proj_weightxlayer27_mlp_intermediate_combined.bin
  Original dims: [1, 1, 1, 4096]
  Result tensor shape: torch.Size([1, 4096]), size: 16,384 bytes
  Data range: [-0.955280, 0.370265]
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Preparing full matrix: input_token_embeddings.bin
Local paths - DISK: matrix_shards/input_token_embeddings.bin, RAM: /dev/shm/matrix_shards/input_token_embeddings.bin
Saving to local storage...
Saving matrix to binary file (OpenCL): matrix_shards/input_token_embeddings.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Saving matrix to binary file (OpenCL): /dev/shm/matrix_shards/input_token_embeddings.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Remote paths - RAM: /dev/shm/matrix_shards/input_token_embeddings.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/input_token_embeddings.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
üì§ Sent file input_token_embeddings.bin to 192.168.2.104
‚úÖ Received input_token_embeddings.bin 1/1
‚úÖ All ACKs received!
Sending to 192.168.2.101
üì§ Sent file input_token_embeddings.bin to 192.168.2.101
‚úÖ Received input_token_embeddings.bin 1/1
‚úÖ All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_28_self_attn_q_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_28_self_attn_q_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_28_self_attn_q_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_28_self_attn_k_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_28_self_attn_k_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_28_self_attn_k_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_28_self_attn_v_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_28_self_attn_v_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_28_self_attn_v_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embeddings
Matrix B: layers_28_self_attn_q_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_self_attn_q_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_self_attn_q_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_self_attn_q_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_self_attn_q_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.17 seconds

üìä Result base: layers_28_self_attn_q_proj_weightxinput_token_embeddings (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_28_self_attn_q_proj_weightxinput_token_embeddings_combined.bin
  Original dims: [1, 1, 1, 4096]
  Result tensor shape: torch.Size([1, 4096]), size: 16,384 bytes
  Data range: [-10.573003, 11.332829]

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embeddings
Matrix B: layers_28_self_attn_k_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_self_attn_k_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_self_attn_k_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_self_attn_k_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_self_attn_k_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.06 seconds

üìä Result base: layers_28_self_attn_k_proj_weightxinput_token_embeddings (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_28_self_attn_k_proj_weightxinput_token_embeddings_combined.bin
  Original dims: [1, 1, 1, 1024]
  Result tensor shape: torch.Size([1, 1024]), size: 4,096 bytes
  Data range: [-14.925938, 13.575823]

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embeddings
Matrix B: layers_28_self_attn_v_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_self_attn_v_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_self_attn_v_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_self_attn_v_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_self_attn_v_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.05 seconds

üìä Result base: layers_28_self_attn_v_proj_weightxinput_token_embeddings (send_back=True)
‚úÖ Received ACK_combined_matrix_saved 1/1
‚úÖ All ACKs received!
‚úÖ Loaded /dev/shm/matrix_shards/layers_28_self_attn_v_proj_weightxinput_token_embeddings_combined.bin
  Original dims: [1, 1, 1, 1024]
  Result tensor shape: torch.Size([1, 1024]), size: 4,096 bytes
  Data range: [-1.488743, 1.363684]
RoPE q shape: (32, 128)
RoPE k shape: (8, 128)
KV cache layer 28: K=(32, 25, 128) V=(32, 25, 128)
attn_output (per-head): (32, 128)
attn_hidden: (1, 4096)
residual: (1, 4096)
hidden_out(after attn+res): (1, 4096)
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Preparing full matrix: layer28_mlp_in.bin
Local paths - DISK: matrix_shards/layer28_mlp_in.bin, RAM: /dev/shm/matrix_shards/layer28_mlp_in.bin
Saving to local storage...
Saving matrix to binary file (OpenCL): matrix_shards/layer28_mlp_in.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Saving matrix to binary file (OpenCL): /dev/shm/matrix_shards/layer28_mlp_in.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Remote paths - RAM: /dev/shm/matrix_shards/layer28_mlp_in.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/layer28_mlp_in.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
üì§ Sent file layer28_mlp_in.bin to 192.168.2.104
‚úÖ Received layer28_mlp_in.bin 1/1
‚úÖ All ACKs received!
Sending to 192.168.2.101
üì§ Sent file layer28_mlp_in.bin to 192.168.2.101
‚úÖ Received layer28_mlp_in.bin 1/1
‚úÖ All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_28_mlp_gate_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_28_mlp_gate_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_28_mlp_gate_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_28_mlp_up_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_28_mlp_up_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_28_mlp_up_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_28_mlp_down_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_28_mlp_down_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_28_mlp_down_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: layer28_mlp_in
Matrix B: layers_28_mlp_gate_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer28_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_mlp_gate_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/layer28_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_mlp_gate_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer28_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_mlp_gate_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer28_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_mlp_gate_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.54 seconds

üìä Result base: layers_28_mlp_gate_proj_weightxlayer28_mlp_in (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_28_mlp_gate_proj_weightxlayer28_mlp_in_combined.bin
  Original dims: [1, 1, 1, 14336]
  Result tensor shape: torch.Size([1, 14336]), size: 57,344 bytes
  Data range: [-6.048700, 3.417745]

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: layer28_mlp_in
Matrix B: layers_28_mlp_up_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer28_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_mlp_up_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/layer28_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_mlp_up_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer28_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_mlp_up_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer28_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_mlp_up_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.53 seconds

üìä Result base: layers_28_mlp_up_proj_weightxlayer28_mlp_in (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_28_mlp_up_proj_weightxlayer28_mlp_in_combined.bin
  Original dims: [1, 1, 1, 14336]
  Result tensor shape: torch.Size([1, 14336]), size: 57,344 bytes
  Data range: [-3.050683, 3.186635]
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Preparing full matrix: layer28_mlp_intermediate.bin
Local paths - DISK: matrix_shards/layer28_mlp_intermediate.bin, RAM: /dev/shm/matrix_shards/layer28_mlp_intermediate.bin
Saving to local storage...
Saving matrix to binary file (OpenCL): matrix_shards/layer28_mlp_intermediate.bin
Original shape: (14336, 1)
Converted to 4D: (1, 1, 14336, 1)
Writing binary file...
Done.
Saving matrix to binary file (OpenCL): /dev/shm/matrix_shards/layer28_mlp_intermediate.bin
Original shape: (14336, 1)
Converted to 4D: (1, 1, 14336, 1)
Writing binary file...
Done.
Remote paths - RAM: /dev/shm/matrix_shards/layer28_mlp_intermediate.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/layer28_mlp_intermediate.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
üì§ Sent file layer28_mlp_intermediate.bin to 192.168.2.104
‚úÖ Received layer28_mlp_intermediate.bin 1/1
‚úÖ All ACKs received!
Sending to 192.168.2.101
üì§ Sent file layer28_mlp_intermediate.bin to 192.168.2.101
‚úÖ Received layer28_mlp_intermediate.bin 1/1
‚úÖ All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: layer28_mlp_intermediate
Matrix B: layers_28_mlp_down_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer28_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_mlp_down_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/layer28_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_mlp_down_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer28_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_mlp_down_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer28_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_mlp_down_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.54 seconds

üìä Result base: layers_28_mlp_down_proj_weightxlayer28_mlp_intermediate (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_28_mlp_down_proj_weightxlayer28_mlp_intermediate_combined.bin
  Original dims: [1, 1, 1, 4096]
  Result tensor shape: torch.Size([1, 4096]), size: 16,384 bytes
  Data range: [-1.698600, 0.545608]
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Preparing full matrix: input_token_embeddings.bin
Local paths - DISK: matrix_shards/input_token_embeddings.bin, RAM: /dev/shm/matrix_shards/input_token_embeddings.bin
Saving to local storage...
Saving matrix to binary file (OpenCL): matrix_shards/input_token_embeddings.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Saving matrix to binary file (OpenCL): /dev/shm/matrix_shards/input_token_embeddings.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Remote paths - RAM: /dev/shm/matrix_shards/input_token_embeddings.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/input_token_embeddings.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
üì§ Sent file input_token_embeddings.bin to 192.168.2.104
‚úÖ Received input_token_embeddings.bin 1/1
‚úÖ All ACKs received!
Sending to 192.168.2.101
üì§ Sent file input_token_embeddings.bin to 192.168.2.101
‚úÖ Received input_token_embeddings.bin 1/1
‚úÖ All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_29_self_attn_q_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_29_self_attn_q_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_29_self_attn_q_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_29_self_attn_k_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_29_self_attn_k_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_29_self_attn_k_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_29_self_attn_v_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_29_self_attn_v_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_29_self_attn_v_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embeddings
Matrix B: layers_29_self_attn_q_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_self_attn_q_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_self_attn_q_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_self_attn_q_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_self_attn_q_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.17 seconds

üìä Result base: layers_29_self_attn_q_proj_weightxinput_token_embeddings (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_29_self_attn_q_proj_weightxinput_token_embeddings_combined.bin
  Original dims: [1, 1, 1, 4096]
  Result tensor shape: torch.Size([1, 4096]), size: 16,384 bytes
  Data range: [-9.307880, 9.095924]

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embeddings
Matrix B: layers_29_self_attn_k_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_self_attn_k_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_self_attn_k_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_self_attn_k_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_self_attn_k_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.06 seconds

üìä Result base: layers_29_self_attn_k_proj_weightxinput_token_embeddings (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_29_self_attn_k_proj_weightxinput_token_embeddings_combined.bin
  Original dims: [1, 1, 1, 1024]
  Result tensor shape: torch.Size([1, 1024]), size: 4,096 bytes
  Data range: [-15.399681, 27.883986]

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embeddings
Matrix B: layers_29_self_attn_v_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_self_attn_v_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_self_attn_v_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_self_attn_v_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_self_attn_v_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.06 seconds

üìä Result base: layers_29_self_attn_v_proj_weightxinput_token_embeddings (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_29_self_attn_v_proj_weightxinput_token_embeddings_combined.bin
  Original dims: [1, 1, 1, 1024]
  Result tensor shape: torch.Size([1, 1024]), size: 4,096 bytes
  Data range: [-2.085066, 3.002701]
RoPE q shape: (32, 128)
RoPE k shape: (8, 128)
KV cache layer 29: K=(32, 25, 128) V=(32, 25, 128)
attn_output (per-head): (32, 128)
attn_hidden: (1, 4096)
residual: (1, 4096)
hidden_out(after attn+res): (1, 4096)
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Preparing full matrix: layer29_mlp_in.bin
Local paths - DISK: matrix_shards/layer29_mlp_in.bin, RAM: /dev/shm/matrix_shards/layer29_mlp_in.bin
Saving to local storage...
Saving matrix to binary file (OpenCL): matrix_shards/layer29_mlp_in.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Saving matrix to binary file (OpenCL): /dev/shm/matrix_shards/layer29_mlp_in.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Remote paths - RAM: /dev/shm/matrix_shards/layer29_mlp_in.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/layer29_mlp_in.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
üì§ Sent file layer29_mlp_in.bin to 192.168.2.104
‚úÖ Received layer29_mlp_in.bin 1/1
‚úÖ All ACKs received!
Sending to 192.168.2.101
üì§ Sent file layer29_mlp_in.bin to 192.168.2.101
‚úÖ Received layer29_mlp_in.bin 1/1
‚úÖ All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_29_mlp_gate_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_29_mlp_gate_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_29_mlp_gate_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_29_mlp_up_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_29_mlp_up_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_29_mlp_up_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_29_mlp_down_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_29_mlp_down_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_29_mlp_down_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: layer29_mlp_in
Matrix B: layers_29_mlp_gate_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer29_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_mlp_gate_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/layer29_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_mlp_gate_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer29_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_mlp_gate_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer29_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_mlp_gate_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.54 seconds

üìä Result base: layers_29_mlp_gate_proj_weightxlayer29_mlp_in (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_29_mlp_gate_proj_weightxlayer29_mlp_in_combined.bin
  Original dims: [1, 1, 1, 14336]
  Result tensor shape: torch.Size([1, 14336]), size: 57,344 bytes
  Data range: [-6.790711, 4.404361]

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: layer29_mlp_in
Matrix B: layers_29_mlp_up_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer29_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_mlp_up_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/layer29_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_mlp_up_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer29_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_mlp_up_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer29_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_mlp_up_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.53 seconds

üìä Result base: layers_29_mlp_up_proj_weightxlayer29_mlp_in (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_29_mlp_up_proj_weightxlayer29_mlp_in_combined.bin
  Original dims: [1, 1, 1, 14336]
  Result tensor shape: torch.Size([1, 14336]), size: 57,344 bytes
  Data range: [-2.720069, 9.729840]
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Preparing full matrix: layer29_mlp_intermediate.bin
Local paths - DISK: matrix_shards/layer29_mlp_intermediate.bin, RAM: /dev/shm/matrix_shards/layer29_mlp_intermediate.bin
Saving to local storage...
Saving matrix to binary file (OpenCL): matrix_shards/layer29_mlp_intermediate.bin
Original shape: (14336, 1)
Converted to 4D: (1, 1, 14336, 1)
Writing binary file...
Done.
Saving matrix to binary file (OpenCL): /dev/shm/matrix_shards/layer29_mlp_intermediate.bin
Original shape: (14336, 1)
Converted to 4D: (1, 1, 14336, 1)
Writing binary file...
Done.
Remote paths - RAM: /dev/shm/matrix_shards/layer29_mlp_intermediate.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/layer29_mlp_intermediate.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
üì§ Sent file layer29_mlp_intermediate.bin to 192.168.2.104
‚úÖ Received layer29_mlp_intermediate.bin 1/1
‚úÖ All ACKs received!
Sending to 192.168.2.101
üì§ Sent file layer29_mlp_intermediate.bin to 192.168.2.101
‚úÖ Received layer29_mlp_intermediate.bin 1/1
‚úÖ All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: layer29_mlp_intermediate
Matrix B: layers_29_mlp_down_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer29_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_mlp_down_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/layer29_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_mlp_down_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer29_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_mlp_down_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer29_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_mlp_down_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.54 seconds

üìä Result base: layers_29_mlp_down_proj_weightxlayer29_mlp_intermediate (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_29_mlp_down_proj_weightxlayer29_mlp_intermediate_combined.bin
  Original dims: [1, 1, 1, 4096]
  Result tensor shape: torch.Size([1, 4096]), size: 16,384 bytes
  Data range: [-2.318924, 0.546386]
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Preparing full matrix: input_token_embeddings.bin
Local paths - DISK: matrix_shards/input_token_embeddings.bin, RAM: /dev/shm/matrix_shards/input_token_embeddings.bin
Saving to local storage...
Saving matrix to binary file (OpenCL): matrix_shards/input_token_embeddings.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Saving matrix to binary file (OpenCL): /dev/shm/matrix_shards/input_token_embeddings.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Remote paths - RAM: /dev/shm/matrix_shards/input_token_embeddings.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/input_token_embeddings.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
üì§ Sent file input_token_embeddings.bin to 192.168.2.104
‚úÖ Received input_token_embeddings.bin 1/1
‚úÖ All ACKs received!
Sending to 192.168.2.101
üì§ Sent file input_token_embeddings.bin to 192.168.2.101
‚úÖ Received input_token_embeddings.bin 1/1
‚úÖ All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_30_self_attn_q_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_30_self_attn_q_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_30_self_attn_q_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_30_self_attn_k_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_30_self_attn_k_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_30_self_attn_k_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_30_self_attn_v_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_30_self_attn_v_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_30_self_attn_v_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embeddings
Matrix B: layers_30_self_attn_q_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_self_attn_q_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_self_attn_q_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_self_attn_q_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_self_attn_q_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.15 seconds

üìä Result base: layers_30_self_attn_q_proj_weightxinput_token_embeddings (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_30_self_attn_q_proj_weightxinput_token_embeddings_combined.bin
  Original dims: [1, 1, 1, 4096]
  Result tensor shape: torch.Size([1, 4096]), size: 16,384 bytes
  Data range: [-13.551665, 8.571438]

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embeddings
Matrix B: layers_30_self_attn_k_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_self_attn_k_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_self_attn_k_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_self_attn_k_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_self_attn_k_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.05 seconds

üìä Result base: layers_30_self_attn_k_proj_weightxinput_token_embeddings (send_back=True)
‚úÖ Received ACK_combined_matrix_saved 1/1
‚úÖ All ACKs received!
‚úÖ Loaded /dev/shm/matrix_shards/layers_30_self_attn_k_proj_weightxinput_token_embeddings_combined.bin
  Original dims: [1, 1, 1, 1024]
  Result tensor shape: torch.Size([1, 1024]), size: 4,096 bytes
  Data range: [-13.419723, 12.219686]

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embeddings
Matrix B: layers_30_self_attn_v_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_self_attn_v_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_self_attn_v_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_self_attn_v_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_self_attn_v_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.05 seconds

üìä Result base: layers_30_self_attn_v_proj_weightxinput_token_embeddings (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_30_self_attn_v_proj_weightxinput_token_embeddings_combined.bin
  Original dims: [1, 1, 1, 1024]
  Result tensor shape: torch.Size([1, 1024]), size: 4,096 bytes
  Data range: [-2.015151, 1.786360]
RoPE q shape: (32, 128)
RoPE k shape: (8, 128)
KV cache layer 30: K=(32, 25, 128) V=(32, 25, 128)
attn_output (per-head): (32, 128)
attn_hidden: (1, 4096)
residual: (1, 4096)
hidden_out(after attn+res): (1, 4096)
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Preparing full matrix: layer30_mlp_in.bin
Local paths - DISK: matrix_shards/layer30_mlp_in.bin, RAM: /dev/shm/matrix_shards/layer30_mlp_in.bin
Saving to local storage...
Saving matrix to binary file (OpenCL): matrix_shards/layer30_mlp_in.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Saving matrix to binary file (OpenCL): /dev/shm/matrix_shards/layer30_mlp_in.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Remote paths - RAM: /dev/shm/matrix_shards/layer30_mlp_in.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/layer30_mlp_in.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
üì§ Sent file layer30_mlp_in.bin to 192.168.2.104
‚úÖ Received layer30_mlp_in.bin 1/1
‚úÖ All ACKs received!
Sending to 192.168.2.101
üì§ Sent file layer30_mlp_in.bin to 192.168.2.101
‚úÖ Received layer30_mlp_in.bin 1/1
‚úÖ All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_30_mlp_gate_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_30_mlp_gate_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_30_mlp_gate_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_30_mlp_up_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_30_mlp_up_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_30_mlp_up_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_30_mlp_down_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_30_mlp_down_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_30_mlp_down_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: layer30_mlp_in
Matrix B: layers_30_mlp_gate_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer30_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_mlp_gate_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/layer30_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_mlp_gate_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer30_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_mlp_gate_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer30_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_mlp_gate_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.55 seconds

üìä Result base: layers_30_mlp_gate_proj_weightxlayer30_mlp_in (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_30_mlp_gate_proj_weightxlayer30_mlp_in_combined.bin
  Original dims: [1, 1, 1, 14336]
  Result tensor shape: torch.Size([1, 14336]), size: 57,344 bytes
  Data range: [-6.627398, 7.243565]

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: layer30_mlp_in
Matrix B: layers_30_mlp_up_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer30_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_mlp_up_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/layer30_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_mlp_up_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer30_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_mlp_up_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer30_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_mlp_up_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.54 seconds

üìä Result base: layers_30_mlp_up_proj_weightxlayer30_mlp_in (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_30_mlp_up_proj_weightxlayer30_mlp_in_combined.bin
  Original dims: [1, 1, 1, 14336]
  Result tensor shape: torch.Size([1, 14336]), size: 57,344 bytes
  Data range: [-7.250746, 8.776639]
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Preparing full matrix: layer30_mlp_intermediate.bin
Local paths - DISK: matrix_shards/layer30_mlp_intermediate.bin, RAM: /dev/shm/matrix_shards/layer30_mlp_intermediate.bin
Saving to local storage...
Saving matrix to binary file (OpenCL): matrix_shards/layer30_mlp_intermediate.bin
Original shape: (14336, 1)
Converted to 4D: (1, 1, 14336, 1)
Writing binary file...
Done.
Saving matrix to binary file (OpenCL): /dev/shm/matrix_shards/layer30_mlp_intermediate.bin
Original shape: (14336, 1)
Converted to 4D: (1, 1, 14336, 1)
Writing binary file...
Done.
Remote paths - RAM: /dev/shm/matrix_shards/layer30_mlp_intermediate.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/layer30_mlp_intermediate.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
üì§ Sent file layer30_mlp_intermediate.bin to 192.168.2.104
‚úÖ Received layer30_mlp_intermediate.bin 1/1
‚úÖ All ACKs received!
Sending to 192.168.2.101
üì§ Sent file layer30_mlp_intermediate.bin to 192.168.2.101
‚úÖ Received layer30_mlp_intermediate.bin 1/1
‚úÖ All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: layer30_mlp_intermediate
Matrix B: layers_30_mlp_down_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer30_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_mlp_down_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/layer30_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_mlp_down_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer30_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_mlp_down_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer30_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_mlp_down_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.54 seconds

üìä Result base: layers_30_mlp_down_proj_weightxlayer30_mlp_intermediate (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_30_mlp_down_proj_weightxlayer30_mlp_intermediate_combined.bin
  Original dims: [1, 1, 1, 4096]
  Result tensor shape: torch.Size([1, 4096]), size: 16,384 bytes
  Data range: [-1.086993, 1.565067]
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Preparing full matrix: input_token_embeddings.bin
Local paths - DISK: matrix_shards/input_token_embeddings.bin, RAM: /dev/shm/matrix_shards/input_token_embeddings.bin
Saving to local storage...
Saving matrix to binary file (OpenCL): matrix_shards/input_token_embeddings.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Saving matrix to binary file (OpenCL): /dev/shm/matrix_shards/input_token_embeddings.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Remote paths - RAM: /dev/shm/matrix_shards/input_token_embeddings.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/input_token_embeddings.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
üì§ Sent file input_token_embeddings.bin to 192.168.2.104
‚úÖ Received input_token_embeddings.bin 1/1
‚úÖ All ACKs received!
Sending to 192.168.2.101
üì§ Sent file input_token_embeddings.bin to 192.168.2.101
‚úÖ Received input_token_embeddings.bin 1/1
‚úÖ All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_31_self_attn_q_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_31_self_attn_q_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_31_self_attn_q_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_31_self_attn_k_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_31_self_attn_k_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_31_self_attn_k_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_31_self_attn_v_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_31_self_attn_v_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_31_self_attn_v_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embeddings
Matrix B: layers_31_self_attn_q_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_self_attn_q_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_self_attn_q_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_self_attn_q_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_self_attn_q_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.19 seconds

üìä Result base: layers_31_self_attn_q_proj_weightxinput_token_embeddings (send_back=True)
‚úÖ Received ACK_combined_matrix_saved 1/1
‚úÖ All ACKs received!
‚úÖ Loaded /dev/shm/matrix_shards/layers_31_self_attn_q_proj_weightxinput_token_embeddings_combined.bin
  Original dims: [1, 1, 1, 4096]
  Result tensor shape: torch.Size([1, 4096]), size: 16,384 bytes
  Data range: [-11.355783, 11.953050]

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embeddings
Matrix B: layers_31_self_attn_k_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_self_attn_k_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_self_attn_k_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_self_attn_k_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_self_attn_k_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.06 seconds

üìä Result base: layers_31_self_attn_k_proj_weightxinput_token_embeddings (send_back=True)
‚úÖ Received ACK_combined_matrix_saved 1/1
‚úÖ All ACKs received!
‚úÖ Loaded /dev/shm/matrix_shards/layers_31_self_attn_k_proj_weightxinput_token_embeddings_combined.bin
  Original dims: [1, 1, 1, 1024]
  Result tensor shape: torch.Size([1, 1024]), size: 4,096 bytes
  Data range: [-16.871967, 13.698719]

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embeddings
Matrix B: layers_31_self_attn_v_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_self_attn_v_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_self_attn_v_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_self_attn_v_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embeddings.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_self_attn_v_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.06 seconds

üìä Result base: layers_31_self_attn_v_proj_weightxinput_token_embeddings (send_back=True)
‚úÖ Received ACK_combined_matrix_saved 1/1
‚úÖ All ACKs received!
‚úÖ Loaded /dev/shm/matrix_shards/layers_31_self_attn_v_proj_weightxinput_token_embeddings_combined.bin
  Original dims: [1, 1, 1, 1024]
  Result tensor shape: torch.Size([1, 1024]), size: 4,096 bytes
  Data range: [-2.487383, 1.619485]
RoPE q shape: (32, 128)
RoPE k shape: (8, 128)
KV cache layer 31: K=(32, 25, 128) V=(32, 25, 128)
attn_output (per-head): (32, 128)
attn_hidden: (1, 4096)
residual: (1, 4096)
hidden_out(after attn+res): (1, 4096)
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Preparing full matrix: layer31_mlp_in.bin
Local paths - DISK: matrix_shards/layer31_mlp_in.bin, RAM: /dev/shm/matrix_shards/layer31_mlp_in.bin
Saving to local storage...
Saving matrix to binary file (OpenCL): matrix_shards/layer31_mlp_in.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Saving matrix to binary file (OpenCL): /dev/shm/matrix_shards/layer31_mlp_in.bin
Original shape: (4096, 1)
Converted to 4D: (1, 1, 4096, 1)
Writing binary file...
Done.
Remote paths - RAM: /dev/shm/matrix_shards/layer31_mlp_in.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/layer31_mlp_in.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
üì§ Sent file layer31_mlp_in.bin to 192.168.2.104
‚úÖ Received layer31_mlp_in.bin 1/1
‚úÖ All ACKs received!
Sending to 192.168.2.101
üì§ Sent file layer31_mlp_in.bin to 192.168.2.101
‚úÖ Received layer31_mlp_in.bin 1/1
‚úÖ All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_31_mlp_gate_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_31_mlp_gate_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_31_mlp_gate_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_31_mlp_up_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_31_mlp_up_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_31_mlp_up_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_31_mlp_down_proj_weight
   Split Matrix: True
   Dimension: 1

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Loading cluster matrix shards: layers_31_mlp_down_proj_weight
Number of nodes/shard locations: 4
Checking for existing shards in RAM: /dev/shm/matrix_shards/layers_31_mlp_down_proj_weight_shard_0.bin
Found existing matrix shards in local RAM
  Shard 0: Using existing RAM path
  Shard 1: Using existing RAM path
  Shard 2: Using existing RAM path
  Shard 3: Using existing RAM path

Matrix shard loading complete
Total shard paths tracked: 4

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: layer31_mlp_in
Matrix B: layers_31_mlp_gate_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer31_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_mlp_gate_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/layer31_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_mlp_gate_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer31_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_mlp_gate_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer31_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_mlp_gate_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.54 seconds

üìä Result base: layers_31_mlp_gate_proj_weightxlayer31_mlp_in (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_31_mlp_gate_proj_weightxlayer31_mlp_in_combined.bin
  Original dims: [1, 1, 1, 14336]
  Result tensor shape: torch.Size([1, 14336]), size: 57,344 bytes
  Data range: [-13.063742, 8.188663]

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: layer31_mlp_in
Matrix B: layers_31_mlp_up_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer31_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_mlp_up_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/layer31_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_mlp_up_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer31_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_mlp_up_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer31_mlp_in.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_mlp_up_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.55 seconds

üìä Result base: layers_31_mlp_up_proj_weightxlayer31_mlp_in (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_31_mlp_up_proj_weightxlayer31_mlp_in_combined.bin
  Original dims: [1, 1, 1, 14336]
  Result tensor shape: torch.Size([1, 14336]), size: 57,344 bytes
  Data range: [-4.678010, 6.814502]
======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 4 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790
Preparing full matrix: layer31_mlp_intermediate.bin
Local paths - DISK: matrix_shards/layer31_mlp_intermediate.bin, RAM: /dev/shm/matrix_shards/layer31_mlp_intermediate.bin
Saving to local storage...
Saving matrix to binary file (OpenCL): matrix_shards/layer31_mlp_intermediate.bin
Original shape: (14336, 1)
Converted to 4D: (1, 1, 14336, 1)
Writing binary file...
Done.
Saving matrix to binary file (OpenCL): /dev/shm/matrix_shards/layer31_mlp_intermediate.bin
Original shape: (14336, 1)
Converted to 4D: (1, 1, 14336, 1)
Writing binary file...
Done.
Remote paths - RAM: /dev/shm/matrix_shards/layer31_mlp_intermediate.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/layer31_mlp_intermediate.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
üì§ Sent file layer31_mlp_intermediate.bin to 192.168.2.104
‚úÖ Received layer31_mlp_intermediate.bin 1/1
‚úÖ All ACKs received!
Sending to 192.168.2.101
üì§ Sent file layer31_mlp_intermediate.bin to 192.168.2.101
‚úÖ Received layer31_mlp_intermediate.bin 1/1
‚úÖ All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: layer31_mlp_intermediate
Matrix B: layers_31_mlp_down_proj_weight
Operation: mul
Transpose A: True, Transpose B: False
Send back result: True
Number of shards: 4

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer31_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_mlp_down_proj_weight_shard_0.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/layer31_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_mlp_down_proj_weight_shard_1.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer31_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_mlp_down_proj_weight_shard_2.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/layer31_mlp_intermediate.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_mlp_down_proj_weight_shard_3.bin
  Final transpose flags - A: true, B: true
Send back result: Yes (14 shards will be combined)
  ‚Üí system=1, join_dim=1, shards=4
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

‚è≥ WAITING FOR ACKS FROM NODES (4)
‚úÖ Received ACK_matrixOp_complete 1/4
‚úÖ Received ACK_matrixOp_complete 2/4
‚úÖ Received ACK_matrixOp_complete 3/4
‚úÖ Received ACK_matrixOp_complete 4/4
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.55 seconds

üìä Result base: layers_31_mlp_down_proj_weightxlayer31_mlp_intermediate (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_31_mlp_down_proj_weightxlayer31_mlp_intermediate_combined.bin
  Original dims: [1, 1, 1, 4096]
  Result tensor shape: torch.Size([1, 4096]), size: 16,384 bytes
  Data range: [-2.545979, 17.320435]

=== model output ===

<|begin_of_text|>tell me a joke
Here's one:

Why couldn't the bicycle stand up by itself?

(wait for it...)

Because

====================
(ray-conda-env) rino@rino-Z370-HD3:~/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix$ 
