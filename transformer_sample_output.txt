  âœ… Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_self_attn_k_proj_weight_shard_1.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_self_attn_k_proj_weight_shard_2.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_self_attn_k_proj_weight_shard_3.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.104

â³ WAITING FOR ACKS FROM NODES (4)
âœ… Received ACK_matrixOp_complete 1/4
âœ… Received ACK_matrixOp_complete 2/4
âœ… Received ACK_matrixOp_complete 3/4
âœ… Received ACK_matrixOp_complete 4/4
âœ… All ACKs received!

============================================================
âœ… CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.03 seconds

ðŸ“Š Result base: layers_28_self_attn_k_proj_weightxinput_token_embedding_matrix (send_back=True)
âœ… Loaded /dev/shm/matrix_shards/layers_28_self_attn_k_proj_weightxinput_token_embedding_matrix_combined.bin
  Original dims: [1, 1, 18, 1024]
  Result tensor shape: torch.Size([18, 1024]), size: 73,728 bytes
  Data range: [-0.077254, 0.058846]
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_28_self_attn_v_proj_weight
   Split Matrix: True
   Dimension: 0

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([358, 4096])
  Node 1: shard shape torch.Size([358, 4096])
  Node 2: shard shape torch.Size([153, 4096])
  Node 3: shard shape torch.Size([155, 4096])
Starting distribution of 4 shards to 3 unique nodes
Processing shard 0 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_28_self_attn_v_proj_weight_shard_0.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_28_self_attn_v_proj_weight_shard_0.bin
Saving matrix to binary file: matrix_shards/layers_28_self_attn_v_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([358, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(358, 4096), dtype=float32
  Converting to 4D format...
    2D (358, 4096) -> 4D (1, 1, 358, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 358 Ã— 4096
    Wrote 1,466,368 float32 elements
  File saved successfully
  File size: 5,865,492 bytes
  Expected size: 5,865,492 bytes
  âœ“ File size verification passed
  Memory usage: 5.59 MB
  Save completed: matrix_shards/layers_28_self_attn_v_proj_weight_shard_0.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_28_self_attn_v_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([358, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(358, 4096), dtype=float32
  Converting to 4D format...
    2D (358, 4096) -> 4D (1, 1, 358, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 358 Ã— 4096
    Wrote 1,466,368 float32 elements
  File saved successfully
  File size: 5,865,492 bytes
  Expected size: 5,865,492 bytes
  âœ“ File size verification passed
  Memory usage: 5.59 MB
  Save completed: /dev/shm/matrix_shards/layers_28_self_attn_v_proj_weight_shard_0.bin
  Added RAM path to file list
Processing shard 1 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_28_self_attn_v_proj_weight_shard_1.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_28_self_attn_v_proj_weight_shard_1.bin
Saving matrix to binary file: matrix_shards/layers_28_self_attn_v_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([358, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(358, 4096), dtype=float32
  Converting to 4D format...
    2D (358, 4096) -> 4D (1, 1, 358, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 358 Ã— 4096
    Wrote 1,466,368 float32 elements
  File saved successfully
  File size: 5,865,492 bytes
  Expected size: 5,865,492 bytes
  âœ“ File size verification passed
  Memory usage: 5.59 MB
  Save completed: matrix_shards/layers_28_self_attn_v_proj_weight_shard_1.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_28_self_attn_v_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([358, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(358, 4096), dtype=float32
  Converting to 4D format...
    2D (358, 4096) -> 4D (1, 1, 358, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 358 Ã— 4096
    Wrote 1,466,368 float32 elements
  File saved successfully
  File size: 5,865,492 bytes
  Expected size: 5,865,492 bytes
  âœ“ File size verification passed
  Memory usage: 5.59 MB
  Save completed: /dev/shm/matrix_shards/layers_28_self_attn_v_proj_weight_shard_1.bin
  Added RAM path to file list
Processing shard 2 for node 192.168.2.101
  Remote node 192.168.2.101: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_28_self_attn_v_proj_weight_shard_2.bin
Saving matrix to binary file: matrix_shards/layers_28_self_attn_v_proj_weight_shard_2.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([153, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(153, 4096), dtype=float32
  Converting to 4D format...
    2D (153, 4096) -> 4D (1, 1, 153, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 153 Ã— 4096
    Wrote 626,688 float32 elements
  File saved successfully
  File size: 2,506,772 bytes
  Expected size: 2,506,772 bytes
  âœ“ File size verification passed
  Memory usage: 2.39 MB
  Save completed: matrix_shards/layers_28_self_attn_v_proj_weight_shard_2.bin
DEBUG::: layers_28_self_attn_v_proj_weight_shard_2.bin
  Step 2: Sending file to remote node 192.168.2.101
ðŸ“¤ Sent file layers_28_self_attn_v_proj_weight_shard_2.bin to 192.168.2.101
âœ… Received layers_28_self_attn_v_proj_weight_shard_2.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_28_self_attn_v_proj_weight_shard_2.bin
Processing shard 3 for node 192.168.2.104
  Remote node 192.168.2.104: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_28_self_attn_v_proj_weight_shard_3.bin
Saving matrix to binary file: matrix_shards/layers_28_self_attn_v_proj_weight_shard_3.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([155, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(155, 4096), dtype=float32
  Converting to 4D format...
    2D (155, 4096) -> 4D (1, 1, 155, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 155 Ã— 4096
    Wrote 634,880 float32 elements
  File saved successfully
  File size: 2,539,540 bytes
  Expected size: 2,539,540 bytes
  âœ“ File size verification passed
  Memory usage: 2.42 MB
  Save completed: matrix_shards/layers_28_self_attn_v_proj_weight_shard_3.bin
DEBUG::: layers_28_self_attn_v_proj_weight_shard_3.bin
  Step 2: Sending file to remote node 192.168.2.104
ðŸ“¤ Sent file layers_28_self_attn_v_proj_weight_shard_3.bin to 192.168.2.104
âœ… Received layers_28_self_attn_v_proj_weight_shard_3.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_28_self_attn_v_proj_weight_shard_3.bin
Distribution complete: 4 shards saved and distributed

============================================================
ðŸš€ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embedding_matrix
Matrix B: layers_28_self_attn_v_proj_weight
Operation: mul
Transpose A: False, Transpose B: True
Send back result: True
Number of shards: 4

ðŸ“¤ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_self_attn_v_proj_weight_shard_0.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_self_attn_v_proj_weight_shard_1.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_self_attn_v_proj_weight_shard_2.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_self_attn_v_proj_weight_shard_3.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.104

â³ WAITING FOR ACKS FROM NODES (4)
âœ… Received ACK_matrixOp_complete 1/4
âœ… Received ACK_matrixOp_complete 2/4
âœ… Received ACK_matrixOp_complete 3/4
âœ… Received ACK_matrixOp_complete 4/4
âœ… All ACKs received!

============================================================
âœ… CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.03 seconds

ðŸ“Š Result base: layers_28_self_attn_v_proj_weightxinput_token_embedding_matrix (send_back=True)
âœ… Loaded /dev/shm/matrix_shards/layers_28_self_attn_v_proj_weightxinput_token_embedding_matrix_combined.bin
  Original dims: [1, 1, 18, 1024]
  Result tensor shape: torch.Size([18, 1024]), size: 73,728 bytes
  Data range: [-0.031780, 0.037447]
   Computing GQA attention with TORCH...
attn_output_flat shape : torch.Size([18, 4096])
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
Preparing full matrix: attn_output_flat.bin
Local paths - DISK: matrix_shards/attn_output_flat.bin, RAM: /dev/shm/matrix_shards/attn_output_flat.bin
Saving to local storage...
Saving matrix to binary file: matrix_shards/attn_output_flat.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 4096), dtype=float32
  Converting to 4D format...
    2D (18, 4096) -> 4D (1, 1, 18, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 4096
    Wrote 73,728 float32 elements
  File saved successfully
  File size: 294,932 bytes
  Expected size: 294,932 bytes
  âœ“ File size verification passed
  Memory usage: 0.28 MB
  Save completed: matrix_shards/attn_output_flat.bin
Saving matrix to binary file: /dev/shm/matrix_shards/attn_output_flat.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 4096), dtype=float32
  Converting to 4D format...
    2D (18, 4096) -> 4D (1, 1, 18, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 4096
    Wrote 73,728 float32 elements
  File saved successfully
  File size: 294,932 bytes
  Expected size: 294,932 bytes
  âœ“ File size verification passed
  Memory usage: 0.28 MB
  Save completed: /dev/shm/matrix_shards/attn_output_flat.bin
Remote paths - RAM: /dev/shm/matrix_shards/attn_output_flat.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/attn_output_flat.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
ðŸ“¤ Sent file attn_output_flat.bin to 192.168.2.104
âœ… Received attn_output_flat.bin 1/1
âœ… All ACKs received!
Sending to 192.168.2.101
ðŸ“¤ Sent file attn_output_flat.bin to 192.168.2.101
âœ… Received attn_output_flat.bin 1/1
âœ… All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_28_mlp_gate_proj_weight
   Split Matrix: True
   Dimension: 0

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([5017, 4096])
  Node 1: shard shape torch.Size([5017, 4096])
  Node 2: shard shape torch.Size([2150, 4096])
  Node 3: shard shape torch.Size([2152, 4096])
Starting distribution of 4 shards to 3 unique nodes
Processing shard 0 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_28_mlp_gate_proj_weight_shard_0.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_28_mlp_gate_proj_weight_shard_0.bin
Saving matrix to binary file: matrix_shards/layers_28_mlp_gate_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: matrix_shards/layers_28_mlp_gate_proj_weight_shard_0.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_28_mlp_gate_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: /dev/shm/matrix_shards/layers_28_mlp_gate_proj_weight_shard_0.bin
  Added RAM path to file list
Processing shard 1 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_28_mlp_gate_proj_weight_shard_1.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_28_mlp_gate_proj_weight_shard_1.bin
Saving matrix to binary file: matrix_shards/layers_28_mlp_gate_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: matrix_shards/layers_28_mlp_gate_proj_weight_shard_1.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_28_mlp_gate_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: /dev/shm/matrix_shards/layers_28_mlp_gate_proj_weight_shard_1.bin
  Added RAM path to file list
Processing shard 2 for node 192.168.2.101
  Remote node 192.168.2.101: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_28_mlp_gate_proj_weight_shard_2.bin
Saving matrix to binary file: matrix_shards/layers_28_mlp_gate_proj_weight_shard_2.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([2150, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(2150, 4096), dtype=float32
  Converting to 4D format...
    2D (2150, 4096) -> 4D (1, 1, 2150, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 2150 Ã— 4096
    Wrote 8,806,400 float32 elements
  File saved successfully
  File size: 35,225,620 bytes
  Expected size: 35,225,620 bytes
  âœ“ File size verification passed
  Memory usage: 33.59 MB
  Save completed: matrix_shards/layers_28_mlp_gate_proj_weight_shard_2.bin
DEBUG::: layers_28_mlp_gate_proj_weight_shard_2.bin
  Step 2: Sending file to remote node 192.168.2.101
ðŸ“¤ Sent file layers_28_mlp_gate_proj_weight_shard_2.bin to 192.168.2.101
âœ… Received layers_28_mlp_gate_proj_weight_shard_2.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_28_mlp_gate_proj_weight_shard_2.bin
Processing shard 3 for node 192.168.2.104
  Remote node 192.168.2.104: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_28_mlp_gate_proj_weight_shard_3.bin
Saving matrix to binary file: matrix_shards/layers_28_mlp_gate_proj_weight_shard_3.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([2152, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(2152, 4096), dtype=float32
  Converting to 4D format...
    2D (2152, 4096) -> 4D (1, 1, 2152, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 2152 Ã— 4096
    Wrote 8,814,592 float32 elements
  File saved successfully
  File size: 35,258,388 bytes
  Expected size: 35,258,388 bytes
  âœ“ File size verification passed
  Memory usage: 33.63 MB
  Save completed: matrix_shards/layers_28_mlp_gate_proj_weight_shard_3.bin
DEBUG::: layers_28_mlp_gate_proj_weight_shard_3.bin
  Step 2: Sending file to remote node 192.168.2.104
ðŸ“¤ Sent file layers_28_mlp_gate_proj_weight_shard_3.bin to 192.168.2.104
âœ… Received layers_28_mlp_gate_proj_weight_shard_3.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_28_mlp_gate_proj_weight_shard_3.bin
Distribution complete: 4 shards saved and distributed

============================================================
ðŸš€ STARTING CLUSTER OPERATION
============================================================
Matrix A: attn_output_flat
Matrix B: layers_28_mlp_gate_proj_weight
Operation: mul
Transpose A: False, Transpose B: True
Send back result: True
Number of shards: 4

ðŸ“¤ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_mlp_gate_proj_weight_shard_0.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_mlp_gate_proj_weight_shard_1.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_mlp_gate_proj_weight_shard_2.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_mlp_gate_proj_weight_shard_3.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.104

â³ WAITING FOR ACKS FROM NODES (4)
âœ… Received ACK_matrixOp_complete 1/4
âœ… Received ACK_matrixOp_complete 2/4
âœ… Received ACK_matrixOp_complete 3/4
âœ… Received ACK_matrixOp_complete 4/4
âœ… All ACKs received!

============================================================
âœ… CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.14 seconds

ðŸ“Š Result base: layers_28_mlp_gate_proj_weightxattn_output_flat (send_back=True)
âœ… Loaded /dev/shm/matrix_shards/layers_28_mlp_gate_proj_weightxattn_output_flat_combined.bin
  Original dims: [1, 1, 18, 14336]
  Result tensor shape: torch.Size([18, 14336]), size: 1,032,192 bytes
  Data range: [-0.041600, 0.029244]
   Running MLP up projection...
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_28_mlp_up_proj_weight
   Split Matrix: True
   Dimension: 0

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([5017, 4096])
  Node 1: shard shape torch.Size([5017, 4096])
  Node 2: shard shape torch.Size([2150, 4096])
  Node 3: shard shape torch.Size([2152, 4096])
Starting distribution of 4 shards to 3 unique nodes
Processing shard 0 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_28_mlp_up_proj_weight_shard_0.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_28_mlp_up_proj_weight_shard_0.bin
Saving matrix to binary file: matrix_shards/layers_28_mlp_up_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: matrix_shards/layers_28_mlp_up_proj_weight_shard_0.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_28_mlp_up_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: /dev/shm/matrix_shards/layers_28_mlp_up_proj_weight_shard_0.bin
  Added RAM path to file list
Processing shard 1 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_28_mlp_up_proj_weight_shard_1.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_28_mlp_up_proj_weight_shard_1.bin
Saving matrix to binary file: matrix_shards/layers_28_mlp_up_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: matrix_shards/layers_28_mlp_up_proj_weight_shard_1.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_28_mlp_up_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: /dev/shm/matrix_shards/layers_28_mlp_up_proj_weight_shard_1.bin
  Added RAM path to file list
Processing shard 2 for node 192.168.2.101
  Remote node 192.168.2.101: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_28_mlp_up_proj_weight_shard_2.bin
Saving matrix to binary file: matrix_shards/layers_28_mlp_up_proj_weight_shard_2.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([2150, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(2150, 4096), dtype=float32
  Converting to 4D format...
    2D (2150, 4096) -> 4D (1, 1, 2150, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 2150 Ã— 4096
    Wrote 8,806,400 float32 elements
  File saved successfully
  File size: 35,225,620 bytes
  Expected size: 35,225,620 bytes
  âœ“ File size verification passed
  Memory usage: 33.59 MB
  Save completed: matrix_shards/layers_28_mlp_up_proj_weight_shard_2.bin
DEBUG::: layers_28_mlp_up_proj_weight_shard_2.bin
  Step 2: Sending file to remote node 192.168.2.101
ðŸ“¤ Sent file layers_28_mlp_up_proj_weight_shard_2.bin to 192.168.2.101
âœ… Received layers_28_mlp_up_proj_weight_shard_2.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_28_mlp_up_proj_weight_shard_2.bin
Processing shard 3 for node 192.168.2.104
  Remote node 192.168.2.104: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_28_mlp_up_proj_weight_shard_3.bin
Saving matrix to binary file: matrix_shards/layers_28_mlp_up_proj_weight_shard_3.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([2152, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(2152, 4096), dtype=float32
  Converting to 4D format...
    2D (2152, 4096) -> 4D (1, 1, 2152, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 2152 Ã— 4096
    Wrote 8,814,592 float32 elements
  File saved successfully
  File size: 35,258,388 bytes
  Expected size: 35,258,388 bytes
  âœ“ File size verification passed
  Memory usage: 33.63 MB
  Save completed: matrix_shards/layers_28_mlp_up_proj_weight_shard_3.bin
DEBUG::: layers_28_mlp_up_proj_weight_shard_3.bin
  Step 2: Sending file to remote node 192.168.2.104
ðŸ“¤ Sent file layers_28_mlp_up_proj_weight_shard_3.bin to 192.168.2.104
âœ… Received layers_28_mlp_up_proj_weight_shard_3.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_28_mlp_up_proj_weight_shard_3.bin
Distribution complete: 4 shards saved and distributed

============================================================
ðŸš€ STARTING CLUSTER OPERATION
============================================================
Matrix A: attn_output_flat
Matrix B: layers_28_mlp_up_proj_weight
Operation: mul
Transpose A: False, Transpose B: True
Send back result: True
Number of shards: 4

ðŸ“¤ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_mlp_up_proj_weight_shard_0.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_mlp_up_proj_weight_shard_1.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_mlp_up_proj_weight_shard_2.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_mlp_up_proj_weight_shard_3.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.104

â³ WAITING FOR ACKS FROM NODES (4)
âœ… Received ACK_matrixOp_complete 1/4
âœ… Received ACK_matrixOp_complete 2/4
âœ… Received ACK_matrixOp_complete 3/4
âœ… Received ACK_matrixOp_complete 4/4
âœ… All ACKs received!

============================================================
âœ… CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.15 seconds

ðŸ“Š Result base: layers_28_mlp_up_proj_weightxattn_output_flat (send_back=True)
âœ… Loaded /dev/shm/matrix_shards/layers_28_mlp_up_proj_weightxattn_output_flat_combined.bin
  Original dims: [1, 1, 18, 14336]
  Result tensor shape: torch.Size([18, 14336]), size: 1,032,192 bytes
  Data range: [-0.018859, 0.016483]
   Up projection result shape: torch.Size([18, 14336])
   Applying SiLU activation to gate...
   Element-wise multiplication (gate_silu * up)...
   MLP intermediate shape: torch.Size([18, 14336])
   Running MLP down projection...
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
Preparing full matrix: mlp_intermediate_cluster.bin
Local paths - DISK: matrix_shards/mlp_intermediate_cluster.bin, RAM: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin
Saving to local storage...
Saving matrix to binary file: matrix_shards/mlp_intermediate_cluster.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 14336), dtype=float32
  Converting to 4D format...
    2D (18, 14336) -> 4D (1, 1, 18, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 14336
    Wrote 258,048 float32 elements
  File saved successfully
  File size: 1,032,212 bytes
  Expected size: 1,032,212 bytes
  âœ“ File size verification passed
  Memory usage: 0.98 MB
  Save completed: matrix_shards/mlp_intermediate_cluster.bin
Saving matrix to binary file: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 14336), dtype=float32
  Converting to 4D format...
    2D (18, 14336) -> 4D (1, 1, 18, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 14336
    Wrote 258,048 float32 elements
  File saved successfully
  File size: 1,032,212 bytes
  Expected size: 1,032,212 bytes
  âœ“ File size verification passed
  Memory usage: 0.98 MB
  Save completed: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin
Remote paths - RAM: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/mlp_intermediate_cluster.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
ðŸ“¤ Sent file mlp_intermediate_cluster.bin to 192.168.2.104
âœ… Received mlp_intermediate_cluster.bin 1/1
âœ… All ACKs received!
Sending to 192.168.2.101
ðŸ“¤ Sent file mlp_intermediate_cluster.bin to 192.168.2.101
âœ… Received mlp_intermediate_cluster.bin 1/1
âœ… All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_28_mlp_down_proj_weight
   Split Matrix: True
   Dimension: 0

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([1433, 14336])
  Node 1: shard shape torch.Size([1433, 14336])
  Node 2: shard shape torch.Size([614, 14336])
  Node 3: shard shape torch.Size([616, 14336])
Starting distribution of 4 shards to 3 unique nodes
Processing shard 0 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_28_mlp_down_proj_weight_shard_0.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_28_mlp_down_proj_weight_shard_0.bin
Saving matrix to binary file: matrix_shards/layers_28_mlp_down_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([1433, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(1433, 14336), dtype=float32
  Converting to 4D format...
    2D (1433, 14336) -> 4D (1, 1, 1433, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 1433 Ã— 14336
    Wrote 20,543,488 float32 elements
  File saved successfully
  File size: 82,173,972 bytes
  Expected size: 82,173,972 bytes
  âœ“ File size verification passed
  Memory usage: 78.37 MB
  Save completed: matrix_shards/layers_28_mlp_down_proj_weight_shard_0.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_28_mlp_down_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([1433, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(1433, 14336), dtype=float32
  Converting to 4D format...
    2D (1433, 14336) -> 4D (1, 1, 1433, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 1433 Ã— 14336
    Wrote 20,543,488 float32 elements
  File saved successfully
  File size: 82,173,972 bytes
  Expected size: 82,173,972 bytes
  âœ“ File size verification passed
  Memory usage: 78.37 MB
  Save completed: /dev/shm/matrix_shards/layers_28_mlp_down_proj_weight_shard_0.bin
  Added RAM path to file list
Processing shard 1 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_28_mlp_down_proj_weight_shard_1.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_28_mlp_down_proj_weight_shard_1.bin
Saving matrix to binary file: matrix_shards/layers_28_mlp_down_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([1433, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(1433, 14336), dtype=float32
  Converting to 4D format...
    2D (1433, 14336) -> 4D (1, 1, 1433, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 1433 Ã— 14336
    Wrote 20,543,488 float32 elements
  File saved successfully
  File size: 82,173,972 bytes
  Expected size: 82,173,972 bytes
  âœ“ File size verification passed
  Memory usage: 78.37 MB
  Save completed: matrix_shards/layers_28_mlp_down_proj_weight_shard_1.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_28_mlp_down_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([1433, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(1433, 14336), dtype=float32
  Converting to 4D format...
    2D (1433, 14336) -> 4D (1, 1, 1433, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 1433 Ã— 14336
    Wrote 20,543,488 float32 elements
  File saved successfully
  File size: 82,173,972 bytes
  Expected size: 82,173,972 bytes
  âœ“ File size verification passed
  Memory usage: 78.37 MB
  Save completed: /dev/shm/matrix_shards/layers_28_mlp_down_proj_weight_shard_1.bin
  Added RAM path to file list
Processing shard 2 for node 192.168.2.101
  Remote node 192.168.2.101: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_28_mlp_down_proj_weight_shard_2.bin
Saving matrix to binary file: matrix_shards/layers_28_mlp_down_proj_weight_shard_2.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([614, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(614, 14336), dtype=float32
  Converting to 4D format...
    2D (614, 14336) -> 4D (1, 1, 614, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 614 Ã— 14336
    Wrote 8,802,304 float32 elements
  File saved successfully
  File size: 35,209,236 bytes
  Expected size: 35,209,236 bytes
  âœ“ File size verification passed
  Memory usage: 33.58 MB
  Save completed: matrix_shards/layers_28_mlp_down_proj_weight_shard_2.bin
DEBUG::: layers_28_mlp_down_proj_weight_shard_2.bin
  Step 2: Sending file to remote node 192.168.2.101
ðŸ“¤ Sent file layers_28_mlp_down_proj_weight_shard_2.bin to 192.168.2.101
âœ… Received layers_28_mlp_down_proj_weight_shard_2.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_28_mlp_down_proj_weight_shard_2.bin
Processing shard 3 for node 192.168.2.104
  Remote node 192.168.2.104: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_28_mlp_down_proj_weight_shard_3.bin
Saving matrix to binary file: matrix_shards/layers_28_mlp_down_proj_weight_shard_3.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([616, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(616, 14336), dtype=float32
  Converting to 4D format...
    2D (616, 14336) -> 4D (1, 1, 616, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 616 Ã— 14336
    Wrote 8,830,976 float32 elements
  File saved successfully
  File size: 35,323,924 bytes
  Expected size: 35,323,924 bytes
  âœ“ File size verification passed
  Memory usage: 33.69 MB
  Save completed: matrix_shards/layers_28_mlp_down_proj_weight_shard_3.bin
DEBUG::: layers_28_mlp_down_proj_weight_shard_3.bin
  Step 2: Sending file to remote node 192.168.2.104
ðŸ“¤ Sent file layers_28_mlp_down_proj_weight_shard_3.bin to 192.168.2.104
âœ… Received layers_28_mlp_down_proj_weight_shard_3.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_28_mlp_down_proj_weight_shard_3.bin
Distribution complete: 4 shards saved and distributed

============================================================
ðŸš€ STARTING CLUSTER OPERATION
============================================================
Matrix A: mlp_intermediate_cluster
Matrix B: layers_28_mlp_down_proj_weight
Operation: mul
Transpose A: False, Transpose B: True
Send back result: True
Number of shards: 4

ðŸ“¤ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_mlp_down_proj_weight_shard_0.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_mlp_down_proj_weight_shard_1.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_mlp_down_proj_weight_shard_2.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin
  Matrix B path: /dev/shm/matrix_shards/layers_28_mlp_down_proj_weight_shard_3.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.104

â³ WAITING FOR ACKS FROM NODES (4)
âœ… Received ACK_matrixOp_complete 1/4
âœ… Received ACK_matrixOp_complete 2/4
âœ… Received ACK_matrixOp_complete 3/4
âœ… Received ACK_matrixOp_complete 4/4
âœ… All ACKs received!

============================================================
âœ… CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.13 seconds

ðŸ“Š Result base: layers_28_mlp_down_proj_weightxmlp_intermediate_cluster (send_back=True)
âœ… Loaded /dev/shm/matrix_shards/layers_28_mlp_down_proj_weightxmlp_intermediate_cluster_combined.bin
  Original dims: [1, 1, 18, 4096]
  Result tensor shape: torch.Size([18, 4096]), size: 294,912 bytes
  Data range: [-0.000065, 0.000068]
   Adding residual connection (MLP + attention)...
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([18, 1433])
  Node 1: shard shape torch.Size([18, 1433])
  Node 2: shard shape torch.Size([18, 614])
  Node 3: shard shape torch.Size([18, 616])
Starting distribution of 4 shards to 3 unique nodes
Processing shard 0 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/mlp_output_cluster_shard_0.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/mlp_output_cluster_shard_0.bin
Saving matrix to binary file: matrix_shards/mlp_output_cluster_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: matrix_shards/mlp_output_cluster_shard_0.bin
Saving matrix to binary file: /dev/shm/matrix_shards/mlp_output_cluster_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: /dev/shm/matrix_shards/mlp_output_cluster_shard_0.bin
  Added RAM path to file list
Processing shard 1 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/mlp_output_cluster_shard_1.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/mlp_output_cluster_shard_1.bin
Saving matrix to binary file: matrix_shards/mlp_output_cluster_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: matrix_shards/mlp_output_cluster_shard_1.bin
Saving matrix to binary file: /dev/shm/matrix_shards/mlp_output_cluster_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: /dev/shm/matrix_shards/mlp_output_cluster_shard_1.bin
  Added RAM path to file list
Processing shard 2 for node 192.168.2.101
  Remote node 192.168.2.101: Beginning distribution
  Step 1: Saving locally to matrix_shards/mlp_output_cluster_shard_2.bin
Saving matrix to binary file: matrix_shards/mlp_output_cluster_shard_2.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 614]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 614), dtype=float32
  Converting to 4D format...
    2D (18, 614) -> 4D (1, 1, 18, 614)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 614
    Wrote 11,052 float32 elements
  File saved successfully
  File size: 44,228 bytes
  Expected size: 44,228 bytes
  âœ“ File size verification passed
  Memory usage: 0.04 MB
  Save completed: matrix_shards/mlp_output_cluster_shard_2.bin
DEBUG::: mlp_output_cluster_shard_2.bin
  Step 2: Sending file to remote node 192.168.2.101
ðŸ“¤ Sent file mlp_output_cluster_shard_2.bin to 192.168.2.101
âœ… Received mlp_output_cluster_shard_2.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/mlp_output_cluster_shard_2.bin
Processing shard 3 for node 192.168.2.104
  Remote node 192.168.2.104: Beginning distribution
  Step 1: Saving locally to matrix_shards/mlp_output_cluster_shard_3.bin
Saving matrix to binary file: matrix_shards/mlp_output_cluster_shard_3.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 616]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 616), dtype=float32
  Converting to 4D format...
    2D (18, 616) -> 4D (1, 1, 18, 616)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 616
    Wrote 11,088 float32 elements
  File saved successfully
  File size: 44,372 bytes
  Expected size: 44,372 bytes
  âœ“ File size verification passed
  Memory usage: 0.04 MB
  Save completed: matrix_shards/mlp_output_cluster_shard_3.bin
DEBUG::: mlp_output_cluster_shard_3.bin
  Step 2: Sending file to remote node 192.168.2.104
ðŸ“¤ Sent file mlp_output_cluster_shard_3.bin to 192.168.2.104
âœ… Received mlp_output_cluster_shard_3.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/mlp_output_cluster_shard_3.bin
Distribution complete: 4 shards saved and distributed
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([18, 1433])
  Node 1: shard shape torch.Size([18, 1433])
  Node 2: shard shape torch.Size([18, 614])
  Node 3: shard shape torch.Size([18, 616])
Starting distribution of 4 shards to 3 unique nodes
Processing shard 0 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/attn_outpuattn_output_clustert_flat_shard_0.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_0.bin
Saving matrix to binary file: matrix_shards/attn_outpuattn_output_clustert_flat_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: matrix_shards/attn_outpuattn_output_clustert_flat_shard_0.bin
Saving matrix to binary file: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_0.bin
  Added RAM path to file list
Processing shard 1 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/attn_outpuattn_output_clustert_flat_shard_1.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_1.bin
Saving matrix to binary file: matrix_shards/attn_outpuattn_output_clustert_flat_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: matrix_shards/attn_outpuattn_output_clustert_flat_shard_1.bin
Saving matrix to binary file: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_1.bin
  Added RAM path to file list
Processing shard 2 for node 192.168.2.101
  Remote node 192.168.2.101: Beginning distribution
  Step 1: Saving locally to matrix_shards/attn_outpuattn_output_clustert_flat_shard_2.bin
Saving matrix to binary file: matrix_shards/attn_outpuattn_output_clustert_flat_shard_2.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 614]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 614), dtype=float32
  Converting to 4D format...
    2D (18, 614) -> 4D (1, 1, 18, 614)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 614
    Wrote 11,052 float32 elements
  File saved successfully
  File size: 44,228 bytes
  Expected size: 44,228 bytes
  âœ“ File size verification passed
  Memory usage: 0.04 MB
  Save completed: matrix_shards/attn_outpuattn_output_clustert_flat_shard_2.bin
DEBUG::: attn_outpuattn_output_clustert_flat_shard_2.bin
  Step 2: Sending file to remote node 192.168.2.101
ðŸ“¤ Sent file attn_outpuattn_output_clustert_flat_shard_2.bin to 192.168.2.101
âœ… Received attn_outpuattn_output_clustert_flat_shard_2.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_2.bin
Processing shard 3 for node 192.168.2.104
  Remote node 192.168.2.104: Beginning distribution
  Step 1: Saving locally to matrix_shards/attn_outpuattn_output_clustert_flat_shard_3.bin
Saving matrix to binary file: matrix_shards/attn_outpuattn_output_clustert_flat_shard_3.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 616]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 616), dtype=float32
  Converting to 4D format...
    2D (18, 616) -> 4D (1, 1, 18, 616)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 616
    Wrote 11,088 float32 elements
  File saved successfully
  File size: 44,372 bytes
  Expected size: 44,372 bytes
  âœ“ File size verification passed
  Memory usage: 0.04 MB
  Save completed: matrix_shards/attn_outpuattn_output_clustert_flat_shard_3.bin
DEBUG::: attn_outpuattn_output_clustert_flat_shard_3.bin
  Step 2: Sending file to remote node 192.168.2.104
ðŸ“¤ Sent file attn_outpuattn_output_clustert_flat_shard_3.bin to 192.168.2.104
âœ… Received attn_outpuattn_output_clustert_flat_shard_3.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_3.bin
Distribution complete: 4 shards saved and distributed

============================================================
ðŸš€ STARTING CLUSTER OPERATION
============================================================
Matrix A: mlp_output_cluster
Matrix B: attn_outpuattn_output_clustert_flat
Operation: add
Transpose A: False, Transpose B: True
Send back result: True
Number of shards: 4

ðŸ“¤ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/mlp_output_cluster_shard_0.bin
  Matrix B path: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_0.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/mlp_output_cluster_shard_1.bin
  Matrix B path: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_1.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/mlp_output_cluster_shard_2.bin
  Matrix B path: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_2.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: False (GPU #0)
  Next GPU for this node will be: #0
  Matrix A path: /dev/shm/matrix_shards/mlp_output_cluster_shard_3.bin
  Matrix B path: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_3.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.104

â³ WAITING FOR ACKS FROM NODES (4)
âœ… Received ACK_matrixOp_complete 1/4
âœ… Received ACK_matrixOp_complete 2/4
âœ… Received ACK_matrixOp_complete 3/4
âœ… Received ACK_matrixOp_complete 4/4
âœ… All ACKs received!

============================================================
âœ… CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.03 seconds

ðŸ“Š Result base: attn_outpuattn_output_clustert_flatxmlp_output_cluster (send_back=True)
âœ… Loaded /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flatxmlp_output_cluster_combined.bin
  Original dims: [1, 1, 18, 4096]
  Result tensor shape: torch.Size([18, 4096]), size: 294,912 bytes
  Data range: [-0.020827, 0.022636]
   Applying post-attention layer normalization...
   Loaded post-attention layer norm weights: shape torch.Size([4096])
   Applied layer normalization
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([18, 1433])
  Node 1: shard shape torch.Size([18, 1433])
  Node 2: shard shape torch.Size([18, 614])
  Node 3: shard shape torch.Size([18, 616])
   Loading attention weights for layer 29...
   Running Q/K/V projections with CLUSTER...
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_29_self_attn_q_proj_weight
   Split Matrix: True
   Dimension: 1

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([4096, 1433])
  Node 1: shard shape torch.Size([4096, 1433])
  Node 2: shard shape torch.Size([4096, 614])
  Node 3: shard shape torch.Size([4096, 616])
Starting distribution of 4 shards to 3 unique nodes
Processing shard 0 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_29_self_attn_q_proj_weight_shard_0.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_29_self_attn_q_proj_weight_shard_0.bin
Saving matrix to binary file: matrix_shards/layers_29_self_attn_q_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([4096, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(4096, 1433), dtype=float32
  Converting to 4D format...
    2D (4096, 1433) -> 4D (1, 1, 4096, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 4096 Ã— 1433
    Wrote 5,869,568 float32 elements
  File saved successfully
  File size: 23,478,292 bytes
  Expected size: 23,478,292 bytes
  âœ“ File size verification passed
  Memory usage: 22.39 MB
  Save completed: matrix_shards/layers_29_self_attn_q_proj_weight_shard_0.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_29_self_attn_q_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([4096, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(4096, 1433), dtype=float32
  Converting to 4D format...
    2D (4096, 1433) -> 4D (1, 1, 4096, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 4096 Ã— 1433
    Wrote 5,869,568 float32 elements
  File saved successfully
  File size: 23,478,292 bytes
  Expected size: 23,478,292 bytes
  âœ“ File size verification passed
  Memory usage: 22.39 MB
  Save completed: /dev/shm/matrix_shards/layers_29_self_attn_q_proj_weight_shard_0.bin
  Added RAM path to file list
Processing shard 1 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_29_self_attn_q_proj_weight_shard_1.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_29_self_attn_q_proj_weight_shard_1.bin
Saving matrix to binary file: matrix_shards/layers_29_self_attn_q_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([4096, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(4096, 1433), dtype=float32
  Converting to 4D format...
    2D (4096, 1433) -> 4D (1, 1, 4096, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 4096 Ã— 1433
    Wrote 5,869,568 float32 elements
  File saved successfully
  File size: 23,478,292 bytes
  Expected size: 23,478,292 bytes
  âœ“ File size verification passed
  Memory usage: 22.39 MB
  Save completed: matrix_shards/layers_29_self_attn_q_proj_weight_shard_1.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_29_self_attn_q_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([4096, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(4096, 1433), dtype=float32
  Converting to 4D format...
    2D (4096, 1433) -> 4D (1, 1, 4096, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 4096 Ã— 1433
    Wrote 5,869,568 float32 elements
  File saved successfully
  File size: 23,478,292 bytes
  Expected size: 23,478,292 bytes
  âœ“ File size verification passed
  Memory usage: 22.39 MB
  Save completed: /dev/shm/matrix_shards/layers_29_self_attn_q_proj_weight_shard_1.bin
  Added RAM path to file list
Processing shard 2 for node 192.168.2.101
  Remote node 192.168.2.101: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_29_self_attn_q_proj_weight_shard_2.bin
Saving matrix to binary file: matrix_shards/layers_29_self_attn_q_proj_weight_shard_2.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([4096, 614]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(4096, 614), dtype=float32
  Converting to 4D format...
    2D (4096, 614) -> 4D (1, 1, 4096, 614)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 4096 Ã— 614
    Wrote 2,514,944 float32 elements
  File saved successfully
  File size: 10,059,796 bytes
  Expected size: 10,059,796 bytes
  âœ“ File size verification passed
  Memory usage: 9.59 MB
  Save completed: matrix_shards/layers_29_self_attn_q_proj_weight_shard_2.bin
DEBUG::: layers_29_self_attn_q_proj_weight_shard_2.bin
  Step 2: Sending file to remote node 192.168.2.101
ðŸ“¤ Sent file layers_29_self_attn_q_proj_weight_shard_2.bin to 192.168.2.101
âœ… Received layers_29_self_attn_q_proj_weight_shard_2.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_29_self_attn_q_proj_weight_shard_2.bin
Processing shard 3 for node 192.168.2.104
  Remote node 192.168.2.104: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_29_self_attn_q_proj_weight_shard_3.bin
Saving matrix to binary file: matrix_shards/layers_29_self_attn_q_proj_weight_shard_3.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([4096, 616]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(4096, 616), dtype=float32
  Converting to 4D format...
    2D (4096, 616) -> 4D (1, 1, 4096, 616)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 4096 Ã— 616
    Wrote 2,523,136 float32 elements
  File saved successfully
  File size: 10,092,564 bytes
  Expected size: 10,092,564 bytes
  âœ“ File size verification passed
  Memory usage: 9.63 MB
  Save completed: matrix_shards/layers_29_self_attn_q_proj_weight_shard_3.bin
DEBUG::: layers_29_self_attn_q_proj_weight_shard_3.bin
  Step 2: Sending file to remote node 192.168.2.104
ðŸ“¤ Sent file layers_29_self_attn_q_proj_weight_shard_3.bin to 192.168.2.104
âœ… Received layers_29_self_attn_q_proj_weight_shard_3.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_29_self_attn_q_proj_weight_shard_3.bin
Distribution complete: 4 shards saved and distributed

============================================================
ðŸš€ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embedding_matrix
Matrix B: layers_29_self_attn_q_proj_weight
Operation: mul
Transpose A: False, Transpose B: False
Send back result: True
Number of shards: 4

ðŸ“¤ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_self_attn_q_proj_weight_shard_0.bin
  Final transpose flags - A: false, B: true
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_self_attn_q_proj_weight_shard_1.bin
  Final transpose flags - A: false, B: true
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_self_attn_q_proj_weight_shard_2.bin
  Final transpose flags - A: false, B: true
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_self_attn_q_proj_weight_shard_3.bin
  Final transpose flags - A: false, B: true
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.104

â³ WAITING FOR ACKS FROM NODES (4)
âœ… Received ACK_matrixOp_complete 1/4
âœ… Received ACK_matrixOp_complete 2/4
âœ… Received ACK_matrixOp_complete 3/4
âœ… Received ACK_matrixOp_complete 4/4
âœ… All ACKs received!

============================================================
âœ… CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.50 seconds

ðŸ“Š Result base: layers_29_self_attn_q_proj_weightxinput_token_embedding_matrix (send_back=True)
âœ… Loaded /dev/shm/matrix_shards/layers_29_self_attn_q_proj_weightxinput_token_embedding_matrix_combined.bin
  Original dims: [1, 1, 18, 4096]
  Result tensor shape: torch.Size([18, 4096]), size: 294,912 bytes
  Data range: [-0.042892, 0.048924]
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_29_self_attn_k_proj_weight
   Split Matrix: True
   Dimension: 0

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([358, 4096])
  Node 1: shard shape torch.Size([358, 4096])
  Node 2: shard shape torch.Size([153, 4096])
  Node 3: shard shape torch.Size([155, 4096])
Starting distribution of 4 shards to 3 unique nodes
Processing shard 0 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_29_self_attn_k_proj_weight_shard_0.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_29_self_attn_k_proj_weight_shard_0.bin
Saving matrix to binary file: matrix_shards/layers_29_self_attn_k_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([358, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(358, 4096), dtype=float32
  Converting to 4D format...
    2D (358, 4096) -> 4D (1, 1, 358, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 358 Ã— 4096
    Wrote 1,466,368 float32 elements
  File saved successfully
  File size: 5,865,492 bytes
  Expected size: 5,865,492 bytes
  âœ“ File size verification passed
  Memory usage: 5.59 MB
  Save completed: matrix_shards/layers_29_self_attn_k_proj_weight_shard_0.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_29_self_attn_k_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([358, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(358, 4096), dtype=float32
  Converting to 4D format...
    2D (358, 4096) -> 4D (1, 1, 358, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 358 Ã— 4096
    Wrote 1,466,368 float32 elements
  File saved successfully
  File size: 5,865,492 bytes
  Expected size: 5,865,492 bytes
  âœ“ File size verification passed
  Memory usage: 5.59 MB
  Save completed: /dev/shm/matrix_shards/layers_29_self_attn_k_proj_weight_shard_0.bin
  Added RAM path to file list
Processing shard 1 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_29_self_attn_k_proj_weight_shard_1.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_29_self_attn_k_proj_weight_shard_1.bin
Saving matrix to binary file: matrix_shards/layers_29_self_attn_k_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([358, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(358, 4096), dtype=float32
  Converting to 4D format...
    2D (358, 4096) -> 4D (1, 1, 358, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 358 Ã— 4096
    Wrote 1,466,368 float32 elements
  File saved successfully
  File size: 5,865,492 bytes
  Expected size: 5,865,492 bytes
  âœ“ File size verification passed
  Memory usage: 5.59 MB
  Save completed: matrix_shards/layers_29_self_attn_k_proj_weight_shard_1.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_29_self_attn_k_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([358, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(358, 4096), dtype=float32
  Converting to 4D format...
    2D (358, 4096) -> 4D (1, 1, 358, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 358 Ã— 4096
    Wrote 1,466,368 float32 elements
  File saved successfully
  File size: 5,865,492 bytes
  Expected size: 5,865,492 bytes
  âœ“ File size verification passed
  Memory usage: 5.59 MB
  Save completed: /dev/shm/matrix_shards/layers_29_self_attn_k_proj_weight_shard_1.bin
  Added RAM path to file list
Processing shard 2 for node 192.168.2.101
  Remote node 192.168.2.101: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_29_self_attn_k_proj_weight_shard_2.bin
Saving matrix to binary file: matrix_shards/layers_29_self_attn_k_proj_weight_shard_2.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([153, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(153, 4096), dtype=float32
  Converting to 4D format...
    2D (153, 4096) -> 4D (1, 1, 153, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 153 Ã— 4096
    Wrote 626,688 float32 elements
  File saved successfully
  File size: 2,506,772 bytes
  Expected size: 2,506,772 bytes
  âœ“ File size verification passed
  Memory usage: 2.39 MB
  Save completed: matrix_shards/layers_29_self_attn_k_proj_weight_shard_2.bin
DEBUG::: layers_29_self_attn_k_proj_weight_shard_2.bin
  Step 2: Sending file to remote node 192.168.2.101
ðŸ“¤ Sent file layers_29_self_attn_k_proj_weight_shard_2.bin to 192.168.2.101
âœ… Received layers_29_self_attn_k_proj_weight_shard_2.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_29_self_attn_k_proj_weight_shard_2.bin
Processing shard 3 for node 192.168.2.104
  Remote node 192.168.2.104: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_29_self_attn_k_proj_weight_shard_3.bin
Saving matrix to binary file: matrix_shards/layers_29_self_attn_k_proj_weight_shard_3.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([155, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(155, 4096), dtype=float32
  Converting to 4D format...
    2D (155, 4096) -> 4D (1, 1, 155, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 155 Ã— 4096
    Wrote 634,880 float32 elements
  File saved successfully
  File size: 2,539,540 bytes
  Expected size: 2,539,540 bytes
  âœ“ File size verification passed
  Memory usage: 2.42 MB
  Save completed: matrix_shards/layers_29_self_attn_k_proj_weight_shard_3.bin
DEBUG::: layers_29_self_attn_k_proj_weight_shard_3.bin
  Step 2: Sending file to remote node 192.168.2.104
ðŸ“¤ Sent file layers_29_self_attn_k_proj_weight_shard_3.bin to 192.168.2.104
âœ… Received layers_29_self_attn_k_proj_weight_shard_3.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_29_self_attn_k_proj_weight_shard_3.bin
Distribution complete: 4 shards saved and distributed

============================================================
ðŸš€ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embedding_matrix
Matrix B: layers_29_self_attn_k_proj_weight
Operation: mul
Transpose A: False, Transpose B: True
Send back result: True
Number of shards: 4

ðŸ“¤ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_self_attn_k_proj_weight_shard_0.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_self_attn_k_proj_weight_shard_1.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_self_attn_k_proj_weight_shard_2.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_self_attn_k_proj_weight_shard_3.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.104

â³ WAITING FOR ACKS FROM NODES (4)
âœ… Received ACK_matrixOp_complete 1/4
âœ… Received ACK_matrixOp_complete 2/4
âœ… Received ACK_matrixOp_complete 3/4
âœ… Received ACK_matrixOp_complete 4/4
âœ… All ACKs received!

============================================================
âœ… CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.02 seconds

ðŸ“Š Result base: layers_29_self_attn_k_proj_weightxinput_token_embedding_matrix (send_back=True)
âœ… Received ACK_combined_matrix_saved 1/1
âœ… All ACKs received!
âœ… Loaded /dev/shm/matrix_shards/layers_29_self_attn_k_proj_weightxinput_token_embedding_matrix_combined.bin
  Original dims: [1, 1, 18, 1024]
  Result tensor shape: torch.Size([18, 1024]), size: 73,728 bytes
  Data range: [-0.078092, 0.084314]
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_29_self_attn_v_proj_weight
   Split Matrix: True
   Dimension: 0

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([358, 4096])
  Node 1: shard shape torch.Size([358, 4096])
  Node 2: shard shape torch.Size([153, 4096])
  Node 3: shard shape torch.Size([155, 4096])
Starting distribution of 4 shards to 3 unique nodes
Processing shard 0 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_29_self_attn_v_proj_weight_shard_0.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_29_self_attn_v_proj_weight_shard_0.bin
Saving matrix to binary file: matrix_shards/layers_29_self_attn_v_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([358, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(358, 4096), dtype=float32
  Converting to 4D format...
    2D (358, 4096) -> 4D (1, 1, 358, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 358 Ã— 4096
    Wrote 1,466,368 float32 elements
  File saved successfully
  File size: 5,865,492 bytes
  Expected size: 5,865,492 bytes
  âœ“ File size verification passed
  Memory usage: 5.59 MB
  Save completed: matrix_shards/layers_29_self_attn_v_proj_weight_shard_0.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_29_self_attn_v_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([358, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(358, 4096), dtype=float32
  Converting to 4D format...
    2D (358, 4096) -> 4D (1, 1, 358, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 358 Ã— 4096
    Wrote 1,466,368 float32 elements
  File saved successfully
  File size: 5,865,492 bytes
  Expected size: 5,865,492 bytes
  âœ“ File size verification passed
  Memory usage: 5.59 MB
  Save completed: /dev/shm/matrix_shards/layers_29_self_attn_v_proj_weight_shard_0.bin
  Added RAM path to file list
Processing shard 1 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_29_self_attn_v_proj_weight_shard_1.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_29_self_attn_v_proj_weight_shard_1.bin
Saving matrix to binary file: matrix_shards/layers_29_self_attn_v_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([358, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(358, 4096), dtype=float32
  Converting to 4D format...
    2D (358, 4096) -> 4D (1, 1, 358, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 358 Ã— 4096
    Wrote 1,466,368 float32 elements
  File saved successfully
  File size: 5,865,492 bytes
  Expected size: 5,865,492 bytes
  âœ“ File size verification passed
  Memory usage: 5.59 MB
  Save completed: matrix_shards/layers_29_self_attn_v_proj_weight_shard_1.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_29_self_attn_v_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([358, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(358, 4096), dtype=float32
  Converting to 4D format...
    2D (358, 4096) -> 4D (1, 1, 358, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 358 Ã— 4096
    Wrote 1,466,368 float32 elements
  File saved successfully
  File size: 5,865,492 bytes
  Expected size: 5,865,492 bytes
  âœ“ File size verification passed
  Memory usage: 5.59 MB
  Save completed: /dev/shm/matrix_shards/layers_29_self_attn_v_proj_weight_shard_1.bin
  Added RAM path to file list
Processing shard 2 for node 192.168.2.101
  Remote node 192.168.2.101: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_29_self_attn_v_proj_weight_shard_2.bin
Saving matrix to binary file: matrix_shards/layers_29_self_attn_v_proj_weight_shard_2.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([153, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(153, 4096), dtype=float32
  Converting to 4D format...
    2D (153, 4096) -> 4D (1, 1, 153, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 153 Ã— 4096
    Wrote 626,688 float32 elements
  File saved successfully
  File size: 2,506,772 bytes
  Expected size: 2,506,772 bytes
  âœ“ File size verification passed
  Memory usage: 2.39 MB
  Save completed: matrix_shards/layers_29_self_attn_v_proj_weight_shard_2.bin
DEBUG::: layers_29_self_attn_v_proj_weight_shard_2.bin
  Step 2: Sending file to remote node 192.168.2.101
ðŸ“¤ Sent file layers_29_self_attn_v_proj_weight_shard_2.bin to 192.168.2.101
âœ… Received layers_29_self_attn_v_proj_weight_shard_2.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_29_self_attn_v_proj_weight_shard_2.bin
Processing shard 3 for node 192.168.2.104
  Remote node 192.168.2.104: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_29_self_attn_v_proj_weight_shard_3.bin
Saving matrix to binary file: matrix_shards/layers_29_self_attn_v_proj_weight_shard_3.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([155, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(155, 4096), dtype=float32
  Converting to 4D format...
    2D (155, 4096) -> 4D (1, 1, 155, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 155 Ã— 4096
    Wrote 634,880 float32 elements
  File saved successfully
  File size: 2,539,540 bytes
  Expected size: 2,539,540 bytes
  âœ“ File size verification passed
  Memory usage: 2.42 MB
  Save completed: matrix_shards/layers_29_self_attn_v_proj_weight_shard_3.bin
DEBUG::: layers_29_self_attn_v_proj_weight_shard_3.bin
  Step 2: Sending file to remote node 192.168.2.104
ðŸ“¤ Sent file layers_29_self_attn_v_proj_weight_shard_3.bin to 192.168.2.104
âœ… Received layers_29_self_attn_v_proj_weight_shard_3.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_29_self_attn_v_proj_weight_shard_3.bin
Distribution complete: 4 shards saved and distributed

============================================================
ðŸš€ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embedding_matrix
Matrix B: layers_29_self_attn_v_proj_weight
Operation: mul
Transpose A: False, Transpose B: True
Send back result: True
Number of shards: 4

ðŸ“¤ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_self_attn_v_proj_weight_shard_0.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_self_attn_v_proj_weight_shard_1.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_self_attn_v_proj_weight_shard_2.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_self_attn_v_proj_weight_shard_3.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.104

â³ WAITING FOR ACKS FROM NODES (4)
âœ… Received ACK_matrixOp_complete 1/4
âœ… Received ACK_matrixOp_complete 2/4
âœ… Received ACK_matrixOp_complete 3/4
âœ… Received ACK_matrixOp_complete 4/4
âœ… All ACKs received!

============================================================
âœ… CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.02 seconds

ðŸ“Š Result base: layers_29_self_attn_v_proj_weightxinput_token_embedding_matrix (send_back=True)
âœ… Received ACK_combined_matrix_saved 1/1
âœ… All ACKs received!
âœ… Loaded /dev/shm/matrix_shards/layers_29_self_attn_v_proj_weightxinput_token_embedding_matrix_combined.bin
  Original dims: [1, 1, 18, 1024]
  Result tensor shape: torch.Size([18, 1024]), size: 73,728 bytes
  Data range: [-0.037847, 0.037624]
   Computing GQA attention with TORCH...
attn_output_flat shape : torch.Size([18, 4096])
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
Preparing full matrix: attn_output_flat.bin
Local paths - DISK: matrix_shards/attn_output_flat.bin, RAM: /dev/shm/matrix_shards/attn_output_flat.bin
Saving to local storage...
Saving matrix to binary file: matrix_shards/attn_output_flat.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 4096), dtype=float32
  Converting to 4D format...
    2D (18, 4096) -> 4D (1, 1, 18, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 4096
    Wrote 73,728 float32 elements
  File saved successfully
  File size: 294,932 bytes
  Expected size: 294,932 bytes
  âœ“ File size verification passed
  Memory usage: 0.28 MB
  Save completed: matrix_shards/attn_output_flat.bin
Saving matrix to binary file: /dev/shm/matrix_shards/attn_output_flat.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 4096), dtype=float32
  Converting to 4D format...
    2D (18, 4096) -> 4D (1, 1, 18, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 4096
    Wrote 73,728 float32 elements
  File saved successfully
  File size: 294,932 bytes
  Expected size: 294,932 bytes
  âœ“ File size verification passed
  Memory usage: 0.28 MB
  Save completed: /dev/shm/matrix_shards/attn_output_flat.bin
Remote paths - RAM: /dev/shm/matrix_shards/attn_output_flat.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/attn_output_flat.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
ðŸ“¤ Sent file attn_output_flat.bin to 192.168.2.104
âœ… Received attn_output_flat.bin 1/1
âœ… All ACKs received!
Sending to 192.168.2.101
ðŸ“¤ Sent file attn_output_flat.bin to 192.168.2.101
âœ… Received attn_output_flat.bin 1/1
âœ… All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_29_mlp_gate_proj_weight
   Split Matrix: True
   Dimension: 0

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([5017, 4096])
  Node 1: shard shape torch.Size([5017, 4096])
  Node 2: shard shape torch.Size([2150, 4096])
  Node 3: shard shape torch.Size([2152, 4096])
Starting distribution of 4 shards to 3 unique nodes
Processing shard 0 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_29_mlp_gate_proj_weight_shard_0.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_29_mlp_gate_proj_weight_shard_0.bin
Saving matrix to binary file: matrix_shards/layers_29_mlp_gate_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: matrix_shards/layers_29_mlp_gate_proj_weight_shard_0.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_29_mlp_gate_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: /dev/shm/matrix_shards/layers_29_mlp_gate_proj_weight_shard_0.bin
  Added RAM path to file list
Processing shard 1 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_29_mlp_gate_proj_weight_shard_1.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_29_mlp_gate_proj_weight_shard_1.bin
Saving matrix to binary file: matrix_shards/layers_29_mlp_gate_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: matrix_shards/layers_29_mlp_gate_proj_weight_shard_1.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_29_mlp_gate_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: /dev/shm/matrix_shards/layers_29_mlp_gate_proj_weight_shard_1.bin
  Added RAM path to file list
Processing shard 2 for node 192.168.2.101
  Remote node 192.168.2.101: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_29_mlp_gate_proj_weight_shard_2.bin
Saving matrix to binary file: matrix_shards/layers_29_mlp_gate_proj_weight_shard_2.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([2150, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(2150, 4096), dtype=float32
  Converting to 4D format...
    2D (2150, 4096) -> 4D (1, 1, 2150, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 2150 Ã— 4096
    Wrote 8,806,400 float32 elements
  File saved successfully
  File size: 35,225,620 bytes
  Expected size: 35,225,620 bytes
  âœ“ File size verification passed
  Memory usage: 33.59 MB
  Save completed: matrix_shards/layers_29_mlp_gate_proj_weight_shard_2.bin
DEBUG::: layers_29_mlp_gate_proj_weight_shard_2.bin
  Step 2: Sending file to remote node 192.168.2.101
ðŸ“¤ Sent file layers_29_mlp_gate_proj_weight_shard_2.bin to 192.168.2.101
âœ… Received layers_29_mlp_gate_proj_weight_shard_2.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_29_mlp_gate_proj_weight_shard_2.bin
Processing shard 3 for node 192.168.2.104
  Remote node 192.168.2.104: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_29_mlp_gate_proj_weight_shard_3.bin
Saving matrix to binary file: matrix_shards/layers_29_mlp_gate_proj_weight_shard_3.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([2152, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(2152, 4096), dtype=float32
  Converting to 4D format...
    2D (2152, 4096) -> 4D (1, 1, 2152, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 2152 Ã— 4096
    Wrote 8,814,592 float32 elements
  File saved successfully
  File size: 35,258,388 bytes
  Expected size: 35,258,388 bytes
  âœ“ File size verification passed
  Memory usage: 33.63 MB
  Save completed: matrix_shards/layers_29_mlp_gate_proj_weight_shard_3.bin
DEBUG::: layers_29_mlp_gate_proj_weight_shard_3.bin
  Step 2: Sending file to remote node 192.168.2.104
ðŸ“¤ Sent file layers_29_mlp_gate_proj_weight_shard_3.bin to 192.168.2.104
âœ… Received layers_29_mlp_gate_proj_weight_shard_3.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_29_mlp_gate_proj_weight_shard_3.bin
Distribution complete: 4 shards saved and distributed

============================================================
ðŸš€ STARTING CLUSTER OPERATION
============================================================
Matrix A: attn_output_flat
Matrix B: layers_29_mlp_gate_proj_weight
Operation: mul
Transpose A: False, Transpose B: True
Send back result: True
Number of shards: 4

ðŸ“¤ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_mlp_gate_proj_weight_shard_0.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_mlp_gate_proj_weight_shard_1.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_mlp_gate_proj_weight_shard_2.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_mlp_gate_proj_weight_shard_3.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.104

â³ WAITING FOR ACKS FROM NODES (4)
âœ… Received ACK_matrixOp_complete 1/4
âœ… Received ACK_matrixOp_complete 2/4
âœ… Received ACK_matrixOp_complete 3/4
âœ… Received ACK_matrixOp_complete 4/4
âœ… All ACKs received!

============================================================
âœ… CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.14 seconds

ðŸ“Š Result base: layers_29_mlp_gate_proj_weightxattn_output_flat (send_back=True)
âœ… Loaded /dev/shm/matrix_shards/layers_29_mlp_gate_proj_weightxattn_output_flat_combined.bin
  Original dims: [1, 1, 18, 14336]
  Result tensor shape: torch.Size([18, 14336]), size: 1,032,192 bytes
  Data range: [-0.028965, 0.034544]
   Running MLP up projection...
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_29_mlp_up_proj_weight
   Split Matrix: True
   Dimension: 0

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([5017, 4096])
  Node 1: shard shape torch.Size([5017, 4096])
  Node 2: shard shape torch.Size([2150, 4096])
  Node 3: shard shape torch.Size([2152, 4096])
Starting distribution of 4 shards to 3 unique nodes
Processing shard 0 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_29_mlp_up_proj_weight_shard_0.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_29_mlp_up_proj_weight_shard_0.bin
Saving matrix to binary file: matrix_shards/layers_29_mlp_up_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: matrix_shards/layers_29_mlp_up_proj_weight_shard_0.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_29_mlp_up_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: /dev/shm/matrix_shards/layers_29_mlp_up_proj_weight_shard_0.bin
  Added RAM path to file list
Processing shard 1 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_29_mlp_up_proj_weight_shard_1.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_29_mlp_up_proj_weight_shard_1.bin
Saving matrix to binary file: matrix_shards/layers_29_mlp_up_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: matrix_shards/layers_29_mlp_up_proj_weight_shard_1.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_29_mlp_up_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: /dev/shm/matrix_shards/layers_29_mlp_up_proj_weight_shard_1.bin
  Added RAM path to file list
Processing shard 2 for node 192.168.2.101
  Remote node 192.168.2.101: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_29_mlp_up_proj_weight_shard_2.bin
Saving matrix to binary file: matrix_shards/layers_29_mlp_up_proj_weight_shard_2.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([2150, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(2150, 4096), dtype=float32
  Converting to 4D format...
    2D (2150, 4096) -> 4D (1, 1, 2150, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 2150 Ã— 4096
    Wrote 8,806,400 float32 elements
  File saved successfully
  File size: 35,225,620 bytes
  Expected size: 35,225,620 bytes
  âœ“ File size verification passed
  Memory usage: 33.59 MB
  Save completed: matrix_shards/layers_29_mlp_up_proj_weight_shard_2.bin
DEBUG::: layers_29_mlp_up_proj_weight_shard_2.bin
  Step 2: Sending file to remote node 192.168.2.101
ðŸ“¤ Sent file layers_29_mlp_up_proj_weight_shard_2.bin to 192.168.2.101
âœ… Received layers_29_mlp_up_proj_weight_shard_2.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_29_mlp_up_proj_weight_shard_2.bin
Processing shard 3 for node 192.168.2.104
  Remote node 192.168.2.104: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_29_mlp_up_proj_weight_shard_3.bin
Saving matrix to binary file: matrix_shards/layers_29_mlp_up_proj_weight_shard_3.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([2152, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(2152, 4096), dtype=float32
  Converting to 4D format...
    2D (2152, 4096) -> 4D (1, 1, 2152, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 2152 Ã— 4096
    Wrote 8,814,592 float32 elements
  File saved successfully
  File size: 35,258,388 bytes
  Expected size: 35,258,388 bytes
  âœ“ File size verification passed
  Memory usage: 33.63 MB
  Save completed: matrix_shards/layers_29_mlp_up_proj_weight_shard_3.bin
DEBUG::: layers_29_mlp_up_proj_weight_shard_3.bin
  Step 2: Sending file to remote node 192.168.2.104
ðŸ“¤ Sent file layers_29_mlp_up_proj_weight_shard_3.bin to 192.168.2.104
âœ… Received layers_29_mlp_up_proj_weight_shard_3.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_29_mlp_up_proj_weight_shard_3.bin
Distribution complete: 4 shards saved and distributed

============================================================
ðŸš€ STARTING CLUSTER OPERATION
============================================================
Matrix A: attn_output_flat
Matrix B: layers_29_mlp_up_proj_weight
Operation: mul
Transpose A: False, Transpose B: True
Send back result: True
Number of shards: 4

ðŸ“¤ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_mlp_up_proj_weight_shard_0.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_mlp_up_proj_weight_shard_1.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_mlp_up_proj_weight_shard_2.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_mlp_up_proj_weight_shard_3.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.104

â³ WAITING FOR ACKS FROM NODES (4)
âœ… Received ACK_matrixOp_complete 1/4
âœ… Received ACK_matrixOp_complete 2/4
âœ… Received ACK_matrixOp_complete 3/4
âœ… Received ACK_matrixOp_complete 4/4
âœ… All ACKs received!

============================================================
âœ… CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.13 seconds

ðŸ“Š Result base: layers_29_mlp_up_proj_weightxattn_output_flat (send_back=True)
âœ… Loaded /dev/shm/matrix_shards/layers_29_mlp_up_proj_weightxattn_output_flat_combined.bin
  Original dims: [1, 1, 18, 14336]
  Result tensor shape: torch.Size([18, 14336]), size: 1,032,192 bytes
  Data range: [-0.019659, 0.017753]
   Up projection result shape: torch.Size([18, 14336])
   Applying SiLU activation to gate...
   Element-wise multiplication (gate_silu * up)...
   MLP intermediate shape: torch.Size([18, 14336])
   Running MLP down projection...
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
Preparing full matrix: mlp_intermediate_cluster.bin
Local paths - DISK: matrix_shards/mlp_intermediate_cluster.bin, RAM: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin
Saving to local storage...
Saving matrix to binary file: matrix_shards/mlp_intermediate_cluster.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 14336), dtype=float32
  Converting to 4D format...
    2D (18, 14336) -> 4D (1, 1, 18, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 14336
    Wrote 258,048 float32 elements
  File saved successfully
  File size: 1,032,212 bytes
  Expected size: 1,032,212 bytes
  âœ“ File size verification passed
  Memory usage: 0.98 MB
  Save completed: matrix_shards/mlp_intermediate_cluster.bin
Saving matrix to binary file: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 14336), dtype=float32
  Converting to 4D format...
    2D (18, 14336) -> 4D (1, 1, 18, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 14336
    Wrote 258,048 float32 elements
  File saved successfully
  File size: 1,032,212 bytes
  Expected size: 1,032,212 bytes
  âœ“ File size verification passed
  Memory usage: 0.98 MB
  Save completed: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin
Remote paths - RAM: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/mlp_intermediate_cluster.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
ðŸ“¤ Sent file mlp_intermediate_cluster.bin to 192.168.2.104
âœ… Received mlp_intermediate_cluster.bin 1/1
âœ… All ACKs received!
Sending to 192.168.2.101
ðŸ“¤ Sent file mlp_intermediate_cluster.bin to 192.168.2.101
âœ… Received mlp_intermediate_cluster.bin 1/1
âœ… All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_29_mlp_down_proj_weight
   Split Matrix: True
   Dimension: 0

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([1433, 14336])
  Node 1: shard shape torch.Size([1433, 14336])
  Node 2: shard shape torch.Size([614, 14336])
  Node 3: shard shape torch.Size([616, 14336])
Starting distribution of 4 shards to 3 unique nodes
Processing shard 0 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_29_mlp_down_proj_weight_shard_0.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_29_mlp_down_proj_weight_shard_0.bin
Saving matrix to binary file: matrix_shards/layers_29_mlp_down_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([1433, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(1433, 14336), dtype=float32
  Converting to 4D format...
    2D (1433, 14336) -> 4D (1, 1, 1433, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 1433 Ã— 14336
    Wrote 20,543,488 float32 elements
  File saved successfully
  File size: 82,173,972 bytes
  Expected size: 82,173,972 bytes
  âœ“ File size verification passed
  Memory usage: 78.37 MB
  Save completed: matrix_shards/layers_29_mlp_down_proj_weight_shard_0.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_29_mlp_down_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([1433, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(1433, 14336), dtype=float32
  Converting to 4D format...
    2D (1433, 14336) -> 4D (1, 1, 1433, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 1433 Ã— 14336
    Wrote 20,543,488 float32 elements
  File saved successfully
  File size: 82,173,972 bytes
  Expected size: 82,173,972 bytes
  âœ“ File size verification passed
  Memory usage: 78.37 MB
  Save completed: /dev/shm/matrix_shards/layers_29_mlp_down_proj_weight_shard_0.bin
  Added RAM path to file list
Processing shard 1 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_29_mlp_down_proj_weight_shard_1.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_29_mlp_down_proj_weight_shard_1.bin
Saving matrix to binary file: matrix_shards/layers_29_mlp_down_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([1433, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(1433, 14336), dtype=float32
  Converting to 4D format...
    2D (1433, 14336) -> 4D (1, 1, 1433, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 1433 Ã— 14336
    Wrote 20,543,488 float32 elements
  File saved successfully
  File size: 82,173,972 bytes
  Expected size: 82,173,972 bytes
  âœ“ File size verification passed
  Memory usage: 78.37 MB
  Save completed: matrix_shards/layers_29_mlp_down_proj_weight_shard_1.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_29_mlp_down_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([1433, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(1433, 14336), dtype=float32
  Converting to 4D format...
    2D (1433, 14336) -> 4D (1, 1, 1433, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 1433 Ã— 14336
    Wrote 20,543,488 float32 elements
  File saved successfully
  File size: 82,173,972 bytes
  Expected size: 82,173,972 bytes
  âœ“ File size verification passed
  Memory usage: 78.37 MB
  Save completed: /dev/shm/matrix_shards/layers_29_mlp_down_proj_weight_shard_1.bin
  Added RAM path to file list
Processing shard 2 for node 192.168.2.101
  Remote node 192.168.2.101: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_29_mlp_down_proj_weight_shard_2.bin
Saving matrix to binary file: matrix_shards/layers_29_mlp_down_proj_weight_shard_2.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([614, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(614, 14336), dtype=float32
  Converting to 4D format...
    2D (614, 14336) -> 4D (1, 1, 614, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 614 Ã— 14336
    Wrote 8,802,304 float32 elements
  File saved successfully
  File size: 35,209,236 bytes
  Expected size: 35,209,236 bytes
  âœ“ File size verification passed
  Memory usage: 33.58 MB
  Save completed: matrix_shards/layers_29_mlp_down_proj_weight_shard_2.bin
DEBUG::: layers_29_mlp_down_proj_weight_shard_2.bin
  Step 2: Sending file to remote node 192.168.2.101
ðŸ“¤ Sent file layers_29_mlp_down_proj_weight_shard_2.bin to 192.168.2.101
âœ… Received layers_29_mlp_down_proj_weight_shard_2.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_29_mlp_down_proj_weight_shard_2.bin
Processing shard 3 for node 192.168.2.104
  Remote node 192.168.2.104: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_29_mlp_down_proj_weight_shard_3.bin
Saving matrix to binary file: matrix_shards/layers_29_mlp_down_proj_weight_shard_3.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([616, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(616, 14336), dtype=float32
  Converting to 4D format...
    2D (616, 14336) -> 4D (1, 1, 616, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 616 Ã— 14336
    Wrote 8,830,976 float32 elements
  File saved successfully
  File size: 35,323,924 bytes
  Expected size: 35,323,924 bytes
  âœ“ File size verification passed
  Memory usage: 33.69 MB
  Save completed: matrix_shards/layers_29_mlp_down_proj_weight_shard_3.bin
DEBUG::: layers_29_mlp_down_proj_weight_shard_3.bin
  Step 2: Sending file to remote node 192.168.2.104
ðŸ“¤ Sent file layers_29_mlp_down_proj_weight_shard_3.bin to 192.168.2.104
âœ… Received layers_29_mlp_down_proj_weight_shard_3.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_29_mlp_down_proj_weight_shard_3.bin
Distribution complete: 4 shards saved and distributed

============================================================
ðŸš€ STARTING CLUSTER OPERATION
============================================================
Matrix A: mlp_intermediate_cluster
Matrix B: layers_29_mlp_down_proj_weight
Operation: mul
Transpose A: False, Transpose B: True
Send back result: True
Number of shards: 4

ðŸ“¤ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_mlp_down_proj_weight_shard_0.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_mlp_down_proj_weight_shard_1.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_mlp_down_proj_weight_shard_2.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin
  Matrix B path: /dev/shm/matrix_shards/layers_29_mlp_down_proj_weight_shard_3.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.104

â³ WAITING FOR ACKS FROM NODES (4)
âœ… Received ACK_matrixOp_complete 1/4
âœ… Received ACK_matrixOp_complete 2/4
âœ… Received ACK_matrixOp_complete 3/4
âœ… Received ACK_matrixOp_complete 4/4
âœ… All ACKs received!

============================================================
âœ… CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.13 seconds

ðŸ“Š Result base: layers_29_mlp_down_proj_weightxmlp_intermediate_cluster (send_back=True)
âœ… Loaded /dev/shm/matrix_shards/layers_29_mlp_down_proj_weightxmlp_intermediate_cluster_combined.bin
  Original dims: [1, 1, 18, 4096]
  Result tensor shape: torch.Size([18, 4096]), size: 294,912 bytes
  Data range: [-0.000065, 0.000071]
   Adding residual connection (MLP + attention)...
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([18, 1433])
  Node 1: shard shape torch.Size([18, 1433])
  Node 2: shard shape torch.Size([18, 614])
  Node 3: shard shape torch.Size([18, 616])
Starting distribution of 4 shards to 3 unique nodes
Processing shard 0 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/mlp_output_cluster_shard_0.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/mlp_output_cluster_shard_0.bin
Saving matrix to binary file: matrix_shards/mlp_output_cluster_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: matrix_shards/mlp_output_cluster_shard_0.bin
Saving matrix to binary file: /dev/shm/matrix_shards/mlp_output_cluster_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: /dev/shm/matrix_shards/mlp_output_cluster_shard_0.bin
  Added RAM path to file list
Processing shard 1 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/mlp_output_cluster_shard_1.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/mlp_output_cluster_shard_1.bin
Saving matrix to binary file: matrix_shards/mlp_output_cluster_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: matrix_shards/mlp_output_cluster_shard_1.bin
Saving matrix to binary file: /dev/shm/matrix_shards/mlp_output_cluster_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: /dev/shm/matrix_shards/mlp_output_cluster_shard_1.bin
  Added RAM path to file list
Processing shard 2 for node 192.168.2.101
  Remote node 192.168.2.101: Beginning distribution
  Step 1: Saving locally to matrix_shards/mlp_output_cluster_shard_2.bin
Saving matrix to binary file: matrix_shards/mlp_output_cluster_shard_2.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 614]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 614), dtype=float32
  Converting to 4D format...
    2D (18, 614) -> 4D (1, 1, 18, 614)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 614
    Wrote 11,052 float32 elements
  File saved successfully
  File size: 44,228 bytes
  Expected size: 44,228 bytes
  âœ“ File size verification passed
  Memory usage: 0.04 MB
  Save completed: matrix_shards/mlp_output_cluster_shard_2.bin
DEBUG::: mlp_output_cluster_shard_2.bin
  Step 2: Sending file to remote node 192.168.2.101
ðŸ“¤ Sent file mlp_output_cluster_shard_2.bin to 192.168.2.101
âœ… Received mlp_output_cluster_shard_2.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/mlp_output_cluster_shard_2.bin
Processing shard 3 for node 192.168.2.104
  Remote node 192.168.2.104: Beginning distribution
  Step 1: Saving locally to matrix_shards/mlp_output_cluster_shard_3.bin
Saving matrix to binary file: matrix_shards/mlp_output_cluster_shard_3.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 616]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 616), dtype=float32
  Converting to 4D format...
    2D (18, 616) -> 4D (1, 1, 18, 616)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 616
    Wrote 11,088 float32 elements
  File saved successfully
  File size: 44,372 bytes
  Expected size: 44,372 bytes
  âœ“ File size verification passed
  Memory usage: 0.04 MB
  Save completed: matrix_shards/mlp_output_cluster_shard_3.bin
DEBUG::: mlp_output_cluster_shard_3.bin
  Step 2: Sending file to remote node 192.168.2.104
ðŸ“¤ Sent file mlp_output_cluster_shard_3.bin to 192.168.2.104
âœ… Received mlp_output_cluster_shard_3.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/mlp_output_cluster_shard_3.bin
Distribution complete: 4 shards saved and distributed
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([18, 1433])
  Node 1: shard shape torch.Size([18, 1433])
  Node 2: shard shape torch.Size([18, 614])
  Node 3: shard shape torch.Size([18, 616])
Starting distribution of 4 shards to 3 unique nodes
Processing shard 0 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/attn_outpuattn_output_clustert_flat_shard_0.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_0.bin
Saving matrix to binary file: matrix_shards/attn_outpuattn_output_clustert_flat_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: matrix_shards/attn_outpuattn_output_clustert_flat_shard_0.bin
Saving matrix to binary file: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_0.bin
  Added RAM path to file list
Processing shard 1 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/attn_outpuattn_output_clustert_flat_shard_1.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_1.bin
Saving matrix to binary file: matrix_shards/attn_outpuattn_output_clustert_flat_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: matrix_shards/attn_outpuattn_output_clustert_flat_shard_1.bin
Saving matrix to binary file: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_1.bin
  Added RAM path to file list
Processing shard 2 for node 192.168.2.101
  Remote node 192.168.2.101: Beginning distribution
  Step 1: Saving locally to matrix_shards/attn_outpuattn_output_clustert_flat_shard_2.bin
Saving matrix to binary file: matrix_shards/attn_outpuattn_output_clustert_flat_shard_2.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 614]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 614), dtype=float32
  Converting to 4D format...
    2D (18, 614) -> 4D (1, 1, 18, 614)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 614
    Wrote 11,052 float32 elements
  File saved successfully
  File size: 44,228 bytes
  Expected size: 44,228 bytes
  âœ“ File size verification passed
  Memory usage: 0.04 MB
  Save completed: matrix_shards/attn_outpuattn_output_clustert_flat_shard_2.bin
DEBUG::: attn_outpuattn_output_clustert_flat_shard_2.bin
  Step 2: Sending file to remote node 192.168.2.101
ðŸ“¤ Sent file attn_outpuattn_output_clustert_flat_shard_2.bin to 192.168.2.101
âœ… Received attn_outpuattn_output_clustert_flat_shard_2.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_2.bin
Processing shard 3 for node 192.168.2.104
  Remote node 192.168.2.104: Beginning distribution
  Step 1: Saving locally to matrix_shards/attn_outpuattn_output_clustert_flat_shard_3.bin
Saving matrix to binary file: matrix_shards/attn_outpuattn_output_clustert_flat_shard_3.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 616]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 616), dtype=float32
  Converting to 4D format...
    2D (18, 616) -> 4D (1, 1, 18, 616)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 616
    Wrote 11,088 float32 elements
  File saved successfully
  File size: 44,372 bytes
  Expected size: 44,372 bytes
  âœ“ File size verification passed
  Memory usage: 0.04 MB
  Save completed: matrix_shards/attn_outpuattn_output_clustert_flat_shard_3.bin
DEBUG::: attn_outpuattn_output_clustert_flat_shard_3.bin
  Step 2: Sending file to remote node 192.168.2.104
ðŸ“¤ Sent file attn_outpuattn_output_clustert_flat_shard_3.bin to 192.168.2.104
âœ… Received attn_outpuattn_output_clustert_flat_shard_3.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_3.bin
Distribution complete: 4 shards saved and distributed

============================================================
ðŸš€ STARTING CLUSTER OPERATION
============================================================
Matrix A: mlp_output_cluster
Matrix B: attn_outpuattn_output_clustert_flat
Operation: add
Transpose A: False, Transpose B: True
Send back result: True
Number of shards: 4

ðŸ“¤ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/mlp_output_cluster_shard_0.bin
  Matrix B path: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_0.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/mlp_output_cluster_shard_1.bin
  Matrix B path: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_1.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/mlp_output_cluster_shard_2.bin
  Matrix B path: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_2.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: False (GPU #0)
  Next GPU for this node will be: #0
  Matrix A path: /dev/shm/matrix_shards/mlp_output_cluster_shard_3.bin
  Matrix B path: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_3.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.104

â³ WAITING FOR ACKS FROM NODES (4)
âœ… Received ACK_matrixOp_complete 1/4
âœ… Received ACK_matrixOp_complete 2/4
âœ… Received ACK_matrixOp_complete 3/4
âœ… Received ACK_matrixOp_complete 4/4
âœ… All ACKs received!

============================================================
âœ… CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.03 seconds

ðŸ“Š Result base: attn_outpuattn_output_clustert_flatxmlp_output_cluster (send_back=True)
âœ… Loaded /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flatxmlp_output_cluster_combined.bin
  Original dims: [1, 1, 18, 4096]
  Result tensor shape: torch.Size([18, 4096]), size: 294,912 bytes
  Data range: [-0.020036, 0.020191]
   Applying post-attention layer normalization...
   Loaded post-attention layer norm weights: shape torch.Size([4096])
   Applied layer normalization
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([18, 1433])
  Node 1: shard shape torch.Size([18, 1433])
  Node 2: shard shape torch.Size([18, 614])
  Node 3: shard shape torch.Size([18, 616])
   Loading attention weights for layer 30...
   Running Q/K/V projections with CLUSTER...
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_30_self_attn_q_proj_weight
   Split Matrix: True
   Dimension: 1

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([4096, 1433])
  Node 1: shard shape torch.Size([4096, 1433])
  Node 2: shard shape torch.Size([4096, 614])
  Node 3: shard shape torch.Size([4096, 616])
Starting distribution of 4 shards to 3 unique nodes
Processing shard 0 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_30_self_attn_q_proj_weight_shard_0.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_30_self_attn_q_proj_weight_shard_0.bin
Saving matrix to binary file: matrix_shards/layers_30_self_attn_q_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([4096, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(4096, 1433), dtype=float32
  Converting to 4D format...
    2D (4096, 1433) -> 4D (1, 1, 4096, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 4096 Ã— 1433
    Wrote 5,869,568 float32 elements
  File saved successfully
  File size: 23,478,292 bytes
  Expected size: 23,478,292 bytes
  âœ“ File size verification passed
  Memory usage: 22.39 MB
  Save completed: matrix_shards/layers_30_self_attn_q_proj_weight_shard_0.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_30_self_attn_q_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([4096, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(4096, 1433), dtype=float32
  Converting to 4D format...
    2D (4096, 1433) -> 4D (1, 1, 4096, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 4096 Ã— 1433
    Wrote 5,869,568 float32 elements
  File saved successfully
  File size: 23,478,292 bytes
  Expected size: 23,478,292 bytes
  âœ“ File size verification passed
  Memory usage: 22.39 MB
  Save completed: /dev/shm/matrix_shards/layers_30_self_attn_q_proj_weight_shard_0.bin
  Added RAM path to file list
Processing shard 1 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_30_self_attn_q_proj_weight_shard_1.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_30_self_attn_q_proj_weight_shard_1.bin
Saving matrix to binary file: matrix_shards/layers_30_self_attn_q_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([4096, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(4096, 1433), dtype=float32
  Converting to 4D format...
    2D (4096, 1433) -> 4D (1, 1, 4096, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 4096 Ã— 1433
    Wrote 5,869,568 float32 elements
  File saved successfully
  File size: 23,478,292 bytes
  Expected size: 23,478,292 bytes
  âœ“ File size verification passed
  Memory usage: 22.39 MB
  Save completed: matrix_shards/layers_30_self_attn_q_proj_weight_shard_1.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_30_self_attn_q_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([4096, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(4096, 1433), dtype=float32
  Converting to 4D format...
    2D (4096, 1433) -> 4D (1, 1, 4096, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 4096 Ã— 1433
    Wrote 5,869,568 float32 elements
  File saved successfully
  File size: 23,478,292 bytes
  Expected size: 23,478,292 bytes
  âœ“ File size verification passed
  Memory usage: 22.39 MB
  Save completed: /dev/shm/matrix_shards/layers_30_self_attn_q_proj_weight_shard_1.bin
  Added RAM path to file list
Processing shard 2 for node 192.168.2.101
  Remote node 192.168.2.101: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_30_self_attn_q_proj_weight_shard_2.bin
Saving matrix to binary file: matrix_shards/layers_30_self_attn_q_proj_weight_shard_2.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([4096, 614]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(4096, 614), dtype=float32
  Converting to 4D format...
    2D (4096, 614) -> 4D (1, 1, 4096, 614)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 4096 Ã— 614
    Wrote 2,514,944 float32 elements
  File saved successfully
  File size: 10,059,796 bytes
  Expected size: 10,059,796 bytes
  âœ“ File size verification passed
  Memory usage: 9.59 MB
  Save completed: matrix_shards/layers_30_self_attn_q_proj_weight_shard_2.bin
DEBUG::: layers_30_self_attn_q_proj_weight_shard_2.bin
  Step 2: Sending file to remote node 192.168.2.101
ðŸ“¤ Sent file layers_30_self_attn_q_proj_weight_shard_2.bin to 192.168.2.101
âœ… Received layers_30_self_attn_q_proj_weight_shard_2.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_30_self_attn_q_proj_weight_shard_2.bin
Processing shard 3 for node 192.168.2.104
  Remote node 192.168.2.104: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_30_self_attn_q_proj_weight_shard_3.bin
Saving matrix to binary file: matrix_shards/layers_30_self_attn_q_proj_weight_shard_3.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([4096, 616]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(4096, 616), dtype=float32
  Converting to 4D format...
    2D (4096, 616) -> 4D (1, 1, 4096, 616)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 4096 Ã— 616
    Wrote 2,523,136 float32 elements
  File saved successfully
  File size: 10,092,564 bytes
  Expected size: 10,092,564 bytes
  âœ“ File size verification passed
  Memory usage: 9.63 MB
  Save completed: matrix_shards/layers_30_self_attn_q_proj_weight_shard_3.bin
DEBUG::: layers_30_self_attn_q_proj_weight_shard_3.bin
  Step 2: Sending file to remote node 192.168.2.104
ðŸ“¤ Sent file layers_30_self_attn_q_proj_weight_shard_3.bin to 192.168.2.104
âœ… Received layers_30_self_attn_q_proj_weight_shard_3.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_30_self_attn_q_proj_weight_shard_3.bin
Distribution complete: 4 shards saved and distributed

============================================================
ðŸš€ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embedding_matrix
Matrix B: layers_30_self_attn_q_proj_weight
Operation: mul
Transpose A: False, Transpose B: False
Send back result: True
Number of shards: 4

ðŸ“¤ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_self_attn_q_proj_weight_shard_0.bin
  Final transpose flags - A: false, B: true
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_self_attn_q_proj_weight_shard_1.bin
  Final transpose flags - A: false, B: true
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_self_attn_q_proj_weight_shard_2.bin
  Final transpose flags - A: false, B: true
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_self_attn_q_proj_weight_shard_3.bin
  Final transpose flags - A: false, B: true
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.104

â³ WAITING FOR ACKS FROM NODES (4)
âœ… Received ACK_matrixOp_complete 1/4
âœ… Received ACK_matrixOp_complete 2/4
âœ… Received ACK_matrixOp_complete 3/4
âœ… Received ACK_matrixOp_complete 4/4
âœ… All ACKs received!

============================================================
âœ… CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.51 seconds

ðŸ“Š Result base: layers_30_self_attn_q_proj_weightxinput_token_embedding_matrix (send_back=True)
âœ… Loaded /dev/shm/matrix_shards/layers_30_self_attn_q_proj_weightxinput_token_embedding_matrix_combined.bin
  Original dims: [1, 1, 18, 4096]
  Result tensor shape: torch.Size([18, 4096]), size: 294,912 bytes
  Data range: [-0.043738, 0.059497]
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_30_self_attn_k_proj_weight
   Split Matrix: True
   Dimension: 0

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([358, 4096])
  Node 1: shard shape torch.Size([358, 4096])
  Node 2: shard shape torch.Size([153, 4096])
  Node 3: shard shape torch.Size([155, 4096])
Starting distribution of 4 shards to 3 unique nodes
Processing shard 0 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_30_self_attn_k_proj_weight_shard_0.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_30_self_attn_k_proj_weight_shard_0.bin
Saving matrix to binary file: matrix_shards/layers_30_self_attn_k_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([358, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(358, 4096), dtype=float32
  Converting to 4D format...
    2D (358, 4096) -> 4D (1, 1, 358, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 358 Ã— 4096
    Wrote 1,466,368 float32 elements
  File saved successfully
  File size: 5,865,492 bytes
  Expected size: 5,865,492 bytes
  âœ“ File size verification passed
  Memory usage: 5.59 MB
  Save completed: matrix_shards/layers_30_self_attn_k_proj_weight_shard_0.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_30_self_attn_k_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([358, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(358, 4096), dtype=float32
  Converting to 4D format...
    2D (358, 4096) -> 4D (1, 1, 358, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 358 Ã— 4096
    Wrote 1,466,368 float32 elements
  File saved successfully
  File size: 5,865,492 bytes
  Expected size: 5,865,492 bytes
  âœ“ File size verification passed
  Memory usage: 5.59 MB
  Save completed: /dev/shm/matrix_shards/layers_30_self_attn_k_proj_weight_shard_0.bin
  Added RAM path to file list
Processing shard 1 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_30_self_attn_k_proj_weight_shard_1.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_30_self_attn_k_proj_weight_shard_1.bin
Saving matrix to binary file: matrix_shards/layers_30_self_attn_k_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([358, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(358, 4096), dtype=float32
  Converting to 4D format...
    2D (358, 4096) -> 4D (1, 1, 358, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 358 Ã— 4096
    Wrote 1,466,368 float32 elements
  File saved successfully
  File size: 5,865,492 bytes
  Expected size: 5,865,492 bytes
  âœ“ File size verification passed
  Memory usage: 5.59 MB
  Save completed: matrix_shards/layers_30_self_attn_k_proj_weight_shard_1.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_30_self_attn_k_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([358, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(358, 4096), dtype=float32
  Converting to 4D format...
    2D (358, 4096) -> 4D (1, 1, 358, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 358 Ã— 4096
    Wrote 1,466,368 float32 elements
  File saved successfully
  File size: 5,865,492 bytes
  Expected size: 5,865,492 bytes
  âœ“ File size verification passed
  Memory usage: 5.59 MB
  Save completed: /dev/shm/matrix_shards/layers_30_self_attn_k_proj_weight_shard_1.bin
  Added RAM path to file list
Processing shard 2 for node 192.168.2.101
  Remote node 192.168.2.101: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_30_self_attn_k_proj_weight_shard_2.bin
Saving matrix to binary file: matrix_shards/layers_30_self_attn_k_proj_weight_shard_2.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([153, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(153, 4096), dtype=float32
  Converting to 4D format...
    2D (153, 4096) -> 4D (1, 1, 153, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 153 Ã— 4096
    Wrote 626,688 float32 elements
  File saved successfully
  File size: 2,506,772 bytes
  Expected size: 2,506,772 bytes
  âœ“ File size verification passed
  Memory usage: 2.39 MB
  Save completed: matrix_shards/layers_30_self_attn_k_proj_weight_shard_2.bin
DEBUG::: layers_30_self_attn_k_proj_weight_shard_2.bin
  Step 2: Sending file to remote node 192.168.2.101
ðŸ“¤ Sent file layers_30_self_attn_k_proj_weight_shard_2.bin to 192.168.2.101
âœ… Received layers_30_self_attn_k_proj_weight_shard_2.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_30_self_attn_k_proj_weight_shard_2.bin
Processing shard 3 for node 192.168.2.104
  Remote node 192.168.2.104: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_30_self_attn_k_proj_weight_shard_3.bin
Saving matrix to binary file: matrix_shards/layers_30_self_attn_k_proj_weight_shard_3.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([155, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(155, 4096), dtype=float32
  Converting to 4D format...
    2D (155, 4096) -> 4D (1, 1, 155, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 155 Ã— 4096
    Wrote 634,880 float32 elements
  File saved successfully
  File size: 2,539,540 bytes
  Expected size: 2,539,540 bytes
  âœ“ File size verification passed
  Memory usage: 2.42 MB
  Save completed: matrix_shards/layers_30_self_attn_k_proj_weight_shard_3.bin
DEBUG::: layers_30_self_attn_k_proj_weight_shard_3.bin
  Step 2: Sending file to remote node 192.168.2.104
ðŸ“¤ Sent file layers_30_self_attn_k_proj_weight_shard_3.bin to 192.168.2.104
âœ… Received layers_30_self_attn_k_proj_weight_shard_3.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_30_self_attn_k_proj_weight_shard_3.bin
Distribution complete: 4 shards saved and distributed

============================================================
ðŸš€ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embedding_matrix
Matrix B: layers_30_self_attn_k_proj_weight
Operation: mul
Transpose A: False, Transpose B: True
Send back result: True
Number of shards: 4

ðŸ“¤ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_self_attn_k_proj_weight_shard_0.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_self_attn_k_proj_weight_shard_1.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_self_attn_k_proj_weight_shard_2.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_self_attn_k_proj_weight_shard_3.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.104

â³ WAITING FOR ACKS FROM NODES (4)
âœ… Received ACK_matrixOp_complete 1/4
âœ… Received ACK_matrixOp_complete 2/4
âœ… Received ACK_matrixOp_complete 3/4
âœ… Received ACK_matrixOp_complete 4/4
âœ… All ACKs received!

============================================================
âœ… CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.04 seconds

ðŸ“Š Result base: layers_30_self_attn_k_proj_weightxinput_token_embedding_matrix (send_back=True)
âœ… Loaded /dev/shm/matrix_shards/layers_30_self_attn_k_proj_weightxinput_token_embedding_matrix_combined.bin
  Original dims: [1, 1, 18, 1024]
  Result tensor shape: torch.Size([18, 1024]), size: 73,728 bytes
  Data range: [-0.062161, 0.057690]
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_30_self_attn_v_proj_weight
   Split Matrix: True
   Dimension: 0

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([358, 4096])
  Node 1: shard shape torch.Size([358, 4096])
  Node 2: shard shape torch.Size([153, 4096])
  Node 3: shard shape torch.Size([155, 4096])
Starting distribution of 4 shards to 3 unique nodes
Processing shard 0 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_30_self_attn_v_proj_weight_shard_0.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_30_self_attn_v_proj_weight_shard_0.bin
Saving matrix to binary file: matrix_shards/layers_30_self_attn_v_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([358, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(358, 4096), dtype=float32
  Converting to 4D format...
    2D (358, 4096) -> 4D (1, 1, 358, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 358 Ã— 4096
    Wrote 1,466,368 float32 elements
  File saved successfully
  File size: 5,865,492 bytes
  Expected size: 5,865,492 bytes
  âœ“ File size verification passed
  Memory usage: 5.59 MB
  Save completed: matrix_shards/layers_30_self_attn_v_proj_weight_shard_0.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_30_self_attn_v_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([358, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(358, 4096), dtype=float32
  Converting to 4D format...
    2D (358, 4096) -> 4D (1, 1, 358, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 358 Ã— 4096
    Wrote 1,466,368 float32 elements
  File saved successfully
  File size: 5,865,492 bytes
  Expected size: 5,865,492 bytes
  âœ“ File size verification passed
  Memory usage: 5.59 MB
  Save completed: /dev/shm/matrix_shards/layers_30_self_attn_v_proj_weight_shard_0.bin
  Added RAM path to file list
Processing shard 1 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_30_self_attn_v_proj_weight_shard_1.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_30_self_attn_v_proj_weight_shard_1.bin
Saving matrix to binary file: matrix_shards/layers_30_self_attn_v_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([358, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(358, 4096), dtype=float32
  Converting to 4D format...
    2D (358, 4096) -> 4D (1, 1, 358, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 358 Ã— 4096
    Wrote 1,466,368 float32 elements
  File saved successfully
  File size: 5,865,492 bytes
  Expected size: 5,865,492 bytes
  âœ“ File size verification passed
  Memory usage: 5.59 MB
  Save completed: matrix_shards/layers_30_self_attn_v_proj_weight_shard_1.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_30_self_attn_v_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([358, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(358, 4096), dtype=float32
  Converting to 4D format...
    2D (358, 4096) -> 4D (1, 1, 358, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 358 Ã— 4096
    Wrote 1,466,368 float32 elements
  File saved successfully
  File size: 5,865,492 bytes
  Expected size: 5,865,492 bytes
  âœ“ File size verification passed
  Memory usage: 5.59 MB
  Save completed: /dev/shm/matrix_shards/layers_30_self_attn_v_proj_weight_shard_1.bin
  Added RAM path to file list
Processing shard 2 for node 192.168.2.101
  Remote node 192.168.2.101: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_30_self_attn_v_proj_weight_shard_2.bin
Saving matrix to binary file: matrix_shards/layers_30_self_attn_v_proj_weight_shard_2.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([153, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(153, 4096), dtype=float32
  Converting to 4D format...
    2D (153, 4096) -> 4D (1, 1, 153, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 153 Ã— 4096
    Wrote 626,688 float32 elements
  File saved successfully
  File size: 2,506,772 bytes
  Expected size: 2,506,772 bytes
  âœ“ File size verification passed
  Memory usage: 2.39 MB
  Save completed: matrix_shards/layers_30_self_attn_v_proj_weight_shard_2.bin
DEBUG::: layers_30_self_attn_v_proj_weight_shard_2.bin
  Step 2: Sending file to remote node 192.168.2.101
ðŸ“¤ Sent file layers_30_self_attn_v_proj_weight_shard_2.bin to 192.168.2.101
âœ… Received layers_30_self_attn_v_proj_weight_shard_2.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_30_self_attn_v_proj_weight_shard_2.bin
Processing shard 3 for node 192.168.2.104
  Remote node 192.168.2.104: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_30_self_attn_v_proj_weight_shard_3.bin
Saving matrix to binary file: matrix_shards/layers_30_self_attn_v_proj_weight_shard_3.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([155, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(155, 4096), dtype=float32
  Converting to 4D format...
    2D (155, 4096) -> 4D (1, 1, 155, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 155 Ã— 4096
    Wrote 634,880 float32 elements
  File saved successfully
  File size: 2,539,540 bytes
  Expected size: 2,539,540 bytes
  âœ“ File size verification passed
  Memory usage: 2.42 MB
  Save completed: matrix_shards/layers_30_self_attn_v_proj_weight_shard_3.bin
DEBUG::: layers_30_self_attn_v_proj_weight_shard_3.bin
  Step 2: Sending file to remote node 192.168.2.104
ðŸ“¤ Sent file layers_30_self_attn_v_proj_weight_shard_3.bin to 192.168.2.104
âœ… Received layers_30_self_attn_v_proj_weight_shard_3.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_30_self_attn_v_proj_weight_shard_3.bin
Distribution complete: 4 shards saved and distributed

============================================================
ðŸš€ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embedding_matrix
Matrix B: layers_30_self_attn_v_proj_weight
Operation: mul
Transpose A: False, Transpose B: True
Send back result: True
Number of shards: 4

ðŸ“¤ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_self_attn_v_proj_weight_shard_0.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_self_attn_v_proj_weight_shard_1.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_self_attn_v_proj_weight_shard_2.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_self_attn_v_proj_weight_shard_3.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.104

â³ WAITING FOR ACKS FROM NODES (4)
âœ… Received ACK_matrixOp_complete 1/4
âœ… Received ACK_matrixOp_complete 2/4
âœ… Received ACK_matrixOp_complete 3/4
âœ… Received ACK_matrixOp_complete 4/4
âœ… All ACKs received!

============================================================
âœ… CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.02 seconds

ðŸ“Š Result base: layers_30_self_attn_v_proj_weightxinput_token_embedding_matrix (send_back=True)
âœ… Received ACK_combined_matrix_saved 1/1
âœ… All ACKs received!
âœ… Loaded /dev/shm/matrix_shards/layers_30_self_attn_v_proj_weightxinput_token_embedding_matrix_combined.bin
  Original dims: [1, 1, 18, 1024]
  Result tensor shape: torch.Size([18, 1024]), size: 73,728 bytes
  Data range: [-0.050761, 0.054832]
   Computing GQA attention with TORCH...
attn_output_flat shape : torch.Size([18, 4096])
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
Preparing full matrix: attn_output_flat.bin
Local paths - DISK: matrix_shards/attn_output_flat.bin, RAM: /dev/shm/matrix_shards/attn_output_flat.bin
Saving to local storage...
Saving matrix to binary file: matrix_shards/attn_output_flat.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 4096), dtype=float32
  Converting to 4D format...
    2D (18, 4096) -> 4D (1, 1, 18, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 4096
    Wrote 73,728 float32 elements
  File saved successfully
  File size: 294,932 bytes
  Expected size: 294,932 bytes
  âœ“ File size verification passed
  Memory usage: 0.28 MB
  Save completed: matrix_shards/attn_output_flat.bin
Saving matrix to binary file: /dev/shm/matrix_shards/attn_output_flat.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 4096), dtype=float32
  Converting to 4D format...
    2D (18, 4096) -> 4D (1, 1, 18, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 4096
    Wrote 73,728 float32 elements
  File saved successfully
  File size: 294,932 bytes
  Expected size: 294,932 bytes
  âœ“ File size verification passed
  Memory usage: 0.28 MB
  Save completed: /dev/shm/matrix_shards/attn_output_flat.bin
Remote paths - RAM: /dev/shm/matrix_shards/attn_output_flat.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/attn_output_flat.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
ðŸ“¤ Sent file attn_output_flat.bin to 192.168.2.104
âœ… Received attn_output_flat.bin 1/1
âœ… All ACKs received!
Sending to 192.168.2.101
ðŸ“¤ Sent file attn_output_flat.bin to 192.168.2.101
âœ… Received attn_output_flat.bin 1/1
âœ… All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_30_mlp_gate_proj_weight
   Split Matrix: True
   Dimension: 0

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([5017, 4096])
  Node 1: shard shape torch.Size([5017, 4096])
  Node 2: shard shape torch.Size([2150, 4096])
  Node 3: shard shape torch.Size([2152, 4096])
Starting distribution of 4 shards to 3 unique nodes
Processing shard 0 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_30_mlp_gate_proj_weight_shard_0.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_30_mlp_gate_proj_weight_shard_0.bin
Saving matrix to binary file: matrix_shards/layers_30_mlp_gate_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: matrix_shards/layers_30_mlp_gate_proj_weight_shard_0.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_30_mlp_gate_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: /dev/shm/matrix_shards/layers_30_mlp_gate_proj_weight_shard_0.bin
  Added RAM path to file list
Processing shard 1 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_30_mlp_gate_proj_weight_shard_1.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_30_mlp_gate_proj_weight_shard_1.bin
Saving matrix to binary file: matrix_shards/layers_30_mlp_gate_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: matrix_shards/layers_30_mlp_gate_proj_weight_shard_1.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_30_mlp_gate_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: /dev/shm/matrix_shards/layers_30_mlp_gate_proj_weight_shard_1.bin
  Added RAM path to file list
Processing shard 2 for node 192.168.2.101
  Remote node 192.168.2.101: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_30_mlp_gate_proj_weight_shard_2.bin
Saving matrix to binary file: matrix_shards/layers_30_mlp_gate_proj_weight_shard_2.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([2150, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(2150, 4096), dtype=float32
  Converting to 4D format...
    2D (2150, 4096) -> 4D (1, 1, 2150, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 2150 Ã— 4096
    Wrote 8,806,400 float32 elements
  File saved successfully
  File size: 35,225,620 bytes
  Expected size: 35,225,620 bytes
  âœ“ File size verification passed
  Memory usage: 33.59 MB
  Save completed: matrix_shards/layers_30_mlp_gate_proj_weight_shard_2.bin
DEBUG::: layers_30_mlp_gate_proj_weight_shard_2.bin
  Step 2: Sending file to remote node 192.168.2.101
ðŸ“¤ Sent file layers_30_mlp_gate_proj_weight_shard_2.bin to 192.168.2.101
âœ… Received layers_30_mlp_gate_proj_weight_shard_2.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_30_mlp_gate_proj_weight_shard_2.bin
Processing shard 3 for node 192.168.2.104
  Remote node 192.168.2.104: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_30_mlp_gate_proj_weight_shard_3.bin
Saving matrix to binary file: matrix_shards/layers_30_mlp_gate_proj_weight_shard_3.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([2152, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(2152, 4096), dtype=float32
  Converting to 4D format...
    2D (2152, 4096) -> 4D (1, 1, 2152, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 2152 Ã— 4096
    Wrote 8,814,592 float32 elements
  File saved successfully
  File size: 35,258,388 bytes
  Expected size: 35,258,388 bytes
  âœ“ File size verification passed
  Memory usage: 33.63 MB
  Save completed: matrix_shards/layers_30_mlp_gate_proj_weight_shard_3.bin
DEBUG::: layers_30_mlp_gate_proj_weight_shard_3.bin
  Step 2: Sending file to remote node 192.168.2.104
ðŸ“¤ Sent file layers_30_mlp_gate_proj_weight_shard_3.bin to 192.168.2.104
âœ… Received layers_30_mlp_gate_proj_weight_shard_3.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_30_mlp_gate_proj_weight_shard_3.bin
Distribution complete: 4 shards saved and distributed

============================================================
ðŸš€ STARTING CLUSTER OPERATION
============================================================
Matrix A: attn_output_flat
Matrix B: layers_30_mlp_gate_proj_weight
Operation: mul
Transpose A: False, Transpose B: True
Send back result: True
Number of shards: 4

ðŸ“¤ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_mlp_gate_proj_weight_shard_0.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_mlp_gate_proj_weight_shard_1.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_mlp_gate_proj_weight_shard_2.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_mlp_gate_proj_weight_shard_3.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.104

â³ WAITING FOR ACKS FROM NODES (4)
âœ… Received ACK_matrixOp_complete 1/4
âœ… Received ACK_matrixOp_complete 2/4
âœ… Received ACK_matrixOp_complete 3/4
âœ… Received ACK_matrixOp_complete 4/4
âœ… All ACKs received!

============================================================
âœ… CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.13 seconds

ðŸ“Š Result base: layers_30_mlp_gate_proj_weightxattn_output_flat (send_back=True)
âœ… Loaded /dev/shm/matrix_shards/layers_30_mlp_gate_proj_weightxattn_output_flat_combined.bin
  Original dims: [1, 1, 18, 14336]
  Result tensor shape: torch.Size([18, 14336]), size: 1,032,192 bytes
  Data range: [-0.046688, 0.046188]
   Running MLP up projection...
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_30_mlp_up_proj_weight
   Split Matrix: True
   Dimension: 0

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([5017, 4096])
  Node 1: shard shape torch.Size([5017, 4096])
  Node 2: shard shape torch.Size([2150, 4096])
  Node 3: shard shape torch.Size([2152, 4096])
Starting distribution of 4 shards to 3 unique nodes
Processing shard 0 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_30_mlp_up_proj_weight_shard_0.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_30_mlp_up_proj_weight_shard_0.bin
Saving matrix to binary file: matrix_shards/layers_30_mlp_up_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: matrix_shards/layers_30_mlp_up_proj_weight_shard_0.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_30_mlp_up_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: /dev/shm/matrix_shards/layers_30_mlp_up_proj_weight_shard_0.bin
  Added RAM path to file list
Processing shard 1 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_30_mlp_up_proj_weight_shard_1.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_30_mlp_up_proj_weight_shard_1.bin
Saving matrix to binary file: matrix_shards/layers_30_mlp_up_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: matrix_shards/layers_30_mlp_up_proj_weight_shard_1.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_30_mlp_up_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: /dev/shm/matrix_shards/layers_30_mlp_up_proj_weight_shard_1.bin
  Added RAM path to file list
Processing shard 2 for node 192.168.2.101
  Remote node 192.168.2.101: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_30_mlp_up_proj_weight_shard_2.bin
Saving matrix to binary file: matrix_shards/layers_30_mlp_up_proj_weight_shard_2.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([2150, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(2150, 4096), dtype=float32
  Converting to 4D format...
    2D (2150, 4096) -> 4D (1, 1, 2150, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 2150 Ã— 4096
    Wrote 8,806,400 float32 elements
  File saved successfully
  File size: 35,225,620 bytes
  Expected size: 35,225,620 bytes
  âœ“ File size verification passed
  Memory usage: 33.59 MB
  Save completed: matrix_shards/layers_30_mlp_up_proj_weight_shard_2.bin
DEBUG::: layers_30_mlp_up_proj_weight_shard_2.bin
  Step 2: Sending file to remote node 192.168.2.101
ðŸ“¤ Sent file layers_30_mlp_up_proj_weight_shard_2.bin to 192.168.2.101
âœ… Received layers_30_mlp_up_proj_weight_shard_2.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_30_mlp_up_proj_weight_shard_2.bin
Processing shard 3 for node 192.168.2.104
  Remote node 192.168.2.104: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_30_mlp_up_proj_weight_shard_3.bin
Saving matrix to binary file: matrix_shards/layers_30_mlp_up_proj_weight_shard_3.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([2152, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(2152, 4096), dtype=float32
  Converting to 4D format...
    2D (2152, 4096) -> 4D (1, 1, 2152, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 2152 Ã— 4096
    Wrote 8,814,592 float32 elements
  File saved successfully
  File size: 35,258,388 bytes
  Expected size: 35,258,388 bytes
  âœ“ File size verification passed
  Memory usage: 33.63 MB
  Save completed: matrix_shards/layers_30_mlp_up_proj_weight_shard_3.bin
DEBUG::: layers_30_mlp_up_proj_weight_shard_3.bin
  Step 2: Sending file to remote node 192.168.2.104
ðŸ“¤ Sent file layers_30_mlp_up_proj_weight_shard_3.bin to 192.168.2.104
âœ… Received layers_30_mlp_up_proj_weight_shard_3.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_30_mlp_up_proj_weight_shard_3.bin
Distribution complete: 4 shards saved and distributed

============================================================
ðŸš€ STARTING CLUSTER OPERATION
============================================================
Matrix A: attn_output_flat
Matrix B: layers_30_mlp_up_proj_weight
Operation: mul
Transpose A: False, Transpose B: True
Send back result: True
Number of shards: 4

ðŸ“¤ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_mlp_up_proj_weight_shard_0.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_mlp_up_proj_weight_shard_1.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_mlp_up_proj_weight_shard_2.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_mlp_up_proj_weight_shard_3.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.104

â³ WAITING FOR ACKS FROM NODES (4)
âœ… Received ACK_matrixOp_complete 1/4
âœ… Received ACK_matrixOp_complete 2/4
âœ… Received ACK_matrixOp_complete 3/4
âœ… Received ACK_matrixOp_complete 4/4
âœ… All ACKs received!

============================================================
âœ… CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.14 seconds

ðŸ“Š Result base: layers_30_mlp_up_proj_weightxattn_output_flat (send_back=True)
âœ… Loaded /dev/shm/matrix_shards/layers_30_mlp_up_proj_weightxattn_output_flat_combined.bin
  Original dims: [1, 1, 18, 14336]
  Result tensor shape: torch.Size([18, 14336]), size: 1,032,192 bytes
  Data range: [-0.032775, 0.024007]
   Up projection result shape: torch.Size([18, 14336])
   Applying SiLU activation to gate...
   Element-wise multiplication (gate_silu * up)...
   MLP intermediate shape: torch.Size([18, 14336])
   Running MLP down projection...
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
Preparing full matrix: mlp_intermediate_cluster.bin
Local paths - DISK: matrix_shards/mlp_intermediate_cluster.bin, RAM: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin
Saving to local storage...
Saving matrix to binary file: matrix_shards/mlp_intermediate_cluster.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 14336), dtype=float32
  Converting to 4D format...
    2D (18, 14336) -> 4D (1, 1, 18, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 14336
    Wrote 258,048 float32 elements
  File saved successfully
  File size: 1,032,212 bytes
  Expected size: 1,032,212 bytes
  âœ“ File size verification passed
  Memory usage: 0.98 MB
  Save completed: matrix_shards/mlp_intermediate_cluster.bin
Saving matrix to binary file: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 14336), dtype=float32
  Converting to 4D format...
    2D (18, 14336) -> 4D (1, 1, 18, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 14336
    Wrote 258,048 float32 elements
  File saved successfully
  File size: 1,032,212 bytes
  Expected size: 1,032,212 bytes
  âœ“ File size verification passed
  Memory usage: 0.98 MB
  Save completed: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin
Remote paths - RAM: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/mlp_intermediate_cluster.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
ðŸ“¤ Sent file mlp_intermediate_cluster.bin to 192.168.2.104
âœ… Received mlp_intermediate_cluster.bin 1/1
âœ… All ACKs received!
Sending to 192.168.2.101
ðŸ“¤ Sent file mlp_intermediate_cluster.bin to 192.168.2.101
âœ… Received mlp_intermediate_cluster.bin 1/1
âœ… All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_30_mlp_down_proj_weight
   Split Matrix: True
   Dimension: 0

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([1433, 14336])
  Node 1: shard shape torch.Size([1433, 14336])
  Node 2: shard shape torch.Size([614, 14336])
  Node 3: shard shape torch.Size([616, 14336])
Starting distribution of 4 shards to 3 unique nodes
Processing shard 0 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_30_mlp_down_proj_weight_shard_0.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_30_mlp_down_proj_weight_shard_0.bin
Saving matrix to binary file: matrix_shards/layers_30_mlp_down_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([1433, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(1433, 14336), dtype=float32
  Converting to 4D format...
    2D (1433, 14336) -> 4D (1, 1, 1433, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 1433 Ã— 14336
    Wrote 20,543,488 float32 elements
  File saved successfully
  File size: 82,173,972 bytes
  Expected size: 82,173,972 bytes
  âœ“ File size verification passed
  Memory usage: 78.37 MB
  Save completed: matrix_shards/layers_30_mlp_down_proj_weight_shard_0.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_30_mlp_down_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([1433, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(1433, 14336), dtype=float32
  Converting to 4D format...
    2D (1433, 14336) -> 4D (1, 1, 1433, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 1433 Ã— 14336
    Wrote 20,543,488 float32 elements
  File saved successfully
  File size: 82,173,972 bytes
  Expected size: 82,173,972 bytes
  âœ“ File size verification passed
  Memory usage: 78.37 MB
  Save completed: /dev/shm/matrix_shards/layers_30_mlp_down_proj_weight_shard_0.bin
  Added RAM path to file list
Processing shard 1 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_30_mlp_down_proj_weight_shard_1.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_30_mlp_down_proj_weight_shard_1.bin
Saving matrix to binary file: matrix_shards/layers_30_mlp_down_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([1433, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(1433, 14336), dtype=float32
  Converting to 4D format...
    2D (1433, 14336) -> 4D (1, 1, 1433, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 1433 Ã— 14336
    Wrote 20,543,488 float32 elements
  File saved successfully
  File size: 82,173,972 bytes
  Expected size: 82,173,972 bytes
  âœ“ File size verification passed
  Memory usage: 78.37 MB
  Save completed: matrix_shards/layers_30_mlp_down_proj_weight_shard_1.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_30_mlp_down_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([1433, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(1433, 14336), dtype=float32
  Converting to 4D format...
    2D (1433, 14336) -> 4D (1, 1, 1433, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 1433 Ã— 14336
    Wrote 20,543,488 float32 elements
  File saved successfully
  File size: 82,173,972 bytes
  Expected size: 82,173,972 bytes
  âœ“ File size verification passed
  Memory usage: 78.37 MB
  Save completed: /dev/shm/matrix_shards/layers_30_mlp_down_proj_weight_shard_1.bin
  Added RAM path to file list
Processing shard 2 for node 192.168.2.101
  Remote node 192.168.2.101: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_30_mlp_down_proj_weight_shard_2.bin
Saving matrix to binary file: matrix_shards/layers_30_mlp_down_proj_weight_shard_2.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([614, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(614, 14336), dtype=float32
  Converting to 4D format...
    2D (614, 14336) -> 4D (1, 1, 614, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 614 Ã— 14336
    Wrote 8,802,304 float32 elements
  File saved successfully
  File size: 35,209,236 bytes
  Expected size: 35,209,236 bytes
  âœ“ File size verification passed
  Memory usage: 33.58 MB
  Save completed: matrix_shards/layers_30_mlp_down_proj_weight_shard_2.bin
DEBUG::: layers_30_mlp_down_proj_weight_shard_2.bin
  Step 2: Sending file to remote node 192.168.2.101
ðŸ“¤ Sent file layers_30_mlp_down_proj_weight_shard_2.bin to 192.168.2.101
âœ… Received layers_30_mlp_down_proj_weight_shard_2.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_30_mlp_down_proj_weight_shard_2.bin
Processing shard 3 for node 192.168.2.104
  Remote node 192.168.2.104: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_30_mlp_down_proj_weight_shard_3.bin
Saving matrix to binary file: matrix_shards/layers_30_mlp_down_proj_weight_shard_3.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([616, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(616, 14336), dtype=float32
  Converting to 4D format...
    2D (616, 14336) -> 4D (1, 1, 616, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 616 Ã— 14336
    Wrote 8,830,976 float32 elements
  File saved successfully
  File size: 35,323,924 bytes
  Expected size: 35,323,924 bytes
  âœ“ File size verification passed
  Memory usage: 33.69 MB
  Save completed: matrix_shards/layers_30_mlp_down_proj_weight_shard_3.bin
DEBUG::: layers_30_mlp_down_proj_weight_shard_3.bin
  Step 2: Sending file to remote node 192.168.2.104
ðŸ“¤ Sent file layers_30_mlp_down_proj_weight_shard_3.bin to 192.168.2.104
âœ… Received layers_30_mlp_down_proj_weight_shard_3.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_30_mlp_down_proj_weight_shard_3.bin
Distribution complete: 4 shards saved and distributed

============================================================
ðŸš€ STARTING CLUSTER OPERATION
============================================================
Matrix A: mlp_intermediate_cluster
Matrix B: layers_30_mlp_down_proj_weight
Operation: mul
Transpose A: False, Transpose B: True
Send back result: True
Number of shards: 4

ðŸ“¤ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_mlp_down_proj_weight_shard_0.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_mlp_down_proj_weight_shard_1.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_mlp_down_proj_weight_shard_2.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin
  Matrix B path: /dev/shm/matrix_shards/layers_30_mlp_down_proj_weight_shard_3.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.104

â³ WAITING FOR ACKS FROM NODES (4)
âœ… Received ACK_matrixOp_complete 1/4
âœ… Received ACK_matrixOp_complete 2/4
âœ… Received ACK_matrixOp_complete 3/4
âœ… Received ACK_matrixOp_complete 4/4
âœ… All ACKs received!

============================================================
âœ… CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.13 seconds

ðŸ“Š Result base: layers_30_mlp_down_proj_weightxmlp_intermediate_cluster (send_back=True)
âœ… Loaded /dev/shm/matrix_shards/layers_30_mlp_down_proj_weightxmlp_intermediate_cluster_combined.bin
  Original dims: [1, 1, 18, 4096]
  Result tensor shape: torch.Size([18, 4096]), size: 294,912 bytes
  Data range: [-0.000105, 0.000118]
   Adding residual connection (MLP + attention)...
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([18, 1433])
  Node 1: shard shape torch.Size([18, 1433])
  Node 2: shard shape torch.Size([18, 614])
  Node 3: shard shape torch.Size([18, 616])
Starting distribution of 4 shards to 3 unique nodes
Processing shard 0 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/mlp_output_cluster_shard_0.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/mlp_output_cluster_shard_0.bin
Saving matrix to binary file: matrix_shards/mlp_output_cluster_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: matrix_shards/mlp_output_cluster_shard_0.bin
Saving matrix to binary file: /dev/shm/matrix_shards/mlp_output_cluster_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: /dev/shm/matrix_shards/mlp_output_cluster_shard_0.bin
  Added RAM path to file list
Processing shard 1 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/mlp_output_cluster_shard_1.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/mlp_output_cluster_shard_1.bin
Saving matrix to binary file: matrix_shards/mlp_output_cluster_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: matrix_shards/mlp_output_cluster_shard_1.bin
Saving matrix to binary file: /dev/shm/matrix_shards/mlp_output_cluster_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: /dev/shm/matrix_shards/mlp_output_cluster_shard_1.bin
  Added RAM path to file list
Processing shard 2 for node 192.168.2.101
  Remote node 192.168.2.101: Beginning distribution
  Step 1: Saving locally to matrix_shards/mlp_output_cluster_shard_2.bin
Saving matrix to binary file: matrix_shards/mlp_output_cluster_shard_2.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 614]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 614), dtype=float32
  Converting to 4D format...
    2D (18, 614) -> 4D (1, 1, 18, 614)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 614
    Wrote 11,052 float32 elements
  File saved successfully
  File size: 44,228 bytes
  Expected size: 44,228 bytes
  âœ“ File size verification passed
  Memory usage: 0.04 MB
  Save completed: matrix_shards/mlp_output_cluster_shard_2.bin
DEBUG::: mlp_output_cluster_shard_2.bin
  Step 2: Sending file to remote node 192.168.2.101
ðŸ“¤ Sent file mlp_output_cluster_shard_2.bin to 192.168.2.101
âœ… Received mlp_output_cluster_shard_2.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/mlp_output_cluster_shard_2.bin
Processing shard 3 for node 192.168.2.104
  Remote node 192.168.2.104: Beginning distribution
  Step 1: Saving locally to matrix_shards/mlp_output_cluster_shard_3.bin
Saving matrix to binary file: matrix_shards/mlp_output_cluster_shard_3.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 616]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 616), dtype=float32
  Converting to 4D format...
    2D (18, 616) -> 4D (1, 1, 18, 616)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 616
    Wrote 11,088 float32 elements
  File saved successfully
  File size: 44,372 bytes
  Expected size: 44,372 bytes
  âœ“ File size verification passed
  Memory usage: 0.04 MB
  Save completed: matrix_shards/mlp_output_cluster_shard_3.bin
DEBUG::: mlp_output_cluster_shard_3.bin
  Step 2: Sending file to remote node 192.168.2.104
ðŸ“¤ Sent file mlp_output_cluster_shard_3.bin to 192.168.2.104
âœ… Received mlp_output_cluster_shard_3.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/mlp_output_cluster_shard_3.bin
Distribution complete: 4 shards saved and distributed
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([18, 1433])
  Node 1: shard shape torch.Size([18, 1433])
  Node 2: shard shape torch.Size([18, 614])
  Node 3: shard shape torch.Size([18, 616])
Starting distribution of 4 shards to 3 unique nodes
Processing shard 0 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/attn_outpuattn_output_clustert_flat_shard_0.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_0.bin
Saving matrix to binary file: matrix_shards/attn_outpuattn_output_clustert_flat_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: matrix_shards/attn_outpuattn_output_clustert_flat_shard_0.bin
Saving matrix to binary file: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_0.bin
  Added RAM path to file list
Processing shard 1 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/attn_outpuattn_output_clustert_flat_shard_1.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_1.bin
Saving matrix to binary file: matrix_shards/attn_outpuattn_output_clustert_flat_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: matrix_shards/attn_outpuattn_output_clustert_flat_shard_1.bin
Saving matrix to binary file: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_1.bin
  Added RAM path to file list
Processing shard 2 for node 192.168.2.101
  Remote node 192.168.2.101: Beginning distribution
  Step 1: Saving locally to matrix_shards/attn_outpuattn_output_clustert_flat_shard_2.bin
Saving matrix to binary file: matrix_shards/attn_outpuattn_output_clustert_flat_shard_2.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 614]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 614), dtype=float32
  Converting to 4D format...
    2D (18, 614) -> 4D (1, 1, 18, 614)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 614
    Wrote 11,052 float32 elements
  File saved successfully
  File size: 44,228 bytes
  Expected size: 44,228 bytes
  âœ“ File size verification passed
  Memory usage: 0.04 MB
  Save completed: matrix_shards/attn_outpuattn_output_clustert_flat_shard_2.bin
DEBUG::: attn_outpuattn_output_clustert_flat_shard_2.bin
  Step 2: Sending file to remote node 192.168.2.101
ðŸ“¤ Sent file attn_outpuattn_output_clustert_flat_shard_2.bin to 192.168.2.101
âœ… Received attn_outpuattn_output_clustert_flat_shard_2.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_2.bin
Processing shard 3 for node 192.168.2.104
  Remote node 192.168.2.104: Beginning distribution
  Step 1: Saving locally to matrix_shards/attn_outpuattn_output_clustert_flat_shard_3.bin
Saving matrix to binary file: matrix_shards/attn_outpuattn_output_clustert_flat_shard_3.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 616]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 616), dtype=float32
  Converting to 4D format...
    2D (18, 616) -> 4D (1, 1, 18, 616)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 616
    Wrote 11,088 float32 elements
  File saved successfully
  File size: 44,372 bytes
  Expected size: 44,372 bytes
  âœ“ File size verification passed
  Memory usage: 0.04 MB
  Save completed: matrix_shards/attn_outpuattn_output_clustert_flat_shard_3.bin
DEBUG::: attn_outpuattn_output_clustert_flat_shard_3.bin
  Step 2: Sending file to remote node 192.168.2.104
ðŸ“¤ Sent file attn_outpuattn_output_clustert_flat_shard_3.bin to 192.168.2.104
âœ… Received attn_outpuattn_output_clustert_flat_shard_3.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_3.bin
Distribution complete: 4 shards saved and distributed

============================================================
ðŸš€ STARTING CLUSTER OPERATION
============================================================
Matrix A: mlp_output_cluster
Matrix B: attn_outpuattn_output_clustert_flat
Operation: add
Transpose A: False, Transpose B: True
Send back result: True
Number of shards: 4

ðŸ“¤ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/mlp_output_cluster_shard_0.bin
  Matrix B path: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_0.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/mlp_output_cluster_shard_1.bin
  Matrix B path: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_1.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/mlp_output_cluster_shard_2.bin
  Matrix B path: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_2.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: False (GPU #0)
  Next GPU for this node will be: #0
  Matrix A path: /dev/shm/matrix_shards/mlp_output_cluster_shard_3.bin
  Matrix B path: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_3.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.104

â³ WAITING FOR ACKS FROM NODES (4)
âœ… Received ACK_matrixOp_complete 1/4
âœ… Received ACK_matrixOp_complete 2/4
âœ… Received ACK_matrixOp_complete 3/4
âœ… Received ACK_matrixOp_complete 4/4
âœ… All ACKs received!

============================================================
âœ… CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.02 seconds

ðŸ“Š Result base: attn_outpuattn_output_clustert_flatxmlp_output_cluster (send_back=True)
âœ… Received ACK_combined_matrix_saved 1/1
âœ… All ACKs received!
âœ… Loaded /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flatxmlp_output_cluster_combined.bin
  Original dims: [1, 1, 18, 4096]
  Result tensor shape: torch.Size([18, 4096]), size: 294,912 bytes
  Data range: [-0.024983, 0.027751]
   Applying post-attention layer normalization...
   Loaded post-attention layer norm weights: shape torch.Size([4096])
   Applied layer normalization
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([18, 1433])
  Node 1: shard shape torch.Size([18, 1433])
  Node 2: shard shape torch.Size([18, 614])
  Node 3: shard shape torch.Size([18, 616])
   Loading attention weights for layer 31...
   Running Q/K/V projections with CLUSTER...
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_31_self_attn_q_proj_weight
   Split Matrix: True
   Dimension: 1

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([4096, 1433])
  Node 1: shard shape torch.Size([4096, 1433])
  Node 2: shard shape torch.Size([4096, 614])
  Node 3: shard shape torch.Size([4096, 616])
Starting distribution of 4 shards to 3 unique nodes
Processing shard 0 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_31_self_attn_q_proj_weight_shard_0.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_31_self_attn_q_proj_weight_shard_0.bin
Saving matrix to binary file: matrix_shards/layers_31_self_attn_q_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([4096, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(4096, 1433), dtype=float32
  Converting to 4D format...
    2D (4096, 1433) -> 4D (1, 1, 4096, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 4096 Ã— 1433
    Wrote 5,869,568 float32 elements
  File saved successfully
  File size: 23,478,292 bytes
  Expected size: 23,478,292 bytes
  âœ“ File size verification passed
  Memory usage: 22.39 MB
  Save completed: matrix_shards/layers_31_self_attn_q_proj_weight_shard_0.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_31_self_attn_q_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([4096, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(4096, 1433), dtype=float32
  Converting to 4D format...
    2D (4096, 1433) -> 4D (1, 1, 4096, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 4096 Ã— 1433
    Wrote 5,869,568 float32 elements
  File saved successfully
  File size: 23,478,292 bytes
  Expected size: 23,478,292 bytes
  âœ“ File size verification passed
  Memory usage: 22.39 MB
  Save completed: /dev/shm/matrix_shards/layers_31_self_attn_q_proj_weight_shard_0.bin
  Added RAM path to file list
Processing shard 1 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_31_self_attn_q_proj_weight_shard_1.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_31_self_attn_q_proj_weight_shard_1.bin
Saving matrix to binary file: matrix_shards/layers_31_self_attn_q_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([4096, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(4096, 1433), dtype=float32
  Converting to 4D format...
    2D (4096, 1433) -> 4D (1, 1, 4096, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 4096 Ã— 1433
    Wrote 5,869,568 float32 elements
  File saved successfully
  File size: 23,478,292 bytes
  Expected size: 23,478,292 bytes
  âœ“ File size verification passed
  Memory usage: 22.39 MB
  Save completed: matrix_shards/layers_31_self_attn_q_proj_weight_shard_1.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_31_self_attn_q_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([4096, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(4096, 1433), dtype=float32
  Converting to 4D format...
    2D (4096, 1433) -> 4D (1, 1, 4096, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 4096 Ã— 1433
    Wrote 5,869,568 float32 elements
  File saved successfully
  File size: 23,478,292 bytes
  Expected size: 23,478,292 bytes
  âœ“ File size verification passed
  Memory usage: 22.39 MB
  Save completed: /dev/shm/matrix_shards/layers_31_self_attn_q_proj_weight_shard_1.bin
  Added RAM path to file list
Processing shard 2 for node 192.168.2.101
  Remote node 192.168.2.101: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_31_self_attn_q_proj_weight_shard_2.bin
Saving matrix to binary file: matrix_shards/layers_31_self_attn_q_proj_weight_shard_2.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([4096, 614]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(4096, 614), dtype=float32
  Converting to 4D format...
    2D (4096, 614) -> 4D (1, 1, 4096, 614)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 4096 Ã— 614
    Wrote 2,514,944 float32 elements
  File saved successfully
  File size: 10,059,796 bytes
  Expected size: 10,059,796 bytes
  âœ“ File size verification passed
  Memory usage: 9.59 MB
  Save completed: matrix_shards/layers_31_self_attn_q_proj_weight_shard_2.bin
DEBUG::: layers_31_self_attn_q_proj_weight_shard_2.bin
  Step 2: Sending file to remote node 192.168.2.101
ðŸ“¤ Sent file layers_31_self_attn_q_proj_weight_shard_2.bin to 192.168.2.101
âœ… Received layers_31_self_attn_q_proj_weight_shard_2.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_31_self_attn_q_proj_weight_shard_2.bin
Processing shard 3 for node 192.168.2.104
  Remote node 192.168.2.104: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_31_self_attn_q_proj_weight_shard_3.bin
Saving matrix to binary file: matrix_shards/layers_31_self_attn_q_proj_weight_shard_3.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([4096, 616]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(4096, 616), dtype=float32
  Converting to 4D format...
    2D (4096, 616) -> 4D (1, 1, 4096, 616)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 4096 Ã— 616
    Wrote 2,523,136 float32 elements
  File saved successfully
  File size: 10,092,564 bytes
  Expected size: 10,092,564 bytes
  âœ“ File size verification passed
  Memory usage: 9.63 MB
  Save completed: matrix_shards/layers_31_self_attn_q_proj_weight_shard_3.bin
DEBUG::: layers_31_self_attn_q_proj_weight_shard_3.bin
  Step 2: Sending file to remote node 192.168.2.104
ðŸ“¤ Sent file layers_31_self_attn_q_proj_weight_shard_3.bin to 192.168.2.104
âœ… Received layers_31_self_attn_q_proj_weight_shard_3.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_31_self_attn_q_proj_weight_shard_3.bin
Distribution complete: 4 shards saved and distributed

============================================================
ðŸš€ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embedding_matrix
Matrix B: layers_31_self_attn_q_proj_weight
Operation: mul
Transpose A: False, Transpose B: False
Send back result: True
Number of shards: 4

ðŸ“¤ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_self_attn_q_proj_weight_shard_0.bin
  Final transpose flags - A: false, B: true
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_self_attn_q_proj_weight_shard_1.bin
  Final transpose flags - A: false, B: true
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_self_attn_q_proj_weight_shard_2.bin
  Final transpose flags - A: false, B: true
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_self_attn_q_proj_weight_shard_3.bin
  Final transpose flags - A: false, B: true
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.104

â³ WAITING FOR ACKS FROM NODES (4)
âœ… Received ACK_matrixOp_complete 1/4
âœ… Received ACK_matrixOp_complete 2/4
âœ… Received ACK_matrixOp_complete 3/4
âœ… Received ACK_matrixOp_complete 4/4
âœ… All ACKs received!

============================================================
âœ… CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.18 seconds

ðŸ“Š Result base: layers_31_self_attn_q_proj_weightxinput_token_embedding_matrix (send_back=True)
âœ… Received ACK_combined_matrix_saved 1/1
âœ… All ACKs received!
âœ… Loaded /dev/shm/matrix_shards/layers_31_self_attn_q_proj_weightxinput_token_embedding_matrix_combined.bin
  Original dims: [1, 1, 18, 4096]
  Result tensor shape: torch.Size([18, 4096]), size: 294,912 bytes
  Data range: [-0.081512, 0.079557]
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_31_self_attn_k_proj_weight
   Split Matrix: True
   Dimension: 0

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([358, 4096])
  Node 1: shard shape torch.Size([358, 4096])
  Node 2: shard shape torch.Size([153, 4096])
  Node 3: shard shape torch.Size([155, 4096])
Starting distribution of 4 shards to 3 unique nodes
Processing shard 0 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_31_self_attn_k_proj_weight_shard_0.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_31_self_attn_k_proj_weight_shard_0.bin
Saving matrix to binary file: matrix_shards/layers_31_self_attn_k_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([358, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(358, 4096), dtype=float32
  Converting to 4D format...
    2D (358, 4096) -> 4D (1, 1, 358, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 358 Ã— 4096
    Wrote 1,466,368 float32 elements
  File saved successfully
  File size: 5,865,492 bytes
  Expected size: 5,865,492 bytes
  âœ“ File size verification passed
  Memory usage: 5.59 MB
  Save completed: matrix_shards/layers_31_self_attn_k_proj_weight_shard_0.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_31_self_attn_k_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([358, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(358, 4096), dtype=float32
  Converting to 4D format...
    2D (358, 4096) -> 4D (1, 1, 358, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 358 Ã— 4096
    Wrote 1,466,368 float32 elements
  File saved successfully
  File size: 5,865,492 bytes
  Expected size: 5,865,492 bytes
  âœ“ File size verification passed
  Memory usage: 5.59 MB
  Save completed: /dev/shm/matrix_shards/layers_31_self_attn_k_proj_weight_shard_0.bin
  Added RAM path to file list
Processing shard 1 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_31_self_attn_k_proj_weight_shard_1.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_31_self_attn_k_proj_weight_shard_1.bin
Saving matrix to binary file: matrix_shards/layers_31_self_attn_k_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([358, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(358, 4096), dtype=float32
  Converting to 4D format...
    2D (358, 4096) -> 4D (1, 1, 358, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 358 Ã— 4096
    Wrote 1,466,368 float32 elements
  File saved successfully
  File size: 5,865,492 bytes
  Expected size: 5,865,492 bytes
  âœ“ File size verification passed
  Memory usage: 5.59 MB
  Save completed: matrix_shards/layers_31_self_attn_k_proj_weight_shard_1.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_31_self_attn_k_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([358, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(358, 4096), dtype=float32
  Converting to 4D format...
    2D (358, 4096) -> 4D (1, 1, 358, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 358 Ã— 4096
    Wrote 1,466,368 float32 elements
  File saved successfully
  File size: 5,865,492 bytes
  Expected size: 5,865,492 bytes
  âœ“ File size verification passed
  Memory usage: 5.59 MB
  Save completed: /dev/shm/matrix_shards/layers_31_self_attn_k_proj_weight_shard_1.bin
  Added RAM path to file list
Processing shard 2 for node 192.168.2.101
  Remote node 192.168.2.101: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_31_self_attn_k_proj_weight_shard_2.bin
Saving matrix to binary file: matrix_shards/layers_31_self_attn_k_proj_weight_shard_2.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([153, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(153, 4096), dtype=float32
  Converting to 4D format...
    2D (153, 4096) -> 4D (1, 1, 153, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 153 Ã— 4096
    Wrote 626,688 float32 elements
  File saved successfully
  File size: 2,506,772 bytes
  Expected size: 2,506,772 bytes
  âœ“ File size verification passed
  Memory usage: 2.39 MB
  Save completed: matrix_shards/layers_31_self_attn_k_proj_weight_shard_2.bin
DEBUG::: layers_31_self_attn_k_proj_weight_shard_2.bin
  Step 2: Sending file to remote node 192.168.2.101
ðŸ“¤ Sent file layers_31_self_attn_k_proj_weight_shard_2.bin to 192.168.2.101
âœ… Received layers_31_self_attn_k_proj_weight_shard_2.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_31_self_attn_k_proj_weight_shard_2.bin
Processing shard 3 for node 192.168.2.104
  Remote node 192.168.2.104: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_31_self_attn_k_proj_weight_shard_3.bin
Saving matrix to binary file: matrix_shards/layers_31_self_attn_k_proj_weight_shard_3.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([155, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(155, 4096), dtype=float32
  Converting to 4D format...
    2D (155, 4096) -> 4D (1, 1, 155, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 155 Ã— 4096
    Wrote 634,880 float32 elements
  File saved successfully
  File size: 2,539,540 bytes
  Expected size: 2,539,540 bytes
  âœ“ File size verification passed
  Memory usage: 2.42 MB
  Save completed: matrix_shards/layers_31_self_attn_k_proj_weight_shard_3.bin
DEBUG::: layers_31_self_attn_k_proj_weight_shard_3.bin
  Step 2: Sending file to remote node 192.168.2.104
ðŸ“¤ Sent file layers_31_self_attn_k_proj_weight_shard_3.bin to 192.168.2.104
âœ… Received layers_31_self_attn_k_proj_weight_shard_3.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_31_self_attn_k_proj_weight_shard_3.bin
Distribution complete: 4 shards saved and distributed

============================================================
ðŸš€ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embedding_matrix
Matrix B: layers_31_self_attn_k_proj_weight
Operation: mul
Transpose A: False, Transpose B: True
Send back result: True
Number of shards: 4

ðŸ“¤ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_self_attn_k_proj_weight_shard_0.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_self_attn_k_proj_weight_shard_1.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_self_attn_k_proj_weight_shard_2.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_self_attn_k_proj_weight_shard_3.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.104

â³ WAITING FOR ACKS FROM NODES (4)
âœ… Received ACK_matrixOp_complete 1/4
âœ… Received ACK_matrixOp_complete 2/4
âœ… Received ACK_matrixOp_complete 3/4
âœ… Received ACK_matrixOp_complete 4/4
âœ… All ACKs received!

============================================================
âœ… CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.03 seconds

ðŸ“Š Result base: layers_31_self_attn_k_proj_weightxinput_token_embedding_matrix (send_back=True)
âœ… Loaded /dev/shm/matrix_shards/layers_31_self_attn_k_proj_weightxinput_token_embedding_matrix_combined.bin
  Original dims: [1, 1, 18, 1024]
  Result tensor shape: torch.Size([18, 1024]), size: 73,728 bytes
  Data range: [-0.060979, 0.054304]
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_31_self_attn_v_proj_weight
   Split Matrix: True
   Dimension: 0

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([358, 4096])
  Node 1: shard shape torch.Size([358, 4096])
  Node 2: shard shape torch.Size([153, 4096])
  Node 3: shard shape torch.Size([155, 4096])
Starting distribution of 4 shards to 3 unique nodes
Processing shard 0 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_31_self_attn_v_proj_weight_shard_0.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_31_self_attn_v_proj_weight_shard_0.bin
Saving matrix to binary file: matrix_shards/layers_31_self_attn_v_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([358, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(358, 4096), dtype=float32
  Converting to 4D format...
    2D (358, 4096) -> 4D (1, 1, 358, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 358 Ã— 4096
    Wrote 1,466,368 float32 elements
  File saved successfully
  File size: 5,865,492 bytes
  Expected size: 5,865,492 bytes
  âœ“ File size verification passed
  Memory usage: 5.59 MB
  Save completed: matrix_shards/layers_31_self_attn_v_proj_weight_shard_0.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_31_self_attn_v_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([358, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(358, 4096), dtype=float32
  Converting to 4D format...
    2D (358, 4096) -> 4D (1, 1, 358, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 358 Ã— 4096
    Wrote 1,466,368 float32 elements
  File saved successfully
  File size: 5,865,492 bytes
  Expected size: 5,865,492 bytes
  âœ“ File size verification passed
  Memory usage: 5.59 MB
  Save completed: /dev/shm/matrix_shards/layers_31_self_attn_v_proj_weight_shard_0.bin
  Added RAM path to file list
Processing shard 1 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_31_self_attn_v_proj_weight_shard_1.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_31_self_attn_v_proj_weight_shard_1.bin
Saving matrix to binary file: matrix_shards/layers_31_self_attn_v_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([358, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(358, 4096), dtype=float32
  Converting to 4D format...
    2D (358, 4096) -> 4D (1, 1, 358, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 358 Ã— 4096
    Wrote 1,466,368 float32 elements
  File saved successfully
  File size: 5,865,492 bytes
  Expected size: 5,865,492 bytes
  âœ“ File size verification passed
  Memory usage: 5.59 MB
  Save completed: matrix_shards/layers_31_self_attn_v_proj_weight_shard_1.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_31_self_attn_v_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([358, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(358, 4096), dtype=float32
  Converting to 4D format...
    2D (358, 4096) -> 4D (1, 1, 358, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 358 Ã— 4096
    Wrote 1,466,368 float32 elements
  File saved successfully
  File size: 5,865,492 bytes
  Expected size: 5,865,492 bytes
  âœ“ File size verification passed
  Memory usage: 5.59 MB
  Save completed: /dev/shm/matrix_shards/layers_31_self_attn_v_proj_weight_shard_1.bin
  Added RAM path to file list
Processing shard 2 for node 192.168.2.101
  Remote node 192.168.2.101: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_31_self_attn_v_proj_weight_shard_2.bin
Saving matrix to binary file: matrix_shards/layers_31_self_attn_v_proj_weight_shard_2.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([153, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(153, 4096), dtype=float32
  Converting to 4D format...
    2D (153, 4096) -> 4D (1, 1, 153, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 153 Ã— 4096
    Wrote 626,688 float32 elements
  File saved successfully
  File size: 2,506,772 bytes
  Expected size: 2,506,772 bytes
  âœ“ File size verification passed
  Memory usage: 2.39 MB
  Save completed: matrix_shards/layers_31_self_attn_v_proj_weight_shard_2.bin
DEBUG::: layers_31_self_attn_v_proj_weight_shard_2.bin
  Step 2: Sending file to remote node 192.168.2.101
ðŸ“¤ Sent file layers_31_self_attn_v_proj_weight_shard_2.bin to 192.168.2.101
âœ… Received layers_31_self_attn_v_proj_weight_shard_2.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_31_self_attn_v_proj_weight_shard_2.bin
Processing shard 3 for node 192.168.2.104
  Remote node 192.168.2.104: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_31_self_attn_v_proj_weight_shard_3.bin
Saving matrix to binary file: matrix_shards/layers_31_self_attn_v_proj_weight_shard_3.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([155, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(155, 4096), dtype=float32
  Converting to 4D format...
    2D (155, 4096) -> 4D (1, 1, 155, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 155 Ã— 4096
    Wrote 634,880 float32 elements
  File saved successfully
  File size: 2,539,540 bytes
  Expected size: 2,539,540 bytes
  âœ“ File size verification passed
  Memory usage: 2.42 MB
  Save completed: matrix_shards/layers_31_self_attn_v_proj_weight_shard_3.bin
DEBUG::: layers_31_self_attn_v_proj_weight_shard_3.bin
  Step 2: Sending file to remote node 192.168.2.104
ðŸ“¤ Sent file layers_31_self_attn_v_proj_weight_shard_3.bin to 192.168.2.104
âœ… Received layers_31_self_attn_v_proj_weight_shard_3.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_31_self_attn_v_proj_weight_shard_3.bin
Distribution complete: 4 shards saved and distributed

============================================================
ðŸš€ STARTING CLUSTER OPERATION
============================================================
Matrix A: input_token_embedding_matrix
Matrix B: layers_31_self_attn_v_proj_weight
Operation: mul
Transpose A: False, Transpose B: True
Send back result: True
Number of shards: 4

ðŸ“¤ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_self_attn_v_proj_weight_shard_0.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_self_attn_v_proj_weight_shard_1.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_self_attn_v_proj_weight_shard_2.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/input_token_embedding_matrix.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_self_attn_v_proj_weight_shard_3.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.104

â³ WAITING FOR ACKS FROM NODES (4)
âœ… Received ACK_matrixOp_complete 1/4
âœ… Received ACK_matrixOp_complete 2/4
âœ… Received ACK_matrixOp_complete 3/4
âœ… Received ACK_matrixOp_complete 4/4
âœ… All ACKs received!

============================================================
âœ… CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.03 seconds

ðŸ“Š Result base: layers_31_self_attn_v_proj_weightxinput_token_embedding_matrix (send_back=True)
âœ… Loaded /dev/shm/matrix_shards/layers_31_self_attn_v_proj_weightxinput_token_embedding_matrix_combined.bin
  Original dims: [1, 1, 18, 1024]
  Result tensor shape: torch.Size([18, 1024]), size: 73,728 bytes
  Data range: [-0.044690, 0.044402]
   Computing GQA attention with TORCH...
attn_output_flat shape : torch.Size([18, 4096])
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
Preparing full matrix: attn_output_flat.bin
Local paths - DISK: matrix_shards/attn_output_flat.bin, RAM: /dev/shm/matrix_shards/attn_output_flat.bin
Saving to local storage...
Saving matrix to binary file: matrix_shards/attn_output_flat.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 4096), dtype=float32
  Converting to 4D format...
    2D (18, 4096) -> 4D (1, 1, 18, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 4096
    Wrote 73,728 float32 elements
  File saved successfully
  File size: 294,932 bytes
  Expected size: 294,932 bytes
  âœ“ File size verification passed
  Memory usage: 0.28 MB
  Save completed: matrix_shards/attn_output_flat.bin
Saving matrix to binary file: /dev/shm/matrix_shards/attn_output_flat.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 4096), dtype=float32
  Converting to 4D format...
    2D (18, 4096) -> 4D (1, 1, 18, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 4096
    Wrote 73,728 float32 elements
  File saved successfully
  File size: 294,932 bytes
  Expected size: 294,932 bytes
  âœ“ File size verification passed
  Memory usage: 0.28 MB
  Save completed: /dev/shm/matrix_shards/attn_output_flat.bin
Remote paths - RAM: /dev/shm/matrix_shards/attn_output_flat.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/attn_output_flat.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
ðŸ“¤ Sent file attn_output_flat.bin to 192.168.2.104
âœ… Received attn_output_flat.bin 1/1
âœ… All ACKs received!
Sending to 192.168.2.101
ðŸ“¤ Sent file attn_output_flat.bin to 192.168.2.101
âœ… Received attn_output_flat.bin 1/1
âœ… All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_31_mlp_gate_proj_weight
   Split Matrix: True
   Dimension: 0

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([5017, 4096])
  Node 1: shard shape torch.Size([5017, 4096])
  Node 2: shard shape torch.Size([2150, 4096])
  Node 3: shard shape torch.Size([2152, 4096])
Starting distribution of 4 shards to 3 unique nodes
Processing shard 0 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_31_mlp_gate_proj_weight_shard_0.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_31_mlp_gate_proj_weight_shard_0.bin
Saving matrix to binary file: matrix_shards/layers_31_mlp_gate_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: matrix_shards/layers_31_mlp_gate_proj_weight_shard_0.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_31_mlp_gate_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: /dev/shm/matrix_shards/layers_31_mlp_gate_proj_weight_shard_0.bin
  Added RAM path to file list
Processing shard 1 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_31_mlp_gate_proj_weight_shard_1.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_31_mlp_gate_proj_weight_shard_1.bin
Saving matrix to binary file: matrix_shards/layers_31_mlp_gate_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: matrix_shards/layers_31_mlp_gate_proj_weight_shard_1.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_31_mlp_gate_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: /dev/shm/matrix_shards/layers_31_mlp_gate_proj_weight_shard_1.bin
  Added RAM path to file list
Processing shard 2 for node 192.168.2.101
  Remote node 192.168.2.101: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_31_mlp_gate_proj_weight_shard_2.bin
Saving matrix to binary file: matrix_shards/layers_31_mlp_gate_proj_weight_shard_2.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([2150, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(2150, 4096), dtype=float32
  Converting to 4D format...
    2D (2150, 4096) -> 4D (1, 1, 2150, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 2150 Ã— 4096
    Wrote 8,806,400 float32 elements
  File saved successfully
  File size: 35,225,620 bytes
  Expected size: 35,225,620 bytes
  âœ“ File size verification passed
  Memory usage: 33.59 MB
  Save completed: matrix_shards/layers_31_mlp_gate_proj_weight_shard_2.bin
DEBUG::: layers_31_mlp_gate_proj_weight_shard_2.bin
  Step 2: Sending file to remote node 192.168.2.101
ðŸ“¤ Sent file layers_31_mlp_gate_proj_weight_shard_2.bin to 192.168.2.101
âœ… Received layers_31_mlp_gate_proj_weight_shard_2.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_31_mlp_gate_proj_weight_shard_2.bin
Processing shard 3 for node 192.168.2.104
  Remote node 192.168.2.104: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_31_mlp_gate_proj_weight_shard_3.bin
Saving matrix to binary file: matrix_shards/layers_31_mlp_gate_proj_weight_shard_3.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([2152, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(2152, 4096), dtype=float32
  Converting to 4D format...
    2D (2152, 4096) -> 4D (1, 1, 2152, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 2152 Ã— 4096
    Wrote 8,814,592 float32 elements
  File saved successfully
  File size: 35,258,388 bytes
  Expected size: 35,258,388 bytes
  âœ“ File size verification passed
  Memory usage: 33.63 MB
  Save completed: matrix_shards/layers_31_mlp_gate_proj_weight_shard_3.bin
DEBUG::: layers_31_mlp_gate_proj_weight_shard_3.bin
  Step 2: Sending file to remote node 192.168.2.104
ðŸ“¤ Sent file layers_31_mlp_gate_proj_weight_shard_3.bin to 192.168.2.104
âœ… Received layers_31_mlp_gate_proj_weight_shard_3.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_31_mlp_gate_proj_weight_shard_3.bin
Distribution complete: 4 shards saved and distributed

============================================================
ðŸš€ STARTING CLUSTER OPERATION
============================================================
Matrix A: attn_output_flat
Matrix B: layers_31_mlp_gate_proj_weight
Operation: mul
Transpose A: False, Transpose B: True
Send back result: True
Number of shards: 4

ðŸ“¤ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_mlp_gate_proj_weight_shard_0.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_mlp_gate_proj_weight_shard_1.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_mlp_gate_proj_weight_shard_2.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_mlp_gate_proj_weight_shard_3.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.104

â³ WAITING FOR ACKS FROM NODES (4)
âœ… Received ACK_matrixOp_complete 1/4
âœ… Received ACK_matrixOp_complete 2/4
âœ… Received ACK_matrixOp_complete 3/4
âœ… Received ACK_matrixOp_complete 4/4
âœ… All ACKs received!

============================================================
âœ… CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.13 seconds

ðŸ“Š Result base: layers_31_mlp_gate_proj_weightxattn_output_flat (send_back=True)
âœ… Loaded /dev/shm/matrix_shards/layers_31_mlp_gate_proj_weightxattn_output_flat_combined.bin
  Original dims: [1, 1, 18, 14336]
  Result tensor shape: torch.Size([18, 14336]), size: 1,032,192 bytes
  Data range: [-0.046599, 0.038430]
   Running MLP up projection...
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_31_mlp_up_proj_weight
   Split Matrix: True
   Dimension: 0

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([5017, 4096])
  Node 1: shard shape torch.Size([5017, 4096])
  Node 2: shard shape torch.Size([2150, 4096])
  Node 3: shard shape torch.Size([2152, 4096])
Starting distribution of 4 shards to 3 unique nodes
Processing shard 0 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_31_mlp_up_proj_weight_shard_0.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_31_mlp_up_proj_weight_shard_0.bin
Saving matrix to binary file: matrix_shards/layers_31_mlp_up_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: matrix_shards/layers_31_mlp_up_proj_weight_shard_0.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_31_mlp_up_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: /dev/shm/matrix_shards/layers_31_mlp_up_proj_weight_shard_0.bin
  Added RAM path to file list
Processing shard 1 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_31_mlp_up_proj_weight_shard_1.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_31_mlp_up_proj_weight_shard_1.bin
Saving matrix to binary file: matrix_shards/layers_31_mlp_up_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: matrix_shards/layers_31_mlp_up_proj_weight_shard_1.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_31_mlp_up_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([5017, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(5017, 4096), dtype=float32
  Converting to 4D format...
    2D (5017, 4096) -> 4D (1, 1, 5017, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 5017 Ã— 4096
    Wrote 20,549,632 float32 elements
  File saved successfully
  File size: 82,198,548 bytes
  Expected size: 82,198,548 bytes
  âœ“ File size verification passed
  Memory usage: 78.39 MB
  Save completed: /dev/shm/matrix_shards/layers_31_mlp_up_proj_weight_shard_1.bin
  Added RAM path to file list
Processing shard 2 for node 192.168.2.101
  Remote node 192.168.2.101: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_31_mlp_up_proj_weight_shard_2.bin
Saving matrix to binary file: matrix_shards/layers_31_mlp_up_proj_weight_shard_2.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([2150, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(2150, 4096), dtype=float32
  Converting to 4D format...
    2D (2150, 4096) -> 4D (1, 1, 2150, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 2150 Ã— 4096
    Wrote 8,806,400 float32 elements
  File saved successfully
  File size: 35,225,620 bytes
  Expected size: 35,225,620 bytes
  âœ“ File size verification passed
  Memory usage: 33.59 MB
  Save completed: matrix_shards/layers_31_mlp_up_proj_weight_shard_2.bin
DEBUG::: layers_31_mlp_up_proj_weight_shard_2.bin
  Step 2: Sending file to remote node 192.168.2.101
ðŸ“¤ Sent file layers_31_mlp_up_proj_weight_shard_2.bin to 192.168.2.101
âœ… Received layers_31_mlp_up_proj_weight_shard_2.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_31_mlp_up_proj_weight_shard_2.bin
Processing shard 3 for node 192.168.2.104
  Remote node 192.168.2.104: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_31_mlp_up_proj_weight_shard_3.bin
Saving matrix to binary file: matrix_shards/layers_31_mlp_up_proj_weight_shard_3.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([2152, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(2152, 4096), dtype=float32
  Converting to 4D format...
    2D (2152, 4096) -> 4D (1, 1, 2152, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 2152 Ã— 4096
    Wrote 8,814,592 float32 elements
  File saved successfully
  File size: 35,258,388 bytes
  Expected size: 35,258,388 bytes
  âœ“ File size verification passed
  Memory usage: 33.63 MB
  Save completed: matrix_shards/layers_31_mlp_up_proj_weight_shard_3.bin
DEBUG::: layers_31_mlp_up_proj_weight_shard_3.bin
  Step 2: Sending file to remote node 192.168.2.104
ðŸ“¤ Sent file layers_31_mlp_up_proj_weight_shard_3.bin to 192.168.2.104
âœ… Received layers_31_mlp_up_proj_weight_shard_3.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_31_mlp_up_proj_weight_shard_3.bin
Distribution complete: 4 shards saved and distributed

============================================================
ðŸš€ STARTING CLUSTER OPERATION
============================================================
Matrix A: attn_output_flat
Matrix B: layers_31_mlp_up_proj_weight
Operation: mul
Transpose A: False, Transpose B: True
Send back result: True
Number of shards: 4

ðŸ“¤ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_mlp_up_proj_weight_shard_0.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_mlp_up_proj_weight_shard_1.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_mlp_up_proj_weight_shard_2.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/attn_output_flat.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_mlp_up_proj_weight_shard_3.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.104

â³ WAITING FOR ACKS FROM NODES (4)
âœ… Received ACK_matrixOp_complete 1/4
âœ… Received ACK_matrixOp_complete 2/4
âœ… Received ACK_matrixOp_complete 3/4
âœ… Received ACK_matrixOp_complete 4/4
âœ… All ACKs received!

============================================================
âœ… CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.13 seconds

ðŸ“Š Result base: layers_31_mlp_up_proj_weightxattn_output_flat (send_back=True)
âœ… Loaded /dev/shm/matrix_shards/layers_31_mlp_up_proj_weightxattn_output_flat_combined.bin
  Original dims: [1, 1, 18, 14336]
  Result tensor shape: torch.Size([18, 14336]), size: 1,032,192 bytes
  Data range: [-0.028438, 0.040903]
   Up projection result shape: torch.Size([18, 14336])
   Applying SiLU activation to gate...
   Element-wise multiplication (gate_silu * up)...
   MLP intermediate shape: torch.Size([18, 14336])
   Running MLP down projection...
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
Preparing full matrix: mlp_intermediate_cluster.bin
Local paths - DISK: matrix_shards/mlp_intermediate_cluster.bin, RAM: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin
Saving to local storage...
Saving matrix to binary file: matrix_shards/mlp_intermediate_cluster.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 14336), dtype=float32
  Converting to 4D format...
    2D (18, 14336) -> 4D (1, 1, 18, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 14336
    Wrote 258,048 float32 elements
  File saved successfully
  File size: 1,032,212 bytes
  Expected size: 1,032,212 bytes
  âœ“ File size verification passed
  Memory usage: 0.98 MB
  Save completed: matrix_shards/mlp_intermediate_cluster.bin
Saving matrix to binary file: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 14336), dtype=float32
  Converting to 4D format...
    2D (18, 14336) -> 4D (1, 1, 18, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 14336
    Wrote 258,048 float32 elements
  File saved successfully
  File size: 1,032,212 bytes
  Expected size: 1,032,212 bytes
  âœ“ File size verification passed
  Memory usage: 0.98 MB
  Save completed: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin
Remote paths - RAM: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin, DISK: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/mlp_intermediate_cluster.bin
Distributing to 2 remote node(s)...
Sending to 192.168.2.104
ðŸ“¤ Sent file mlp_intermediate_cluster.bin to 192.168.2.104
âœ… Received mlp_intermediate_cluster.bin 1/1
âœ… All ACKs received!
Sending to 192.168.2.101
ðŸ“¤ Sent file mlp_intermediate_cluster.bin to 192.168.2.101
âœ… Received mlp_intermediate_cluster.bin 1/1
âœ… All ACKs received!
Full matrix distribution completed
Total file paths tracked: 4
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...
   Matrix Name: layers_31_mlp_down_proj_weight
   Split Matrix: True
   Dimension: 0

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([1433, 14336])
  Node 1: shard shape torch.Size([1433, 14336])
  Node 2: shard shape torch.Size([614, 14336])
  Node 3: shard shape torch.Size([616, 14336])
Starting distribution of 4 shards to 3 unique nodes
Processing shard 0 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_31_mlp_down_proj_weight_shard_0.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_31_mlp_down_proj_weight_shard_0.bin
Saving matrix to binary file: matrix_shards/layers_31_mlp_down_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([1433, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(1433, 14336), dtype=float32
  Converting to 4D format...
    2D (1433, 14336) -> 4D (1, 1, 1433, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 1433 Ã— 14336
    Wrote 20,543,488 float32 elements
  File saved successfully
  File size: 82,173,972 bytes
  Expected size: 82,173,972 bytes
  âœ“ File size verification passed
  Memory usage: 78.37 MB
  Save completed: matrix_shards/layers_31_mlp_down_proj_weight_shard_0.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_31_mlp_down_proj_weight_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([1433, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(1433, 14336), dtype=float32
  Converting to 4D format...
    2D (1433, 14336) -> 4D (1, 1, 1433, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 1433 Ã— 14336
    Wrote 20,543,488 float32 elements
  File saved successfully
  File size: 82,173,972 bytes
  Expected size: 82,173,972 bytes
  âœ“ File size verification passed
  Memory usage: 78.37 MB
  Save completed: /dev/shm/matrix_shards/layers_31_mlp_down_proj_weight_shard_0.bin
  Added RAM path to file list
Processing shard 1 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/layers_31_mlp_down_proj_weight_shard_1.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/layers_31_mlp_down_proj_weight_shard_1.bin
Saving matrix to binary file: matrix_shards/layers_31_mlp_down_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([1433, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(1433, 14336), dtype=float32
  Converting to 4D format...
    2D (1433, 14336) -> 4D (1, 1, 1433, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 1433 Ã— 14336
    Wrote 20,543,488 float32 elements
  File saved successfully
  File size: 82,173,972 bytes
  Expected size: 82,173,972 bytes
  âœ“ File size verification passed
  Memory usage: 78.37 MB
  Save completed: matrix_shards/layers_31_mlp_down_proj_weight_shard_1.bin
Saving matrix to binary file: /dev/shm/matrix_shards/layers_31_mlp_down_proj_weight_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([1433, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(1433, 14336), dtype=float32
  Converting to 4D format...
    2D (1433, 14336) -> 4D (1, 1, 1433, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 1433 Ã— 14336
    Wrote 20,543,488 float32 elements
  File saved successfully
  File size: 82,173,972 bytes
  Expected size: 82,173,972 bytes
  âœ“ File size verification passed
  Memory usage: 78.37 MB
  Save completed: /dev/shm/matrix_shards/layers_31_mlp_down_proj_weight_shard_1.bin
  Added RAM path to file list
Processing shard 2 for node 192.168.2.101
  Remote node 192.168.2.101: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_31_mlp_down_proj_weight_shard_2.bin
Saving matrix to binary file: matrix_shards/layers_31_mlp_down_proj_weight_shard_2.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([614, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(614, 14336), dtype=float32
  Converting to 4D format...
    2D (614, 14336) -> 4D (1, 1, 614, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 614 Ã— 14336
    Wrote 8,802,304 float32 elements
  File saved successfully
  File size: 35,209,236 bytes
  Expected size: 35,209,236 bytes
  âœ“ File size verification passed
  Memory usage: 33.58 MB
  Save completed: matrix_shards/layers_31_mlp_down_proj_weight_shard_2.bin
DEBUG::: layers_31_mlp_down_proj_weight_shard_2.bin
  Step 2: Sending file to remote node 192.168.2.101
ðŸ“¤ Sent file layers_31_mlp_down_proj_weight_shard_2.bin to 192.168.2.101
âœ… Received layers_31_mlp_down_proj_weight_shard_2.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_31_mlp_down_proj_weight_shard_2.bin
Processing shard 3 for node 192.168.2.104
  Remote node 192.168.2.104: Beginning distribution
  Step 1: Saving locally to matrix_shards/layers_31_mlp_down_proj_weight_shard_3.bin
Saving matrix to binary file: matrix_shards/layers_31_mlp_down_proj_weight_shard_3.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([616, 14336]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(616, 14336), dtype=float32
  Converting to 4D format...
    2D (616, 14336) -> 4D (1, 1, 616, 14336)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 616 Ã— 14336
    Wrote 8,830,976 float32 elements
  File saved successfully
  File size: 35,323,924 bytes
  Expected size: 35,323,924 bytes
  âœ“ File size verification passed
  Memory usage: 33.69 MB
  Save completed: matrix_shards/layers_31_mlp_down_proj_weight_shard_3.bin
DEBUG::: layers_31_mlp_down_proj_weight_shard_3.bin
  Step 2: Sending file to remote node 192.168.2.104
ðŸ“¤ Sent file layers_31_mlp_down_proj_weight_shard_3.bin to 192.168.2.104
âœ… Received layers_31_mlp_down_proj_weight_shard_3.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/layers_31_mlp_down_proj_weight_shard_3.bin
Distribution complete: 4 shards saved and distributed

============================================================
ðŸš€ STARTING CLUSTER OPERATION
============================================================
Matrix A: mlp_intermediate_cluster
Matrix B: layers_31_mlp_down_proj_weight
Operation: mul
Transpose A: False, Transpose B: True
Send back result: True
Number of shards: 4

ðŸ“¤ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_mlp_down_proj_weight_shard_0.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_mlp_down_proj_weight_shard_1.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_mlp_down_proj_weight_shard_2.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/mlp_intermediate_cluster.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_mlp_down_proj_weight_shard_3.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.104

â³ WAITING FOR ACKS FROM NODES (4)
âœ… Received ACK_matrixOp_complete 1/4
âœ… Received ACK_matrixOp_complete 2/4
âœ… Received ACK_matrixOp_complete 3/4
âœ… Received ACK_matrixOp_complete 4/4
âœ… All ACKs received!

============================================================
âœ… CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.14 seconds

ðŸ“Š Result base: layers_31_mlp_down_proj_weightxmlp_intermediate_cluster (send_back=True)
âœ… Loaded /dev/shm/matrix_shards/layers_31_mlp_down_proj_weightxmlp_intermediate_cluster_combined.bin
  Original dims: [1, 1, 18, 4096]
  Result tensor shape: torch.Size([18, 4096]), size: 294,912 bytes
  Data range: [-0.000402, 0.000312]
   Adding residual connection (MLP + attention)...
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([18, 1433])
  Node 1: shard shape torch.Size([18, 1433])
  Node 2: shard shape torch.Size([18, 614])
  Node 3: shard shape torch.Size([18, 616])
Starting distribution of 4 shards to 3 unique nodes
Processing shard 0 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/mlp_output_cluster_shard_0.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/mlp_output_cluster_shard_0.bin
Saving matrix to binary file: matrix_shards/mlp_output_cluster_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: matrix_shards/mlp_output_cluster_shard_0.bin
Saving matrix to binary file: /dev/shm/matrix_shards/mlp_output_cluster_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: /dev/shm/matrix_shards/mlp_output_cluster_shard_0.bin
  Added RAM path to file list
Processing shard 1 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/mlp_output_cluster_shard_1.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/mlp_output_cluster_shard_1.bin
Saving matrix to binary file: matrix_shards/mlp_output_cluster_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: matrix_shards/mlp_output_cluster_shard_1.bin
Saving matrix to binary file: /dev/shm/matrix_shards/mlp_output_cluster_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: /dev/shm/matrix_shards/mlp_output_cluster_shard_1.bin
  Added RAM path to file list
Processing shard 2 for node 192.168.2.101
  Remote node 192.168.2.101: Beginning distribution
  Step 1: Saving locally to matrix_shards/mlp_output_cluster_shard_2.bin
Saving matrix to binary file: matrix_shards/mlp_output_cluster_shard_2.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 614]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 614), dtype=float32
  Converting to 4D format...
    2D (18, 614) -> 4D (1, 1, 18, 614)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 614
    Wrote 11,052 float32 elements
  File saved successfully
  File size: 44,228 bytes
  Expected size: 44,228 bytes
  âœ“ File size verification passed
  Memory usage: 0.04 MB
  Save completed: matrix_shards/mlp_output_cluster_shard_2.bin
DEBUG::: mlp_output_cluster_shard_2.bin
  Step 2: Sending file to remote node 192.168.2.101
ðŸ“¤ Sent file mlp_output_cluster_shard_2.bin to 192.168.2.101
âœ… Received mlp_output_cluster_shard_2.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/mlp_output_cluster_shard_2.bin
Processing shard 3 for node 192.168.2.104
  Remote node 192.168.2.104: Beginning distribution
  Step 1: Saving locally to matrix_shards/mlp_output_cluster_shard_3.bin
Saving matrix to binary file: matrix_shards/mlp_output_cluster_shard_3.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 616]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 616), dtype=float32
  Converting to 4D format...
    2D (18, 616) -> 4D (1, 1, 18, 616)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 616
    Wrote 11,088 float32 elements
  File saved successfully
  File size: 44,372 bytes
  Expected size: 44,372 bytes
  âœ“ File size verification passed
  Memory usage: 0.04 MB
  Save completed: matrix_shards/mlp_output_cluster_shard_3.bin
DEBUG::: mlp_output_cluster_shard_3.bin
  Step 2: Sending file to remote node 192.168.2.104
ðŸ“¤ Sent file mlp_output_cluster_shard_3.bin to 192.168.2.104
âœ… Received mlp_output_cluster_shard_3.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/mlp_output_cluster_shard_3.bin
Distribution complete: 4 shards saved and distributed
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([18, 1433])
  Node 1: shard shape torch.Size([18, 1433])
  Node 2: shard shape torch.Size([18, 614])
  Node 3: shard shape torch.Size([18, 616])
Starting distribution of 4 shards to 3 unique nodes
Processing shard 0 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/attn_outpuattn_output_clustert_flat_shard_0.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_0.bin
Saving matrix to binary file: matrix_shards/attn_outpuattn_output_clustert_flat_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: matrix_shards/attn_outpuattn_output_clustert_flat_shard_0.bin
Saving matrix to binary file: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_0.bin
  Added RAM path to file list
Processing shard 1 for node 192.168.2.100
  Head node: Saving to DISK=matrix_shards/attn_outpuattn_output_clustert_flat_shard_1.bin
  Head node: Saving to RAM=/dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_1.bin
Saving matrix to binary file: matrix_shards/attn_outpuattn_output_clustert_flat_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: matrix_shards/attn_outpuattn_output_clustert_flat_shard_1.bin
Saving matrix to binary file: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 1433]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 1433), dtype=float32
  Converting to 4D format...
    2D (18, 1433) -> 4D (1, 1, 18, 1433)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 1433
    Wrote 25,794 float32 elements
  File saved successfully
  File size: 103,196 bytes
  Expected size: 103,196 bytes
  âœ“ File size verification passed
  Memory usage: 0.10 MB
  Save completed: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_1.bin
  Added RAM path to file list
Processing shard 2 for node 192.168.2.101
  Remote node 192.168.2.101: Beginning distribution
  Step 1: Saving locally to matrix_shards/attn_outpuattn_output_clustert_flat_shard_2.bin
Saving matrix to binary file: matrix_shards/attn_outpuattn_output_clustert_flat_shard_2.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 614]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 614), dtype=float32
  Converting to 4D format...
    2D (18, 614) -> 4D (1, 1, 18, 614)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 614
    Wrote 11,052 float32 elements
  File saved successfully
  File size: 44,228 bytes
  Expected size: 44,228 bytes
  âœ“ File size verification passed
  Memory usage: 0.04 MB
  Save completed: matrix_shards/attn_outpuattn_output_clustert_flat_shard_2.bin
DEBUG::: attn_outpuattn_output_clustert_flat_shard_2.bin
  Step 2: Sending file to remote node 192.168.2.101
ðŸ“¤ Sent file attn_outpuattn_output_clustert_flat_shard_2.bin to 192.168.2.101
âœ… Received attn_outpuattn_output_clustert_flat_shard_2.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_2.bin
Processing shard 3 for node 192.168.2.104
  Remote node 192.168.2.104: Beginning distribution
  Step 1: Saving locally to matrix_shards/attn_outpuattn_output_clustert_flat_shard_3.bin
Saving matrix to binary file: matrix_shards/attn_outpuattn_output_clustert_flat_shard_3.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([18, 616]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(18, 616), dtype=float32
  Converting to 4D format...
    2D (18, 616) -> 4D (1, 1, 18, 616)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 Ã— 1 Ã— 18 Ã— 616
    Wrote 11,088 float32 elements
  File saved successfully
  File size: 44,372 bytes
  Expected size: 44,372 bytes
  âœ“ File size verification passed
  Memory usage: 0.04 MB
  Save completed: matrix_shards/attn_outpuattn_output_clustert_flat_shard_3.bin
DEBUG::: attn_outpuattn_output_clustert_flat_shard_3.bin
  Step 2: Sending file to remote node 192.168.2.104
ðŸ“¤ Sent file attn_outpuattn_output_clustert_flat_shard_3.bin to 192.168.2.104
âœ… Received attn_outpuattn_output_clustert_flat_shard_3.bin 1/1
âœ… All ACKs received!
  Step 3: Sending copy command to remote
  Added remote RAM path to file list: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_3.bin
Distribution complete: 4 shards saved and distributed

============================================================
ðŸš€ STARTING CLUSTER OPERATION
============================================================
Matrix A: mlp_output_cluster
Matrix B: attn_outpuattn_output_clustert_flat
Operation: add
Transpose A: False, Transpose B: True
Send back result: True
Number of shards: 4

ðŸ“¤ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/mlp_output_cluster_shard_0.bin
  Matrix B path: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_0.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/mlp_output_cluster_shard_1.bin
  Matrix B path: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_1.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/mlp_output_cluster_shard_2.bin
  Matrix B path: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_2.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: False (GPU #0)
  Next GPU for this node will be: #0
  Matrix A path: /dev/shm/matrix_shards/mlp_output_cluster_shard_3.bin
  Matrix B path: /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flat_shard_3.bin
  Final transpose flags - A: false, B: false
Send back result: Yes (14 shards will be combined)
  â†’ system=1, join_dim=1, shards=4
DEBUG SEND_BACK VALUE: 14
  Sending command to node...
  âœ… Command sent to node 192.168.2.104

â³ WAITING FOR ACKS FROM NODES (4)
âœ… Received ACK_matrixOp_complete 1/4
âœ… Received ACK_matrixOp_complete 2/4
âœ… Received ACK_matrixOp_complete 3/4
âœ… Received ACK_matrixOp_complete 4/4
âœ… All ACKs received!

============================================================
âœ… CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.03 seconds

ðŸ“Š Result base: attn_outpuattn_output_clustert_flatxmlp_output_cluster (send_back=True)
âœ… Received ACK_combined_matrix_saved 1/1
âœ… All ACKs received!
âœ… Loaded /dev/shm/matrix_shards/attn_outpuattn_output_clustert_flatxmlp_output_cluster_combined.bin
  Original dims: [1, 1, 18, 4096]
  Result tensor shape: torch.Size([18, 4096]), size: 294,912 bytes
  Data range: [-0.022634, 0.031906]
   Applying post-attention layer normalization...
   Loaded post-attention layer norm weights: shape torch.Size([4096])
   Applied layer normalization
======================================================================
ðŸš€ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
âœ… Node configuration validated: 4 nodes configured
âœ… Percentage distribution validated: 1.000000

ðŸŒ CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

ðŸ“ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
âœ… Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

ðŸ“Š INITIALIZING INSTANCE VARIABLES...

ðŸ“‚ CREATING LOCAL DIRECTORIES...
âœ… All required directories already exist

ðŸ”Œ SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   âœ… Connected to worker node 192.168.2.101:5557
   âœ… Connected to worker node 192.168.2.104:5557
   âœ… Connected to worker WiFi 192.168.3.13:5557
   âœ… Connected to worker WiFi 192.168.3.243:5557
   âœ… Connected to worker WiFi 192.168.3.165:5557
   âœ… Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

ðŸ”„ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
âœ… ACK receiver already exists on port 7790

ðŸ“¡ CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   âœ… Directory creation command sent to 192.168.2.101
   âœ… Directory creation command sent to 192.168.2.104
   âœ… Directory creation command sent to 192.168.2.100
âœ… Created 4 shards according to node percentages
  Node 0: shard shape torch.Size([18, 1433])
  Node 1: shard shape torch.Size([18, 1433])
  Node 2: shard shape torch.Size([18, 614])
(ray-conda-env) rino@rino-Z370-HD3:~/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix$ 
