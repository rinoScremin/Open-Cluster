======================================================================
üöÄ INITIALIZING CLUSTER MATRIX DISTRIBUTION SYSTEM
======================================================================
‚úÖ Node configuration validated: 6 nodes configured
‚úÖ Percentage distribution validated: 1.000000

üåê CONFIGURING NETWORK SETTINGS...
   Head Node Ethernet IP: 192.168.2.100
   Head Node WiFi IP: 192.168.50.113
   Head Node Ports: PULL=7779, PUSH=7780
   Worker Node Ports: PULL=5557, PUSH=5558
   Cluster Barrier Port: 7790

üìÅ CONFIGURING STORAGE PATHS...
   Local Paths:
     - RAM Results: /dev/shm/matrix_results/
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
   Remote Paths:
     - Disk Folder: matrix_shards/
     - RAM Folder: /dev/shm/matrix_shards/
     - RAM Results: /dev/shm/matrix_results/
     - Project Dir: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/
‚úÖ Using Python path: /home/rino/anaconda3/envs/cluster-worker/bin/python

üìä INITIALIZING INSTANCE VARIABLES...
   Matrix Name: temp_layer31_attn_out
   Split Matrix: True
   Dimension: 0

üìÇ CREATING LOCAL DIRECTORIES...
‚úÖ All required directories already exist

üîå SETTING UP ZEROMQ CONNECTIONS...
   Connecting to 3 unique nodes...
   ‚úÖ Connected to worker node 192.168.2.101:5557
   ‚úÖ Connected to worker node 192.168.2.104:5557
   ‚úÖ Connected to worker WiFi 192.168.3.13:5557
   ‚úÖ Connected to worker WiFi 192.168.3.243:5557
   ‚úÖ Connected to worker WiFi 192.168.3.165:5557
   ‚úÖ Connected to head node (self) 192.168.2.100:7779
   Total sockets in pool: 3

üîÑ SETTING UP CLUSTER BARRIER/ACK RECEIVER...
‚úÖ ACK receiver already exists on port 7790

üì° CREATING REMOTE DIRECTORIES ON WORKER NODES...
   Sending command: mkdir -p /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/ /dev/shm/matrix_shards/ /dev/shm/matrix_results/
   ‚úÖ Directory creation command sent to 192.168.2.101
   ‚úÖ Directory creation command sent to 192.168.2.104
   ‚úÖ Directory creation command sent to 192.168.2.100
‚úÖ Matrix A: torch.Size([4, 4096]) ‚Üí [torch.Size([2, 4096]), torch.Size([2, 4096])] (split along dim=0)

üì§ Distributing Matrix A row shards
Saving matrix to binary file: /dev/shm/matrix_shards/temp_layer31_attn_out_shard_0.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([2, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(2, 4096), dtype=float32
  Converting to 4D format...
    2D (2, 4096) -> 4D (1, 1, 2, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 √ó 1 √ó 2 √ó 4096
    Wrote 8,192 float32 elements
  File saved successfully
  File size: 32,788 bytes
  Expected size: 32,788 bytes
  ‚úì File size verification passed
  Memory usage: 0.03 MB
  Save completed: /dev/shm/matrix_shards/temp_layer31_attn_out_shard_0.bin
Saving matrix to binary file: /dev/shm/matrix_shards/temp_layer31_attn_out_shard_1.bin
  Converting input to numpy array...
    Input is PyTorch tensor: shape=torch.Size([2, 4096]), dtype=torch.float32, device=cpu
    Converted to CPU float32 numpy array
  Final numpy array: shape=(2, 4096), dtype=float32
  Converting to 4D format...
    2D (2, 4096) -> 4D (1, 1, 2, 4096)
  Writing binary file...
    Wrote ndim: 4
    Dimensions: 1 √ó 1 √ó 2 √ó 4096
    Wrote 8,192 float32 elements
  File saved successfully
  File size: 32,788 bytes
  Expected size: 32,788 bytes
  ‚úì File size verification passed
  Memory usage: 0.03 MB
  Save completed: /dev/shm/matrix_shards/temp_layer31_attn_out_shard_1.bin
  Copied shard 0 to: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix//temp_layer31_attn_out_shard_0.bin
  Copied shard 0 to: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/temp_layer31_attn_out_shard_0.bin
  Copied shard 1 to: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix//temp_layer31_attn_out_shard_1.bin
  Copied shard 1 to: /home/rino/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix/matrix_shards/temp_layer31_attn_out_shard_1.bin
üì§ Sent file temp_layer31_attn_out_shard_0.bin to 192.168.2.101
Sent shard 0 to IP: 192.168.2.101
üì§ Sent file temp_layer31_attn_out_shard_1.bin to 192.168.2.104
‚úÖ Received temp_layer31_attn_out_shard_1.bin 1/1
‚úÖ All ACKs received!
Sent shard 1 to IP: 192.168.2.104
üì§ Sent file temp_layer31_attn_out_shard_1.bin to 192.168.2.101
‚úÖ Received temp_layer31_attn_out_shard_1.bin 1/1
‚úÖ All ACKs received!
Sent shard 1 to IP: 192.168.2.101

üìã Node shard assignments:
  192.168.2.100 -> shard_0
  192.168.2.100 -> shard_0
  192.168.2.101 -> shard_0
  192.168.2.104 -> shard_1
  192.168.2.100 -> shard_1
  192.168.2.101 -> shard_1

‚úÖ Final matrix_file_paths_list (paths only):
  Node 0: shard_0
  Node 1: shard_0
  Node 2: shard_0
  Node 3: shard_1
  Node 4: shard_1
  Node 5: shard_1

============================================================
üöÄ STARTING CLUSTER OPERATION
============================================================
Matrix A: temp_layer31_attn_out
Matrix B: layers_31_self_attn_o_proj_weight
Operation: mul
Transpose A: False, Transpose B: True
Send back result: True
Number of shards: 6

üì§ DISTRIBUTING OPERATIONS TO NODES
----------------------------------------

Processing shard 0:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/temp_layer31_attn_out_shard_0.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_self_attn_o_proj_weight_shard_0.bin
  Final transpose flags - A: false, B: false
  Send back result: Yes (-6 shards will be combined)
DEBUG TEST: -6
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 1:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/temp_layer31_attn_out_shard_0.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_self_attn_o_proj_weight_shard_1.bin
  Final transpose flags - A: false, B: false
  Send back result: Yes (-6 shards will be combined)
DEBUG TEST: -6
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 2:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/temp_layer31_attn_out_shard_0.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_self_attn_o_proj_weight_shard_2.bin
  Final transpose flags - A: false, B: false
  Send back result: Yes (-6 shards will be combined)
DEBUG TEST: -6
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

Processing shard 3:
  Node: 192.168.2.104
  Backend: llama
  Use GPU: True (GPU #0)
  Next GPU for this node will be: #1
  Matrix A path: /dev/shm/matrix_shards/temp_layer31_attn_out_shard_1.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_self_attn_o_proj_weight_shard_3.bin
  Final transpose flags - A: false, B: false
  Send back result: Yes (-6 shards will be combined)
DEBUG TEST: -6
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.104

Processing shard 4:
  Node: 192.168.2.100
  Backend: llama
  Use GPU: True (GPU #2)
  Next GPU for this node will be: #3
  Matrix A path: /dev/shm/matrix_shards/temp_layer31_attn_out_shard_1.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_self_attn_o_proj_weight_shard_4.bin
  Final transpose flags - A: false, B: false
  Send back result: Yes (-6 shards will be combined)
DEBUG TEST: -6
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.100

Processing shard 5:
  Node: 192.168.2.101
  Backend: llama
  Use GPU: True (GPU #1)
  Next GPU for this node will be: #2
  Matrix A path: /dev/shm/matrix_shards/temp_layer31_attn_out_shard_1.bin
  Matrix B path: /dev/shm/matrix_shards/layers_31_self_attn_o_proj_weight_shard_5.bin
  Final transpose flags - A: false, B: false
  Send back result: Yes (-6 shards will be combined)
DEBUG TEST: -6
  Sending command to node...
  ‚úÖ Command sent to node 192.168.2.101

‚è≥ WAITING FOR ACKS FROM NODES (6)
‚úÖ Received ACK_matrixOp_complete 1/6
‚úÖ Received ACK_matrixOp_complete 2/6
‚úÖ Received ACK_matrixOp_complete 3/6
‚úÖ Received ACK_matrixOp_complete 4/6
‚úÖ Received ACK_matrixOp_complete 5/6
‚úÖ Received ACK_matrixOp_complete 6/6
‚úÖ All ACKs received!

============================================================
‚úÖ CLUSTER OPERATION COMPLETE
============================================================
Operation time: 0.08 seconds

üìä Result base: layers_31_self_attn_o_proj_weightxtemp_layer31_attn_out (send_back=True)
‚úÖ Loaded /dev/shm/matrix_shards/layers_31_self_attn_o_proj_weightxtemp_layer31_attn_out_combined.bin
  Original dims: [1, 1, 4, 4096]
  Result tensor shape: torch.Size([4, 4096]), size: 65,536 bytes
  Data range: [-13.182899, 5.229529]
   Attention residual connection...
   Post-attention normalization...
   MLP forward pass...
   Final residual connection...

üìä LAYER 31 SUMMARY:
   ‚Ä¢ Hidden state shape: torch.Size([4, 4096])
   ‚Ä¢ Attention output shape: torch.Size([4, 4096])
   ‚Ä¢ Time: 7.98s
   ‚Ä¢ Cluster ops: 6.35s
   ‚Ä¢ Torch ops: 0.07s

üìè STEP 3: FINAL NORMALIZATION
--------------------------------------------------
   Applying final normalization...
   Final normalized shape: torch.Size([4, 4096])

üìù STEP 4: LANGUAGE MODEL HEAD
--------------------------------------------------
   No LM head found, skipping...

======================================================================
üéâ FULL MODEL FORWARD PASS COMPLETE!
======================================================================

üìä PERFORMANCE SUMMARY:
   ‚Ä¢ Total layers processed: 32
   ‚Ä¢ Final hidden state shape: torch.Size([4, 4096])
   ‚Ä¢ Total cluster time: 354.96s
   ‚Ä¢ Total torch time: 4.46s
   ‚Ä¢ Total time: 359.42s
   ‚Ä¢ Efficiency: 98.8% cluster, 1.2% torch

üìã LAYER OUTPUTS STORED FOR:
   ‚Ä¢ Layer 0
   ‚Ä¢ Layer 1
   ‚Ä¢ Layer 2
   ‚Ä¢ Layer 3
   ‚Ä¢ Layer 4
   ‚Ä¢ Layer 5
   ‚Ä¢ Layer 6
   ‚Ä¢ Layer 7
   ‚Ä¢ Layer 8
   ‚Ä¢ Layer 9
   ‚Ä¢ Layer 10
   ‚Ä¢ Layer 11
   ‚Ä¢ Layer 12
   ‚Ä¢ Layer 13
   ‚Ä¢ Layer 14
   ‚Ä¢ Layer 15
   ‚Ä¢ Layer 16
   ‚Ä¢ Layer 17
   ‚Ä¢ Layer 18
   ‚Ä¢ Layer 19
   ‚Ä¢ Layer 20
   ‚Ä¢ Layer 21
   ‚Ä¢ Layer 22
   ‚Ä¢ Layer 23
   ‚Ä¢ Layer 24
   ‚Ä¢ Layer 25
   ‚Ä¢ Layer 26
   ‚Ä¢ Layer 27
   ‚Ä¢ Layer 28
   ‚Ä¢ Layer 29
   ‚Ä¢ Layer 30
   ‚Ä¢ Layer 31

üîç FINAL HIDDEN STATE INFO:
   ‚Ä¢ Shape: torch.Size([4, 4096])
   ‚Ä¢ Mean: 0.097668
   ‚Ä¢ Std: 2.431947
   ‚Ä¢ Min: -29.507620
   ‚Ä¢ Max: 50.291306

(ray-conda-env) rino@rino-Z370-HD3:~/Desktop/Open_Cluster_AI_Station_beta/cluster_matrix$ 
